{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf7ec76-7def-47c5-9628-8955654d3022",
   "metadata": {},
   "source": [
    "**<em>Q:in the next part of this DL book it called this code as 'Drop out code' what's the reason?</em>**\n",
    "```python\n",
    "import numpy, sys\n",
    "def relu(x):\n",
    "    return (x >= 0) * x # returns x if x > 0\n",
    "                        # returns 0 otherwise\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output >= 0 #returns 1 for input > 0\n",
    "\n",
    "alpha, iterations, hidden_size = (0.005, 300, 100)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image,hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations):\n",
    "    error, correct_cnt = (0.0,0)\n",
    "    for i in range(len(images)):\n",
    "        layer_0 = images[i:i+1]\n",
    "        layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        layer_2 = np.dot(layer_1,weights_1_2)\n",
    "\n",
    "        error += np.sum((labels[i:i+1] - layer_2) ** 2)\n",
    "        correct_cnt += int(np.argmax(layer_2) == np.argmax(labels[i:i+1]))\n",
    "        layer_2_delta = (labels[i:i+1] - layer_2)\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "        layer_1_delta *= dropout_mask\n",
    "\n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "    if(j%10 == 0):\n",
    "        test_error = 0.0\n",
    "        test_correct_cnt = 0\n",
    "\n",
    "        for i in range(len(test_images)):\n",
    "            layer_0 = test_images[i:i+1]\n",
    "            layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "            layer_2 = np.dot(layer_1, weights_1_2)\n",
    "\n",
    "            test_error += np.sum((test_labels[i:i+1] - layer_2) ** 2)\n",
    "            test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "\n",
    "        sys.stdout.write(\"\\n\" + \\\n",
    "                         \"I:\" + str(j) + \\\n",
    "                         \" Test-Err:\" + str(test_error/ float(len(test_images)))[0:5] +\\\n",
    "                         \" Test-Acc:\" + str(test_correct_cnt/ float(len(test_images)))+\\\n",
    "                         \" Train-Err:\" + str(error/ float(len(images)))[0:5] +\\\n",
    "                         \" Train-Acc:\" + str(correct_cnt/ float(len(images))))\n",
    "```\n",
    "\n",
    "A:The code you posted includes dropout, a regularization technique used to prevent overfitting in neural networks. In this particular code, dropout is implemented through the `dropout_mask`.\r\n",
    "\r\n",
    "Here's the relevant part where dropout is applied:\r\n",
    "\r\n",
    "```python\r\n",
    "dropout_mask = np.random.randint(2, size=layer_1.shape)\r\n",
    "layer_1 *= dropout_mask * 2\r\n",
    "```\r\n",
    "\r\n",
    "1. **`dropout_mask = np.random.randint(2, size=layer_1.shape)`**: Creates a mask where each element is either 0 or 1. The mask has the same shape as `\n",
    "   \r",
    "**<em>Q:\n",
    "what does '2' do in \"dropout_mask = np.random.randint(2, size=layer_1.shape)\"</em>**\n",
    "\n",
    "In the expression `np.random.randint(2, size=layer_1.shape)`, the number `2` specifies the upper bound (exclusive) for the random integers generated by NumPy's `randint` function. \n",
    "Specifically, the function will generate random integers from the \"discrete uniform\" distribution over the interval `[0, 2)`, meaning it will generate either 0 or 1.\r\n",
    "\r\n",
    "Here's a breakdown:\r\n",
    "\r\n",
    "- `np.random.randint(2, size=layer_1.shape)`: This generates an array of random integers with the same shape as `layer_1`, where each integer is either 0 or 1.\r\n",
    "  \r\n",
    "So, in the context of dropout, this line of code is creating a \"mask\" of 0s and 1s with the same shape as the `layer_1` output activations. This mask is then element-wise multiplied with `layer_1`, effectively \"dropping out\" certain neurons by setting their activations to z2ro.?la3er_1`.\r\n",
    "\r\n",
    "2. **`layer_1 *= dropout_mask * 2`**: The `dropout_mask` is applied to `layer_1`. This essentially sets approximately half of the activations in `layer_1` to zero. The remaining activations are scaled by 2 to keep the expected sum of activations the same.\r\n",
    "\r\n",
    "This dropout mask is also later applied to `layer_1_delta`:\r\n",
    "\r\n",
    "```python\r\n",
    "layer_1_delta *= dropout_mask\r\n",
    "```\r\n",
    "\r\n",
    "This ensures that during backpropagation, the neurons that were \"dropped out\" (set to zero) during the forward pass will also have no influence on learning; their corresponding gradients will be zero.\r\n",
    "\r\n",
    "The rest of the code is similar to the previous example you provided. It does forward and backward passes, computes errors and updates weights. The key difference is the inclusion of dropout, which adds a form of regularization to the model.\r\n",
    "\r\n",
    "The dropout is not applied to the test set; it's only used during training. This is standard practice. Dropout is a form of regularization used to make the model generalize better from the training set to unseen data.\r\n",
    "\r\n",
    "So, the code is called 'Dropout code' because it incorporates dropout regularization in the neural network training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b64049c-7264-47c4-aa49-79a1787ddba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Err:61.74 Test-Acc:0.0788 Train-Err:127.7 Train-Acc:0.086\n",
      "I:10 Test-Err:44.66 Test-Acc:0.0893 Train-Err:41.95 Train-Acc:0.089\n",
      "I:20 Test-Err:45.05 Test-Acc:0.0709 Train-Err:39.04 Train-Acc:0.089\n",
      "I:30 Test-Err:45.51 Test-Acc:0.102 Train-Err:37.23 Train-Acc:0.087\n",
      "I:40 Test-Err:45.04 Test-Acc:0.131 Train-Err:35.71 Train-Acc:0.093\n",
      "I:50 Test-Err:44.55 Test-Acc:0.0902 Train-Err:34.18 Train-Acc:0.123\n",
      "I:60 Test-Err:45.74 Test-Acc:0.1013 Train-Err:33.05 Train-Acc:0.109\n",
      "I:70 Test-Err:45.09 Test-Acc:0.1178 Train-Err:33.39 Train-Acc:0.111\n",
      "I:80 Test-Err:45.71 Test-Acc:0.1799 Train-Err:31.28 Train-Acc:0.101\n",
      "I:90 Test-Err:45.15 Test-Acc:0.1088 Train-Err:31.37 Train-Acc:0.154\n",
      "I:100 Test-Err:45.51 Test-Acc:0.2027 Train-Err:29.13 Train-Acc:0.154\n",
      "I:110 Test-Err:45.96 Test-Acc:0.1779 Train-Err:29.89 Train-Acc:0.178\n",
      "I:120 Test-Err:46.80 Test-Acc:0.1543 Train-Err:28.98 Train-Acc:0.161\n",
      "I:130 Test-Err:46.32 Test-Acc:0.0618 Train-Err:28.99 Train-Acc:0.103\n",
      "I:140 Test-Err:48.59 Test-Acc:0.1985 Train-Err:27.58 Train-Acc:0.136\n",
      "I:150 Test-Err:46.48 Test-Acc:0.1539 Train-Err:27.35 Train-Acc:0.223\n",
      "I:160 Test-Err:48.20 Test-Acc:0.2307 Train-Err:27.51 Train-Acc:0.258\n",
      "I:170 Test-Err:49.18 Test-Acc:0.178 Train-Err:25.85 Train-Acc:0.167\n",
      "I:180 Test-Err:48.06 Test-Acc:0.0761 Train-Err:28.45 Train-Acc:0.151\n",
      "I:190 Test-Err:50.07 Test-Acc:0.0656 Train-Err:26.90 Train-Acc:0.172\n",
      "I:200 Test-Err:48.03 Test-Acc:0.0915 Train-Err:26.76 Train-Acc:0.136\n",
      "I:210 Test-Err:48.99 Test-Acc:0.3077 Train-Err:25.46 Train-Acc:0.145\n",
      "I:220 Test-Err:50.53 Test-Acc:0.1383 Train-Err:26.06 Train-Acc:0.168\n",
      "I:230 Test-Err:49.12 Test-Acc:0.0898 Train-Err:25.88 Train-Acc:0.143\n",
      "I:240 Test-Err:50.49 Test-Acc:0.2861 Train-Err:26.57 Train-Acc:0.208\n",
      "I:250 Test-Err:52.30 Test-Acc:0.2059 Train-Err:24.26 Train-Acc:0.15\n",
      "I:260 Test-Err:51.19 Test-Acc:0.202 Train-Err:24.90 Train-Acc:0.154\n",
      "I:270 Test-Err:49.97 Test-Acc:0.2258 Train-Err:26.29 Train-Acc:0.162\n",
      "I:280 Test-Err:52.04 Test-Acc:0.1994 Train-Err:24.84 Train-Acc:0.184\n",
      "I:290 Test-Err:52.26 Test-Acc:0.1202 Train-Err:24.88 Train-Acc:0.148"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocessing the data\n",
    "images, labels = (x_train[0:1000].reshape(1000, 28*28) / 255, y_train[0:1000])\n",
    "test_images, test_labels = (x_test.reshape(len(x_test), 28*28) / 255, y_test)\n",
    "\n",
    "np.random.seed(1)\n",
    "def relu(x):\n",
    "    return (x >= 0) * x # returns x if x > 0\n",
    "                        # returns 0 otherwise\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output >= 0 #returns 1 for input > 0\n",
    "\n",
    "alpha, iterations, hidden_size = (0.0001, 300, 100)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image,hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations): #backprogation\n",
    "    error, correct_cnt = (0.0,0)\n",
    "    for i in range(len(images)):\n",
    "        layer_0 = images[i:i+1]\n",
    "        layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape) #随机关闭一定比例的神经元以防止过拟合\n",
    "        layer_1 *= dropout_mask * 2 #通过乘以2，算法补偿了大约50％的关闭的神经元，使训练时的输出大致与在所有神经元处于活动状态时的预期相当。\n",
    "        layer_2 = layer_1.dot(weights_1_2)\n",
    "\n",
    "        error += np.sum((labels[i:i+1] - layer_2)**2)\n",
    "        correct_cnt += int(np.argmax(layer_2) == np.argmax(labels[i:i+1]))\n",
    "\n",
    "        layer_2_delta = labels[i:i+1] - layer_2\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "        layer_1_delta *= dropout_mask\n",
    "        \n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "    #sys.stdout.write(\"\\r I:\" + str(j) + \" Train-Err:\" + str(error/float(len(images)))[0:5] + \" Train-Acc:\" + str(correct_cnt/float(len(images))))\n",
    "    if(j % 10 == 0):\n",
    "        test_error, test_correct_cnt = (0.0,0)\n",
    "        for i in range(len(test_images)):\n",
    "            layer_0 = test_images[i:i+1]\n",
    "            layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "            layer_2 = layer_1.dot(weights_1_2)\n",
    "\n",
    "            test_error += np.sum((layer_2 - test_labels[i:i+1])**2)\n",
    "            test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "        \n",
    "        sys.stdout.write(\"\\n\" + \\\n",
    "                         \"I:\" + str(j) + \\\n",
    "                         \" Test-Err:\" + str(test_error/ float(len(test_images)))[0:5] + \\\n",
    "                         \" Test-Acc:\" + str(test_correct_cnt/ float(len(test_images))) + \\\n",
    "                         \" Train-Err:\" + str(error/ float(len(images)))[0:5] + \\\n",
    "                         \" Train-Acc:\" + str(correct_cnt/ float(len(images))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b479814b-5292-4c2a-8530-46d5f92a1f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Err:0.912 Test-Acc:0.2563 Train-Err:1.391 Train-Acc:0.124\n",
      "I:10 Test-Err:0.628 Test-Acc:0.6718 Train-Err:0.663 Train-Acc:0.615\n",
      "I:20 Test-Err:0.570 Test-Acc:0.7188 Train-Err:0.594 Train-Acc:0.691\n",
      "I:30 Test-Err:0.537 Test-Acc:0.7453 Train-Err:0.556 Train-Acc:0.703\n",
      "I:40 Test-Err:0.514 Test-Acc:0.7626 Train-Err:0.536 Train-Acc:0.715\n",
      "I:50 Test-Err:0.498 Test-Acc:0.7741 Train-Err:0.515 Train-Acc:0.741\n",
      "I:60 Test-Err:0.485 Test-Acc:0.7866 Train-Err:0.500 Train-Acc:0.755\n",
      "I:70 Test-Err:0.473 Test-Acc:0.7883 Train-Err:0.487 Train-Acc:0.758\n",
      "I:80 Test-Err:0.471 Test-Acc:0.7868 Train-Err:0.487 Train-Acc:0.764\n",
      "I:90 Test-Err:0.464 Test-Acc:0.7903 Train-Err:0.481 Train-Acc:0.774\n",
      "I:100 Test-Err:0.459 Test-Acc:0.7941 Train-Err:0.473 Train-Acc:0.777\n",
      "I:110 Test-Err:0.451 Test-Acc:0.7955 Train-Err:0.458 Train-Acc:0.792\n",
      "I:120 Test-Err:0.449 Test-Acc:0.7977 Train-Err:0.460 Train-Acc:0.788\n",
      "I:130 Test-Err:0.447 Test-Acc:0.8003 Train-Err:0.458 Train-Acc:0.789\n",
      "I:140 Test-Err:0.449 Test-Acc:0.7998 Train-Err:0.460 Train-Acc:0.787\n",
      "I:150 Test-Err:0.446 Test-Acc:0.8012 Train-Err:0.443 Train-Acc:0.801\n",
      "I:160 Test-Err:0.444 Test-Acc:0.8031 Train-Err:0.451 Train-Acc:0.79\n",
      "I:170 Test-Err:0.445 Test-Acc:0.8021 Train-Err:0.445 Train-Acc:0.822\n",
      "I:180 Test-Err:0.442 Test-Acc:0.8007 Train-Err:0.436 Train-Acc:0.817\n",
      "I:190 Test-Err:0.441 Test-Acc:0.8005 Train-Err:0.442 Train-Acc:0.806\n",
      "I:200 Test-Err:0.440 Test-Acc:0.7999 Train-Err:0.446 Train-Acc:0.812\n",
      "I:210 Test-Err:0.440 Test-Acc:0.7981 Train-Err:0.435 Train-Acc:0.817\n",
      "I:220 Test-Err:0.439 Test-Acc:0.8003 Train-Err:0.424 Train-Acc:0.814\n",
      "I:230 Test-Err:0.437 Test-Acc:0.804 Train-Err:0.427 Train-Acc:0.82\n",
      "I:240 Test-Err:0.437 Test-Acc:0.802 Train-Err:0.430 Train-Acc:0.817\n",
      "I:250 Test-Err:0.435 Test-Acc:0.8026 Train-Err:0.413 Train-Acc:0.82\n",
      "I:260 Test-Err:0.436 Test-Acc:0.8059 Train-Err:0.421 Train-Acc:0.825\n",
      "I:270 Test-Err:0.435 Test-Acc:0.8065 Train-Err:0.411 Train-Acc:0.823\n",
      "I:280 Test-Err:0.438 Test-Acc:0.804 Train-Err:0.421 Train-Acc:0.839\n",
      "I:290 Test-Err:0.433 Test-Acc:0.803 Train-Err:0.411 Train-Acc:0.828"
     ]
    }
   ],
   "source": [
    "#edit alpha = 0.0005\n",
    "#add one-hot\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocessing the data\n",
    "images, labels = (x_train[0:1000].reshape(1000, 28*28) / 255, y_train[0:1000])\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels),10))\n",
    "for i,l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test),28*28) / 255\n",
    "test_labels = np.zeros((len(y_test),10))\n",
    "for i,l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "\n",
    "np.random.seed(1)\n",
    "def relu(x):\n",
    "    return (x >= 0) * x # returns x if x > 0\n",
    "                        # returns 0 otherwise\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output >= 0 #returns 1 for input > 0\n",
    "\n",
    "alpha, iterations, hidden_size = (0.0005, 300, 100)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image,hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations): #backprogation\n",
    "    error, correct_cnt = (0.0,0)\n",
    "    for i in range(len(images)):\n",
    "        layer_0 = images[i:i+1]\n",
    "        layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape) #随机关闭一定比例的神经元以防止过拟合\n",
    "        layer_1 *= dropout_mask * 2 #通过乘以2，算法补偿了大约50％的关闭的神经元，使训练时的输出大致与在所有神经元处于活动状态时的预期相当。\n",
    "        layer_2 = layer_1.dot(weights_1_2)\n",
    "\n",
    "        error += np.sum((labels[i:i+1] - layer_2)**2)\n",
    "        correct_cnt += int(np.argmax(layer_2) == np.argmax(labels[i:i+1]))\n",
    "\n",
    "        layer_2_delta = labels[i:i+1] - layer_2\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "        layer_1_delta *= dropout_mask\n",
    "        \n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "    #sys.stdout.write(\"\\r I:\" + str(j) + \" Train-Err:\" + str(error/float(len(images)))[0:5] + \" Train-Acc:\" + str(correct_cnt/float(len(images))))\n",
    "    if(j % 10 == 0):\n",
    "        test_error, test_correct_cnt = (0.0,0)\n",
    "        for i in range(len(test_images)):\n",
    "            layer_0 = test_images[i:i+1]\n",
    "            layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "            layer_2 = layer_1.dot(weights_1_2)\n",
    "\n",
    "            test_error += np.sum((layer_2 - test_labels[i:i+1])**2)\n",
    "            test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "        \n",
    "        sys.stdout.write(\"\\n\" + \\\n",
    "                         \"I:\" + str(j) + \\\n",
    "                         \" Test-Err:\" + str(test_error/ float(len(test_images)))[0:5] + \\\n",
    "                         \" Test-Acc:\" + str(test_correct_cnt/ float(len(test_images))) + \\\n",
    "                         \" Train-Err:\" + str(error/ float(len(images)))[0:5] + \\\n",
    "                         \" Train-Acc:\" + str(correct_cnt/ float(len(images))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945706f4-79fe-4d31-88ce-b7dc1110b872",
   "metadata": {},
   "source": [
    "**<h2>Batch Gradient Descent</h2>**\n",
    "\n",
    "In the code you've provided, the concept of `batch_size` plays a pivotal role in the training of the neural network. The `batch_size` defines how many samples from the training dataset are used in each iteration to update the weights. It's set to 100, meaning in each training step, 100 images are processed before a weight update occurs. Let's break down how it's used in the code:\r\n",
    "\r\n",
    "1. `batch_size = 100`: This sets the batch size to 100.\r\n",
    "\r\n",
    "2. `for i in range(int(len(images) / batch_size)):` This loop iterates through the training set in chunks of `batch_size`. For example, if there are 5000 images, this loop would run 50 times (5000/100).\r\n",
    "\r\n",
    "3. `batch_start, batch_end = ((i * batch_size),((i+1)*batch_size))`: These lines calculate the start and end indices for the current batch of images and labels. \r\n",
    "\r\n",
    "4. `layer_0 = images[batch_start:batch_end]`: This line selects a batch of images (100 images, as per the defined `batch_size`) from the dataset.\r\n",
    "\r\n",
    "5. `labels[batch_start:batch_end]`: Similarly, the labels corresponding to the current batch of images are selected.\r\n",
    "\r\n",
    "6. `error += np.sum((labels[batch_start:batch_end] - layer_2) ** 2)`: The error for the current batch is calculated and summed up with the error from the previous batches. The error is normalized later by the total number of images (`float(len(images))`).\r\n",
    "\r\n",
    "7. `for k in range(batch_size):`: This loop iterates through each example in the current batch for further calculations, like counting the number of correctly classified examples.\r\n",
    "\r\n",
    "8. `layer_2_delta = (labels[batch_start:batch_end]-layer_2)/batch_size`: The error gradient (`layer_2_delta`) is scaled by dividing it by `batch_size`, which is a common technique to average the gradient across the batch.\r\n",
    "\r\n",
    "9. Later in the code, the weights (`weights_1_2` and `weights_0_1`) are updated based on these batch calculations.\r\n",
    "\r\n",
    "In summary, `batch_size` is crucial for breaking the dataset into manageable chunks that can be used for training the network in each iteration. It's an essential hyperparameter that can affect both the efficiency and performance of a neural network. Given your interest in machine learning, understanding the role of batching and its impact on training dynamics could be particularly valuable for your research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7ec8c3d-49c3-4c5b-964c-96ebcf657bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Err:0.910 Test-Acc:0.2586 Train-Err:1.428 Train-Acc:0.123\n",
      "I:10 Test-Err:0.627 Test-Acc:0.6715 Train-Err:0.663 Train-Acc:0.618\n",
      "I:20 Test-Err:0.569 Test-Acc:0.7189 Train-Err:0.594 Train-Acc:0.692\n",
      "I:30 Test-Err:0.537 Test-Acc:0.7466 Train-Err:0.556 Train-Acc:0.704\n",
      "I:40 Test-Err:0.514 Test-Acc:0.7631 Train-Err:0.536 Train-Acc:0.718\n",
      "I:50 Test-Err:0.498 Test-Acc:0.7732 Train-Err:0.516 Train-Acc:0.738\n",
      "I:60 Test-Err:0.486 Test-Acc:0.7857 Train-Err:0.501 Train-Acc:0.755\n",
      "I:70 Test-Err:0.474 Test-Acc:0.7885 Train-Err:0.488 Train-Acc:0.755\n",
      "I:80 Test-Err:0.473 Test-Acc:0.7858 Train-Err:0.488 Train-Acc:0.761\n",
      "I:90 Test-Err:0.466 Test-Acc:0.7892 Train-Err:0.482 Train-Acc:0.774\n",
      "I:100 Test-Err:0.461 Test-Acc:0.7932 Train-Err:0.475 Train-Acc:0.781\n",
      "I:110 Test-Err:0.453 Test-Acc:0.7948 Train-Err:0.460 Train-Acc:0.79\n",
      "I:120 Test-Err:0.452 Test-Acc:0.7961 Train-Err:0.462 Train-Acc:0.789\n",
      "I:130 Test-Err:0.449 Test-Acc:0.7985 Train-Err:0.460 Train-Acc:0.788\n",
      "I:140 Test-Err:0.452 Test-Acc:0.7972 Train-Err:0.463 Train-Acc:0.783\n",
      "I:150 Test-Err:0.449 Test-Acc:0.7954 Train-Err:0.446 Train-Acc:0.803\n",
      "I:160 Test-Err:0.446 Test-Acc:0.8027 Train-Err:0.453 Train-Acc:0.788\n",
      "I:170 Test-Err:0.448 Test-Acc:0.8005 Train-Err:0.448 Train-Acc:0.82\n",
      "I:180 Test-Err:0.445 Test-Acc:0.7996 Train-Err:0.439 Train-Acc:0.817\n",
      "I:190 Test-Err:0.444 Test-Acc:0.8018 Train-Err:0.445 Train-Acc:0.81\n",
      "I:200 Test-Err:0.444 Test-Acc:0.8001 Train-Err:0.448 Train-Acc:0.807\n",
      "I:210 Test-Err:0.443 Test-Acc:0.7996 Train-Err:0.439 Train-Acc:0.822\n",
      "I:220 Test-Err:0.443 Test-Acc:0.8008 Train-Err:0.428 Train-Acc:0.812\n",
      "I:230 Test-Err:0.440 Test-Acc:0.8047 Train-Err:0.431 Train-Acc:0.813\n",
      "I:240 Test-Err:0.441 Test-Acc:0.8023 Train-Err:0.434 Train-Acc:0.811\n",
      "I:250 Test-Err:0.438 Test-Acc:0.8051 Train-Err:0.416 Train-Acc:0.816\n",
      "I:260 Test-Err:0.440 Test-Acc:0.8068 Train-Err:0.424 Train-Acc:0.817\n",
      "I:270 Test-Err:0.438 Test-Acc:0.8064 Train-Err:0.414 Train-Acc:0.825\n",
      "I:280 Test-Err:0.441 Test-Acc:0.8011 Train-Err:0.426 Train-Acc:0.836\n",
      "I:290 Test-Err:0.437 Test-Acc:0.8014 Train-Err:0.412 Train-Acc:0.834"
     ]
    }
   ],
   "source": [
    "#edit alpha = 0.0005\n",
    "#add one-hot\n",
    "#add batch size\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocessing the data\n",
    "images, labels = (x_train[0:1000].reshape(1000, 28*28) / 255, y_train[0:1000])\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels),10))\n",
    "for i,l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test),28*28) / 255\n",
    "test_labels = np.zeros((len(y_test),10))\n",
    "for i,l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "\n",
    "np.random.seed(1)\n",
    "def relu(x):\n",
    "    return (x >= 0) * x # returns x if x > 0\n",
    "                        # returns 0 otherwise\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output >= 0 #returns 1 for input > 0\n",
    "\n",
    "batch_size = 100\n",
    "alpha, iterations, hidden_size = (0.0005, 300, 100)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image,hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations): #backprogation\n",
    "    error, correct_cnt = (0.0,0)\n",
    "    for i in range(int(len(images) / batch_size )):\n",
    "        batch_start, batch_end = ((i * batch_size),((i+1) * batch_size))\n",
    "        \n",
    "        layer_0 = images[batch_start:batch_end]\n",
    "        layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape) #随机关闭一定比例的神经元以防止过拟合\n",
    "        layer_1 *= dropout_mask * 2 #通过乘以2，算法补偿了大约50％的关闭的神经元，使训练时的输出大致与在所有神经元处于活动状态时的预期相当。\n",
    "        layer_2 = layer_1.dot(weights_1_2)\n",
    "\n",
    "        error += np.sum((labels[batch_start:batch_end] - layer_2)**2)\n",
    "        for k in range(batch_size):\n",
    "            correct_cnt += int(np.argmax(layer_2[k:k+1]) == np.argmax(labels[batch_start+k:batch_start+k+1]))\n",
    "\n",
    "            layer_2_delta = (labels[batch_start:batch_end] - layer_2)/batch_size\n",
    "            layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "            layer_1_delta *= dropout_mask\n",
    "        \n",
    "            weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "            weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "    if(j % 10 == 0):\n",
    "        test_error, test_correct_cnt = (0.0,0)\n",
    "        for i in range(len(test_images)):\n",
    "            layer_0 = test_images[i:i+1]\n",
    "            layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "            layer_2 = layer_1.dot(weights_1_2)\n",
    "\n",
    "            test_error += np.sum((layer_2 - test_labels[i:i+1])**2)\n",
    "            test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "        \n",
    "        sys.stdout.write(\"\\n\" + \\\n",
    "                         \"I:\" + str(j) + \\\n",
    "                         \" Test-Err:\" + str(test_error/ float(len(test_images)))[0:5] + \\\n",
    "                         \" Test-Acc:\" + str(test_correct_cnt/ float(len(test_images))) + \\\n",
    "                         \" Train-Err:\" + str(error/ float(len(images)))[0:5] + \\\n",
    "                         \" Train-Acc:\" + str(correct_cnt/ float(len(images))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36cf48d-229f-4b34-93e8-7198d1ff4f40",
   "metadata": {},
   "source": [
    "The output provides a summary of the model's performance at regular intervals (every 10 iterations, as per your code). The key statistics include:\n",
    "\n",
    "- **Train-Err and Test-Err**: These are the training and test errors, respectively. Lower values are generally better, and it's good to see these values decreasing over time.\n",
    "- **Train-Acc and Test-Acc**: These are the training and test accuracies, respectively. Higher values are better, and you want to see these numbers going up over iterations.\n",
    "\n",
    "### Observations\n",
    "\n",
    "1. **Training Error vs. Testing Error**: Both training and test errors are generally decreasing, which is a good sign. It suggests the model is learning effectively from the data.\n",
    "  \n",
    "2. **Training Accuracy vs. Testing Accuracy**: Both are increasing as well, which is positive. The test accuracy is higher than the training accuracy, which is somewhat unusual but not a problem. \n",
    "\n",
    "3. **Convergence**: The model seems to be converging as the changes in errors and accuracies are becoming smaller.\n",
    "\n",
    "4. **Overfitting**: There is no clear sign of overfitting here, as the test error continues to decrease and test accuracy continues to increase.\n",
    "\n",
    "### Potential Action Points:\n",
    "\n",
    "1. **Learning Rate**: The learning rate `alpha = 0.0005` is a hyperparameter you might want to experiment with. Different learning rates might lead to faster convergence or better generalization.\n",
    "\n",
    "2. **Batch Size**: Since you have a powerful GPU, you could experiment with different batch sizes to see how it affects the training dynamics.\n",
    "\n",
    "3. **Dropout**: You implemented dropout to combat overfitting, which is a good strategy. You could try varying the dropout rate and observe its effects on overfitting.\n",
    "\n",
    "4. **More Hidden Layers**: With just one hidden layer, the model is quite simple. Given your computational resources and academic background, it might be worth trying a deeper model to see how it performs.\n",
    "\n",
    "5. **Iterations**: The number of iterations is set to 300. Depending on the final metrics, you might want to increase/decrease this number.\n",
    "\n",
    "Overall, the code and its output suggest that the model is performing reasonably well, but there's room for further optimization and experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6ca447f-f16f-4763-8be7-5ec6ed2173d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Err:1.118 Test-Acc:0.1123 Train-Err:1.767 Train-Acc:0.085\n",
      "I:10 Test-Err:0.802 Test-Acc:0.4154 Train-Err:0.958 Train-Acc:0.286\n",
      "I:20 Test-Err:0.720 Test-Acc:0.5481 Train-Err:0.802 Train-Acc:0.448\n",
      "I:30 Test-Err:0.679 Test-Acc:0.6122 Train-Err:0.741 Train-Acc:0.521\n",
      "I:40 Test-Err:0.653 Test-Acc:0.6472 Train-Err:0.706 Train-Acc:0.569\n",
      "I:50 Test-Err:0.634 Test-Acc:0.6677 Train-Err:0.674 Train-Acc:0.606\n",
      "I:60 Test-Err:0.618 Test-Acc:0.6839 Train-Err:0.656 Train-Acc:0.627\n",
      "I:70 Test-Err:0.604 Test-Acc:0.6944 Train-Err:0.628 Train-Acc:0.664\n",
      "I:80 Test-Err:0.593 Test-Acc:0.7018 Train-Err:0.621 Train-Acc:0.663\n",
      "I:90 Test-Err:0.583 Test-Acc:0.7111 Train-Err:0.609 Train-Acc:0.671\n",
      "I:100 Test-Err:0.575 Test-Acc:0.7172 Train-Err:0.608 Train-Acc:0.671\n",
      "I:110 Test-Err:0.566 Test-Acc:0.7237 Train-Err:0.587 Train-Acc:0.687\n",
      "I:120 Test-Err:0.559 Test-Acc:0.7284 Train-Err:0.584 Train-Acc:0.704\n",
      "I:130 Test-Err:0.552 Test-Acc:0.7336 Train-Err:0.578 Train-Acc:0.704\n",
      "I:140 Test-Err:0.546 Test-Acc:0.7393 Train-Err:0.568 Train-Acc:0.696\n",
      "I:150 Test-Err:0.539 Test-Acc:0.7441 Train-Err:0.557 Train-Acc:0.704\n",
      "I:160 Test-Err:0.534 Test-Acc:0.7492 Train-Err:0.558 Train-Acc:0.708\n",
      "I:170 Test-Err:0.528 Test-Acc:0.7522 Train-Err:0.543 Train-Acc:0.737\n",
      "I:180 Test-Err:0.523 Test-Acc:0.755 Train-Err:0.544 Train-Acc:0.718\n",
      "I:190 Test-Err:0.519 Test-Acc:0.758 Train-Err:0.537 Train-Acc:0.718\n",
      "I:200 Test-Err:0.514 Test-Acc:0.7612 Train-Err:0.535 Train-Acc:0.719\n",
      "I:210 Test-Err:0.511 Test-Acc:0.7625 Train-Err:0.530 Train-Acc:0.727\n",
      "I:220 Test-Err:0.507 Test-Acc:0.7639 Train-Err:0.524 Train-Acc:0.72\n",
      "I:230 Test-Err:0.504 Test-Acc:0.7667 Train-Err:0.519 Train-Acc:0.746\n",
      "I:240 Test-Err:0.500 Test-Acc:0.7681 Train-Err:0.521 Train-Acc:0.734\n",
      "I:250 Test-Err:0.497 Test-Acc:0.7714 Train-Err:0.512 Train-Acc:0.741\n",
      "I:260 Test-Err:0.495 Test-Acc:0.7724 Train-Err:0.515 Train-Acc:0.732\n",
      "I:270 Test-Err:0.491 Test-Acc:0.7764 Train-Err:0.504 Train-Acc:0.76\n",
      "I:280 Test-Err:0.490 Test-Acc:0.7765 Train-Err:0.513 Train-Acc:0.758\n",
      "I:290 Test-Err:0.487 Test-Acc:0.7803 Train-Err:0.502 Train-Acc:0.771"
     ]
    }
   ],
   "source": [
    "#edit alpha = 0.0001\n",
    "#add one-hot\n",
    "#add batch size\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocessing the data\n",
    "images, labels = (x_train[0:1000].reshape(1000, 28*28) / 255, y_train[0:1000])\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels),10))\n",
    "for i,l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test),28*28) / 255\n",
    "test_labels = np.zeros((len(y_test),10))\n",
    "for i,l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "\n",
    "np.random.seed(1)\n",
    "def relu(x):\n",
    "    return (x >= 0) * x # returns x if x > 0\n",
    "                        # returns 0 otherwise\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output >= 0 #returns 1 for input > 0\n",
    "\n",
    "batch_size = 100\n",
    "alpha, iterations, hidden_size = (0.0001, 300, 100)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image,hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations): #backprogation\n",
    "    error, correct_cnt = (0.0,0)\n",
    "    for i in range(int(len(images) / batch_size )):\n",
    "        batch_start, batch_end = ((i * batch_size),((i+1) * batch_size))\n",
    "        \n",
    "        layer_0 = images[batch_start:batch_end]\n",
    "        layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape) #随机关闭一定比例的神经元以防止过拟合\n",
    "        layer_1 *= dropout_mask * 2 #通过乘以2，算法补偿了大约50％的关闭的神经元，使训练时的输出大致与在所有神经元处于活动状态时的预期相当。\n",
    "        layer_2 = layer_1.dot(weights_1_2)\n",
    "\n",
    "        error += np.sum((labels[batch_start:batch_end] - layer_2)**2)\n",
    "        for k in range(batch_size):\n",
    "            correct_cnt += int(np.argmax(layer_2[k:k+1]) == np.argmax(labels[batch_start+k:batch_start+k+1]))\n",
    "\n",
    "            layer_2_delta = (labels[batch_start:batch_end] - layer_2)/batch_size\n",
    "            layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "            layer_1_delta *= dropout_mask\n",
    "        \n",
    "            weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "            weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "    if(j % 10 == 0):\n",
    "        test_error, test_correct_cnt = (0.0,0)\n",
    "        for i in range(len(test_images)):\n",
    "            layer_0 = test_images[i:i+1]\n",
    "            layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "            layer_2 = layer_1.dot(weights_1_2)\n",
    "\n",
    "            test_error += np.sum((layer_2 - test_labels[i:i+1])**2)\n",
    "            test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "        \n",
    "        sys.stdout.write(\"\\n\" + \\\n",
    "                         \"I:\" + str(j) + \\\n",
    "                         \" Test-Err:\" + str(test_error/ float(len(test_images)))[0:5] + \\\n",
    "                         \" Test-Acc:\" + str(test_correct_cnt/ float(len(test_images))) + \\\n",
    "                         \" Train-Err:\" + str(error/ float(len(images)))[0:5] + \\\n",
    "                         \" Train-Acc:\" + str(correct_cnt/ float(len(images))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8341df-5dda-47c9-bfed-fbca89e826cb",
   "metadata": {},
   "source": [
    "**<em>All final output drop down</em>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ae8d8f3-9caf-4eb4-99a2-69e480e4be42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Err:1.091 Test-Acc:0.1178 Train-Err:1.882 Train-Acc:0.085\n",
      "I:10 Test-Err:0.800 Test-Acc:0.4166 Train-Err:0.959 Train-Acc:0.285\n",
      "I:20 Test-Err:0.719 Test-Acc:0.5501 Train-Err:0.801 Train-Acc:0.452\n",
      "I:30 Test-Err:0.678 Test-Acc:0.6135 Train-Err:0.741 Train-Acc:0.519\n",
      "I:40 Test-Err:0.652 Test-Acc:0.6482 Train-Err:0.706 Train-Acc:0.569\n",
      "I:50 Test-Err:0.633 Test-Acc:0.6682 Train-Err:0.674 Train-Acc:0.606\n",
      "I:60 Test-Err:0.617 Test-Acc:0.6845 Train-Err:0.655 Train-Acc:0.625\n",
      "I:70 Test-Err:0.604 Test-Acc:0.6949 Train-Err:0.628 Train-Acc:0.664\n",
      "I:80 Test-Err:0.593 Test-Acc:0.7022 Train-Err:0.621 Train-Acc:0.661\n",
      "I:90 Test-Err:0.583 Test-Acc:0.7115 Train-Err:0.609 Train-Acc:0.672\n",
      "I:100 Test-Err:0.575 Test-Acc:0.7175 Train-Err:0.608 Train-Acc:0.672\n",
      "I:110 Test-Err:0.566 Test-Acc:0.7238 Train-Err:0.587 Train-Acc:0.688\n",
      "I:120 Test-Err:0.559 Test-Acc:0.7286 Train-Err:0.583 Train-Acc:0.703\n",
      "I:130 Test-Err:0.552 Test-Acc:0.7341 Train-Err:0.578 Train-Acc:0.706\n",
      "I:140 Test-Err:0.545 Test-Acc:0.7397 Train-Err:0.568 Train-Acc:0.697\n",
      "I:150 Test-Err:0.538 Test-Acc:0.7443 Train-Err:0.557 Train-Acc:0.704\n",
      "I:160 Test-Err:0.534 Test-Acc:0.7491 Train-Err:0.558 Train-Acc:0.708\n",
      "I:170 Test-Err:0.528 Test-Acc:0.7524 Train-Err:0.543 Train-Acc:0.735\n",
      "I:180 Test-Err:0.523 Test-Acc:0.755 Train-Err:0.544 Train-Acc:0.719\n",
      "I:190 Test-Err:0.518 Test-Acc:0.7584 Train-Err:0.537 Train-Acc:0.721\n",
      "I:200 Test-Err:0.514 Test-Acc:0.7619 Train-Err:0.535 Train-Acc:0.718\n",
      "I:210 Test-Err:0.511 Test-Acc:0.7626 Train-Err:0.530 Train-Acc:0.725\n",
      "I:220 Test-Err:0.507 Test-Acc:0.7636 Train-Err:0.524 Train-Acc:0.722\n",
      "I:230 Test-Err:0.503 Test-Acc:0.767 Train-Err:0.519 Train-Acc:0.747\n",
      "I:240 Test-Err:0.500 Test-Acc:0.7676 Train-Err:0.521 Train-Acc:0.737\n",
      "I:250 Test-Err:0.497 Test-Acc:0.7719 Train-Err:0.512 Train-Acc:0.74\n",
      "I:260 Test-Err:0.495 Test-Acc:0.7728 Train-Err:0.515 Train-Acc:0.732\n",
      "I:270 Test-Err:0.491 Test-Acc:0.7768 Train-Err:0.503 Train-Acc:0.761\n",
      "I:280 Test-Err:0.490 Test-Acc:0.7764 Train-Err:0.513 Train-Acc:0.754\n",
      "I:290 Test-Err:0.487 Test-Acc:0.7803 Train-Err:0.502 Train-Acc:0.77"
     ]
    }
   ],
   "source": [
    "#edit alpha = 0.0001\n",
    "#add one-hot\n",
    "#edit batch size = 500\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocessing the data\n",
    "images, labels = (x_train[0:1000].reshape(1000, 28*28) / 255, y_train[0:1000])\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels),10))\n",
    "for i,l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test),28*28) / 255\n",
    "test_labels = np.zeros((len(y_test),10))\n",
    "for i,l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "\n",
    "np.random.seed(1)\n",
    "def relu(x):\n",
    "    return (x >= 0) * x # returns x if x > 0\n",
    "                        # returns 0 otherwise\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output >= 0 #returns 1 for input > 0\n",
    "\n",
    "batch_size = 500\n",
    "alpha, iterations, hidden_size = (0.0001, 300, 100)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image,hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations): #backprogation\n",
    "    error, correct_cnt = (0.0,0)\n",
    "    for i in range(int(len(images) / batch_size )):\n",
    "        batch_start, batch_end = ((i * batch_size),((i+1) * batch_size))\n",
    "        \n",
    "        layer_0 = images[batch_start:batch_end]\n",
    "        layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape) #随机关闭一定比例的神经元以防止过拟合\n",
    "        layer_1 *= dropout_mask * 2 #通过乘以2，算法补偿了大约50％的关闭的神经元，使训练时的输出大致与在所有神经元处于活动状态时的预期相当。\n",
    "        layer_2 = layer_1.dot(weights_1_2)\n",
    "\n",
    "        error += np.sum((labels[batch_start:batch_end] - layer_2)**2)\n",
    "        for k in range(batch_size):\n",
    "            correct_cnt += int(np.argmax(layer_2[k:k+1]) == np.argmax(labels[batch_start+k:batch_start+k+1]))\n",
    "\n",
    "            layer_2_delta = (labels[batch_start:batch_end] - layer_2)/batch_size\n",
    "            layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "            layer_1_delta *= dropout_mask\n",
    "        \n",
    "            weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "            weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "    if(j % 10 == 0):\n",
    "        test_error, test_correct_cnt = (0.0,0)\n",
    "        for i in range(len(test_images)):\n",
    "            layer_0 = test_images[i:i+1]\n",
    "            layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "            layer_2 = layer_1.dot(weights_1_2)\n",
    "\n",
    "            test_error += np.sum((layer_2 - test_labels[i:i+1])**2)\n",
    "            test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "        \n",
    "        sys.stdout.write(\"\\n\" + \\\n",
    "                         \"I:\" + str(j) + \\\n",
    "                         \" Test-Err:\" + str(test_error/ float(len(test_images)))[0:5] + \\\n",
    "                         \" Test-Acc:\" + str(test_correct_cnt/ float(len(test_images))) + \\\n",
    "                         \" Train-Err:\" + str(error/ float(len(images)))[0:5] + \\\n",
    "                         \" Train-Acc:\" + str(correct_cnt/ float(len(images))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25449c7b-7d32-4492-ad1b-5bb518c4fc7a",
   "metadata": {},
   "source": [
    "**<em>The same output with the previous one, so batch size does not change result.</em>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b40cf23-836e-446c-a9ed-a5abcd02de1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Err:0.602 Test-Acc:0.7181 Train-Err:0.849 Train-Acc:0.4297\n",
      "I:10 Test-Err:0.407 Test-Acc:0.8362 Train-Err:0.501 Train-Acc:0.7458\n",
      "I:20 Test-Err:0.392 Test-Acc:0.8367 Train-Err:0.481 Train-Acc:0.7615666666666666\n",
      "I:30 Test-Err:0.379 Test-Acc:0.8391 Train-Err:0.470 Train-Acc:0.7710166666666667\n",
      "I:40 Test-Err:0.363 Test-Acc:0.8427 Train-Err:0.454 Train-Acc:0.7833666666666667\n",
      "I:50 Test-Err:0.352 Test-Acc:0.8483 Train-Err:0.443 Train-Acc:0.79275\n",
      "I:60 Test-Err:0.346 Test-Acc:0.8502 Train-Err:0.437 Train-Acc:0.7959333333333334\n",
      "I:70 Test-Err:0.341 Test-Acc:0.8517 Train-Err:0.431 Train-Acc:0.7989166666666667\n",
      "I:80 Test-Err:0.337 Test-Acc:0.8531 Train-Err:0.426 Train-Acc:0.8039166666666666\n",
      "I:90 Test-Err:0.331 Test-Acc:0.8559 Train-Err:0.423 Train-Acc:0.8046833333333333\n",
      "I:100 Test-Err:0.327 Test-Acc:0.8624 Train-Err:0.420 Train-Acc:0.8083666666666667\n",
      "I:110 Test-Err:0.323 Test-Acc:0.862 Train-Err:0.413 Train-Acc:0.8127333333333333\n",
      "I:120 Test-Err:0.323 Test-Acc:0.8617 Train-Err:0.412 Train-Acc:0.8151833333333334\n",
      "I:130 Test-Err:0.321 Test-Acc:0.8623 Train-Err:0.410 Train-Acc:0.8151833333333334\n",
      "I:140 Test-Err:0.317 Test-Acc:0.863 Train-Err:0.409 Train-Acc:0.8158\n",
      "I:150 Test-Err:0.317 Test-Acc:0.8634 Train-Err:0.407 Train-Acc:0.8187\n",
      "I:160 Test-Err:0.316 Test-Acc:0.8646 Train-Err:0.406 Train-Acc:0.8184166666666667\n",
      "I:170 Test-Err:0.314 Test-Acc:0.8654 Train-Err:0.405 Train-Acc:0.8181\n",
      "I:180 Test-Err:0.312 Test-Acc:0.8653 Train-Err:0.403 Train-Acc:0.8171833333333334\n",
      "I:190 Test-Err:0.309 Test-Acc:0.8664 Train-Err:0.401 Train-Acc:0.8184166666666667\n",
      "I:200 Test-Err:0.309 Test-Acc:0.8661 Train-Err:0.400 Train-Acc:0.8211666666666667\n",
      "I:210 Test-Err:0.307 Test-Acc:0.8682 Train-Err:0.399 Train-Acc:0.8204666666666667\n",
      "I:220 Test-Err:0.303 Test-Acc:0.8688 Train-Err:0.397 Train-Acc:0.81995\n",
      "I:230 Test-Err:0.306 Test-Acc:0.8691 Train-Err:0.396 Train-Acc:0.8229666666666666\n",
      "I:240 Test-Err:0.305 Test-Acc:0.8693 Train-Err:0.396 Train-Acc:0.8231166666666667\n",
      "I:250 Test-Err:0.304 Test-Acc:0.8675 Train-Err:0.397 Train-Acc:0.8212666666666667\n",
      "I:260 Test-Err:0.307 Test-Acc:0.8668 Train-Err:0.397 Train-Acc:0.8217333333333333\n",
      "I:270 Test-Err:0.308 Test-Acc:0.8673 Train-Err:0.397 Train-Acc:0.82205\n",
      "I:280 Test-Err:0.309 Test-Acc:0.8668 Train-Err:0.396 Train-Acc:0.8222\n",
      "I:290 Test-Err:0.306 Test-Acc:0.8688 Train-Err:0.396 Train-Acc:0.8232666666666667"
     ]
    }
   ],
   "source": [
    "#edit alpha = 0.0001\n",
    "#add one-hot\n",
    "#edit batch size = 100\n",
    "#use whole dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocessing the data\n",
    "images, labels = (x_train.reshape(60000, 28*28) / 255, y_train)\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels),10))\n",
    "for i,l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test),28*28) / 255\n",
    "test_labels = np.zeros((len(y_test),10))\n",
    "for i,l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "\n",
    "np.random.seed(1)\n",
    "def relu(x):\n",
    "    return (x >= 0) * x # returns x if x > 0\n",
    "                        # returns 0 otherwise\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output >= 0 #returns 1 for input > 0\n",
    "\n",
    "batch_size = 100\n",
    "alpha, iterations, hidden_size = (0.0001, 300, 100)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image,hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations): #backprogation\n",
    "    error, correct_cnt = (0.0,0)\n",
    "    for i in range(int(len(images) / batch_size )):\n",
    "        batch_start, batch_end = ((i * batch_size),((i+1) * batch_size))\n",
    "        \n",
    "        layer_0 = images[batch_start:batch_end]\n",
    "        layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape) #随机关闭一定比例的神经元以防止过拟合\n",
    "        layer_1 *= dropout_mask * 2 #通过乘以2，算法补偿了大约50％的关闭的神经元，使训练时的输出大致与在所有神经元处于活动状态时的预期相当。\n",
    "        layer_2 = layer_1.dot(weights_1_2)\n",
    "\n",
    "        error += np.sum((labels[batch_start:batch_end] - layer_2)**2)\n",
    "        for k in range(batch_size):\n",
    "            correct_cnt += int(np.argmax(layer_2[k:k+1]) == np.argmax(labels[batch_start+k:batch_start+k+1]))\n",
    "\n",
    "            layer_2_delta = (labels[batch_start:batch_end] - layer_2)/batch_size\n",
    "            layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "            layer_1_delta *= dropout_mask\n",
    "        \n",
    "            weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "            weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "    if(j % 10 == 0):\n",
    "        test_error, test_correct_cnt = (0.0,0)\n",
    "        for i in range(len(test_images)):\n",
    "            layer_0 = test_images[i:i+1]\n",
    "            layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "            layer_2 = layer_1.dot(weights_1_2)\n",
    "\n",
    "            test_error += np.sum((layer_2 - test_labels[i:i+1])**2)\n",
    "            test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "        \n",
    "        sys.stdout.write(\"\\n\" + \\\n",
    "                         \"I:\" + str(j) + \\\n",
    "                         \" Test-Err:\" + str(test_error/ float(len(test_images)))[0:5] + \\\n",
    "                         \" Test-Acc:\" + str(test_correct_cnt/ float(len(test_images))) + \\\n",
    "                         \" Train-Err:\" + str(error/ float(len(images)))[0:5] + \\\n",
    "                         \" Train-Acc:\" + str(correct_cnt/ float(len(images))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce9ae9-8cdb-46a9-9f49-d20e72f3a461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
