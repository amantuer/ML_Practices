{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3552aa6-e72d-411b-a18a-1eeff216e444",
   "metadata": {},
   "source": [
    "**<h2>Chapter10 - Intro to Convolutional Neural Networks - Learning Edges and Corners.ipynb</h2>**\n",
    "\n",
    "![Deep Learning Algorithm Structure](https://showme.redstarplugin.com/d/d:MvQXLjhj)\n",
    "\n",
    "Certainly! This code is implementing a basic CNN (Convolutional Neural Network) in NumPy. CNNs are especially useful for image-related tasks. The following parts are new compared to the previous version of the code and are relevant to the implementation of a CNN:\n",
    "\n",
    "### Kernel Initialization\n",
    "\n",
    "```python\n",
    "kernel_rows = 3\n",
    "kernel_cols = 3\n",
    "num_kernels = 16\n",
    "```\n",
    "\n",
    "Here, you initialize the kernels (also known as filters). Each kernel is a 3x3 matrix, and there are 16 such kernels. These kernels are what will \"slide over\" the input image to produce feature maps.\n",
    "\n",
    "\n",
    "```python\n",
    "hidden_size = ((input_rows - kernel_rows) *  (input_cols - kernel_cols)) * num_kernels\n",
    "```\n",
    "\n",
    "\n",
    "hidden_size变量用于定义卷积操作后的后续层的维度。让我们分解一下为什么要这样计算：\n",
    "\n",
    "卷积图像的空间维度：当您用3x3内核卷积28x28图像时，输出特征图的尺寸将为（28-3 +1）x（28-3 +1）= 26x26。这是一种计算卷积层输出尺寸的基本公式，不带填充和步幅：O =（W-K +1）x（H-K +1），其中W和H是输入层的宽度和高度，K是内核的大小。在您的情况下，宽度和高度都减小到26。\n",
    "\n",
    "多个内核：该代码使用16个不同的内核（num_kernels设置为16）。当与输入图像卷积时，每个内核都会产生自己的26x26特征图。\n",
    "\n",
    "展平输出：为了将这个卷积层连接到后续的全连接层，我们通常会展平输出。每个26x26输出将被展平为大小为676（26 x 26 = 676）的1D数组。\n",
    "\n",
    "连接多个展平输出：由于有16个这样的展平输出（每个内核一个），因此总大小将为16 x 676 = 10816。\n",
    "\n",
    "因此，hidden_size的计算方式为（（input_rows-kernel_rows）（input_cols-kernel_cols）） num_kernels =（26 * 26）* 16 = 10816。这将是通过完全连接层之前卷积并通过的第一层中每个特征向量的大小。\n",
    "\n",
    "The `hidden_size` variable is used to define the dimensions of the subsequent layers after the convolutional operation. Let's break down why it is calculated the way it is:\n",
    "\n",
    "1. **Spatial Dimensions of Convolved Image**: When you convolve a 28x28 image with a 3x3 kernel, the dimensions of the output feature map would be `(28 - 3 + 1) x (28 - 3 + 1)` = 26x26. This is a basic formula for calculating the dimensions of the output of a convolutional layer without padding and stride: `O = (W - K + 1) x (H - K + 1)`, where `W` and `H` are the width and height of the input layer, and `K` is the size of the kernel. In your case, both the width and the height are reduced to 26.\n",
    "\n",
    "2. **Multiple Kernels**: The code uses 16 different kernels (`num_kernels` is set to 16). Each of these kernels will produce its own 26x26 feature map when convolved with the input image.\n",
    "\n",
    "3. **Flattening the Output**: To connect this convolved layer to subsequent fully-connected layers, we usually flatten the output. Each 26x26 output would be flattened to a 1D array of size 676 (`26 x 26 = 676`).\n",
    "\n",
    "4. **Concatenating Multiple Flattened Outputs**: Since there are 16 such flattened outputs (one for each kernel), the overall size would be `16 x 676 = 10816`.\n",
    "\n",
    "\n",
    "Therefore, the `hidden_size` is calculated as `((input_rows - kernel_rows) * (input_cols - kernel_cols)) * num_kernels = (26 * 26) * 16 = 10816`. This will be the size of each feature vector in `layer_1` after the convolution and before being passed through the fully-connected layer.\n",
    "\n",
    "```python\n",
    "kernels = 0.02*np.random.random((kernel_rows*kernel_cols, num_kernels))-0.01\n",
    "```\n",
    "\n",
    "**<em>Q:why there is only 'weights_1_2' be used not 'weights_0_1' be used in this code?**</em>\n",
    "\n",
    "In a typical neural network with fully connected layers, weights between each layer, often denoted as `weights_0_1`, `weights_1_2`, etc., are used to map from one layer to the next. However, in the provided code, convolutional operations have replaced the fully connected layer that would have otherwise existed between `layer_0` (input layer) and `layer_1` (hidden layer). This is a key feature of Convolutional Neural Networks (CNNs).\n",
    "\n",
    "Here's how it works in the given code:\n",
    "\n",
    "1. The input images (stored in `layer_0`) are not mapped to `layer_1` using a weight matrix `weights_0_1`. Instead, they are convolved with a set of kernels. These kernels effectively replace `weights_0_1`. \n",
    "\n",
    "2. After convolution and activation, the output (stored in `layer_1`) is then mapped to `layer_2` (output layer) using a weight matrix, which is `weights_1_2`.\n",
    "\n",
    "So, in summary, `weights_0_1` is effectively replaced by the set of kernels used for the convolutional operation, and that's why only `weights_1_2` appears in the code.\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### Image Section Extraction\n",
    "\n",
    "```python\n",
    "def get_image_section(layer,row_from, row_to, col_from, col_to):\n",
    "    section = layer[:,row_from:row_to,col_from:col_to]\n",
    "    return section.reshape(-1,1,row_to-row_from, col_to-col_from)\n",
    "```\n",
    "\n",
    "This function extracts a section of the image to apply the kernel. It reshapes the extracted section so that it can be mu\n",
    "\n",
    "Certainly! The code snippet is from the function `get_image_section`, which extracts a specific region from the 2D input array `layer`. This region is defined by the rows `[row_from:row_to]` and columns `[col_from:col_to]`. Let's break down each line:\n",
    "\n",
    "### Line 1: Extracting the Region\n",
    "\n",
    "```python\n",
    "section = layer[:,row_from:row_to,col_from:col_to]\n",
    "```\n",
    "\n",
    "- `layer`: This is a 3D array where the first dimension usually represents the batch size, the second represents rows, and the third represents columns of the image.\n",
    "  \n",
    "- `[:, row_from:row_to, col_from:col_to]`: This is NumPy slicing syntax, which is extracting a section of the array `layer`.\n",
    "  - `:` means to take all elements along the first dimension (usually the batch dimension in this case).\n",
    "  - `row_from:row_to` means to take all rows from `row_from` to `row_to-1`.\n",
    "  - `col_from:col_to` means to take all columns from `col_from` to `col_to-1`.\n",
    "\n",
    "Certainly! The code snippet is from the function `get_image_section`, which extracts a specific region from the 2D input array `layer`. This region is defined by the rows `[row_from:row_to]` and columns `[col_from:col_to]`. Let's break down each line:\n",
    "\n",
    "### Line 1: Extracting the Region\n",
    "\n",
    "```python\n",
    "section = layer[:,row_from:row_to,col_from:col_to]\n",
    "```\n",
    "\n",
    "- `layer`: This is a 3D array where the first dimension usually represents the batch size, the second represents rows, and the third represents columns of the image.\n",
    "  \n",
    "- `[:, row_from:row_to, col_from:col_to]`: This is NumPy slicing syntax, which is extracting a section of the array `layer`.\n",
    "  - `:` means to take all elements along the first dimension (usually the batch dimension in this case).\n",
    "  - `row_from:row_to` means to take all rows from `row_from` to `row_to-1`.\n",
    "  - `col_from:col_to` means to take all columns from `col_from` to `col_to-1`.\n",
    "\n",
    "\n",
    "### Line 2: Reshaping the Region\n",
    "\n",
    "```python\n",
    "return section.reshape(-1,1,row_to-row_from, col_to-col_from)\n",
    "```\n",
    "\n",
    "- `reshape(-1, 1, row_to-row_from, col_to-col_from)`: This reshapes the `section` array. Here's what each argument does:\n",
    "  - `-1`: The size of this dimension is automatically calculated. This usually would be `batch_size * num_of_sections`, where `num_of_sections` is the number of distinct sections you're taking from each image in the batch.\n",
    "  - `1`: Adds an extra dimension. This is useful for keeping the shape consistent with potential future operations, like concatenation.\n",
    "  - `row_to-row_from`: The height of the section. It's the difference between `row_to` and `row_from`.\n",
    "  - `col_to-col_from`: The width of the section. It's the difference between `col_to` and `col_from`.\n",
    "\n",
    "This reshaped section is then returned by the function. The reshaping is necessary for future operations, such as dot products with the kernels. The shape after reshaping essentially stacks all the individual sections and makes it ready for subsequent matrix operations.\n",
    "ltiplied by the kernel.\n",
    "\n",
    "### The `-1` in `reshape`\n",
    "\n",
    "When using `-1` as a dimension in `numpy.reshape`, it's essentially a \"wildcard\" that tells NumPy to automatically calculate the size of that dimension. The size is determined such that the total number of elements in the array remains unchanged.\n",
    "\n",
    "For example, suppose you have an array of shape `(2, 3, 4)`; this array has `2 * 3 * 4 = 24` total elements. If you reshape it with dimensions `(2, -1, 2)`, NumPy will automatically calculate that the `-1` should be `6`, because `2 * 6 * 2 = 24`. So, the shape becomes `(2, 6, 2)`.\n",
    "\n",
    "In your code, the `-1` allows the function to be more flexible with respect to the batch size and the number of sections you're taking from each image. It ensures that the reshaping works correctly, regardless of these sizes, as long as the total number of elements remains constant.\n",
    "\n",
    "### The `1` in `reshape`\n",
    "\n",
    "Adding an extra dimension with size `1` can be useful for several reasons:\n",
    "\n",
    "1. **Broadcasting:** NumPy allows broadcasting of dimensions with size `1` when performing array operations. This can be useful if you want to perform element-wise operations between arrays that mostly have matching dimensions, except for one that is missing or has a size of `1`.\n",
    "\n",
    "2. **Shape Compatibility:** Some machine learning libraries and functions require input arrays to have a specific number of dimensions. Adding a dimension with size `1` allows the array to meet this requirement without changing the overall semantics of the data.\n",
    "\n",
    "3. **Concatenation and Stacking:** If you plan to concatenate or stack arrays along a new dimension, initializing that dimension with a size of `1` is useful.\n",
    "\n",
    "4. **Clearer Semantics:** Sometimes an extra dimension is added to make it clear what each dimension represents. For example, a 3D array where the first dimension represents different samples, the second represents the height, and the third represents the width can be made 4D to indicate that there's only one \"channel\" (like grayscale in image data), even though this channel contains only a single slice.\n",
    "\n",
    "In your code, the extra dimension helps mainly with shape compatibility and potential future concatenation operations.\n",
    "\n",
    "Certainly! Adding an extra dimension with size `1` is often done in NumPy using the `reshape` method. Here's a simple example:\n",
    "\n",
    "### Original Array\n",
    "\n",
    "Let's say we have a 1D array with shape `(4,)` and it looks like this:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "arr = np.array([1, 2, 3, 4])\n",
    "print(\"Original array:\", arr)\n",
    "print(\"Shape:\", arr.shape)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Original array: [1 2 3 4]\n",
    "Shape: (4,)\n",
    "```\n",
    "\n",
    "### Adding an Extra Dimension\n",
    "\n",
    "Now, we'll add an extra dimension with size `1`:\n",
    "\n",
    "#### Row Vector\n",
    "\n",
    "For making it a row vector:\n",
    "\n",
    "```python\n",
    "reshaped_arr = arr.reshape((1, 4))\n",
    "print(\"Reshaped array:\", reshaped_arr)\n",
    "print(\"Shape:\", reshaped_arr.shape)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Reshaped array: [[1 2 3 4]]\n",
    "Shape: (1, 4)\n",
    "```\n",
    "\n",
    "#### Column Vector\n",
    "\n",
    "For making it a column vector:\n",
    "\n",
    "```python\n",
    "reshaped_arr = arr.reshape((4, 1))\n",
    "print(\"Reshaped array:\", reshaped_arr)\n",
    "print(\"Shape:\", reshaped_arr.shape)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Reshaped array: [[1]\n",
    "                 [2]\n",
    "                 [3]\n",
    "                 [4]]\n",
    "Shape: (4, 1)\n",
    "```\n",
    "\n",
    "In both of these reshaped arrays, we added an extra dimension with size `1`. The reshaped array still represents the same data but in a form that may be more suitable for certain types of operations.\n",
    "\n",
    "For example, this kind of reshaping is often useful when you're working with machine learning libraries that expect inputs to have a certain number of dimensions.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "```python\n",
    "        batch_start, batch_end=((i * batch_size),((i+1)*batch_size))\n",
    "```\n",
    "\n",
    "This sets up the indices for batching the dataset. batch_start and batch_end determine the beginning and end of each batch.\n",
    "\n",
    "```python\n",
    "        layer_0 = images[batch_start:batch_end]\n",
    "        layer_0 = layer_0.reshape(layer_0.shape[0],28,28)\n",
    "```\n",
    "\n",
    "layer_0 contains the batch of input images. They are reshaped to 28x28 pixels each.\n",
    "In the reshaped version `layer_0.reshape(layer_0.shape[0], 28, 28)`, `layer_0` becomes a 3D array or tensor, not a 1D matrix. Each of the 1000 elements along the first dimension is a 2D matrix of shape `28x28`, representing an image. Here's the breakdown:\n",
    "\n",
    "1. Initially, `layer_0` is a 2D array with shape `(1000, 784)`, representing 1000 images where each image is a 1D array of 784 pixels.\n",
    "2. After the reshape operation, `layer_0` becomes a 3D array with shape `(1000, 28, 28)`. Now it represents 1000 images, where each image is a 2D array (or matrix) of shape `28x28`.\n",
    "\n",
    "So, in the reshaped `layer_0`, each image has been reorganized from a 1D array of 784 pixels into a 2D array of `28x28` pixels, which can be easier to work with or visualize, especially if the images are meant to be 28 pixels in height and 28 pixels in width.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### Feature Map Creation\n",
    "\n",
    "```python\n",
    "sects = list()\n",
    "for row_start in range(layer_0.shape[1]-kernel_rows):\n",
    "    for col_start in range(layer_0.shape[2] - kernel_cols):\n",
    "        sect = get_image_section(layer_0,\n",
    "                                 row_start,\n",
    "                                 row_start+kernel_rows,\n",
    "                                 col_start,\n",
    "                                 col_start+kernel_cols)\n",
    "        sects.append(sect)\n",
    "```\n",
    "\n",
    "This loop iterates over the image, and at each iteration, it calls `get_image_section` to obtain a section of the image. These sections are what the kernel will be applied to.\n",
    "\n",
    "The line `sects = list()` initializes an empty list and assigns it to the variable `sects`. This is preparation for populating this list later in the code.\n",
    "This empty list will hold the sections of the image to which the kernel (convolution filter) will be applied.\n",
    "\n",
    "In the context of the given code, `sects` is meant to hold \"sections\" of the input images that are processed through the convolutional kernels. Specifically, the code loops through different positions of each image in the mini-batch to extract subsections of the image, defined by the `kernel_rows` and `kernel_cols`. These subsections are then appended to the `sects` list.\n",
    "\n",
    "Here's a simplified explanation:\n",
    "\n",
    "- `sects` starts as an empty list.\n",
    "- The code extracts subsections of the image and appends them to `sects`.\n",
    "- Eventually, `sects` contains all the different portions of the input image that the convolutional kernel will scan through.\n",
    "\n",
    "By initializing `sects` as an empty list, you ensure that it's ready to accept these subsections as they are computed.\n",
    "\n",
    "In one iteration of 'i', sects will hold all patches that extracted from 128(batch size) images.\n",
    "\n",
    "For a single 28x28 image and a 3x3 kernel, you can slide the kernel across the image in strides of 1 pixel both horizontally and vertically. \n",
    "\n",
    "The number of ways you can fit a 3x3 kernel into a 28x28 image can be calculated as follows:\n",
    "\n",
    "1. Horizontally: You can start the kernel at columns 0 to 25, making it 26 possible starting positions.\n",
    "2. Vertically: You can start the kernel at rows 0 to 25, making it 26 possible starting positions as well.\n",
    "\n",
    "So for each image, you'll have 26 (horizontal positions) x 26 (vertical positions) = 676 patches.\n",
    "\n",
    "Therefore, for a single 28x28 image with a 3x3 kernel, the list `sects` will hold 676 patches.\n",
    "\n",
    "**<em>Q:why this for loop \"for row_start in range(layer_0.shape[1]-kernel_rows):\" start with 'layer_0.shape[1]' not 'layer_0.shape[0]'?</em>**\n",
    "\n",
    "In the code, `layer_0` is reshaped as `[batch_size, 28, 28]`, where `batch_size` is the number of images in a mini-batch, and 28x28 is the dimension of each image.\n",
    "\n",
    "When you look at `layer_0.shape[1]`, it refers to the number of rows in each image, which is 28 in this case. Similarly, `layer_0.shape[2]` would refer to the number of columns in each image, which is also 28.\n",
    "\n",
    "The reason for using `layer_0.shape[1]` instead of `layer_0.shape[0]` is that the for-loop is iterating over the rows of each individual image, not over the batch size. It's moving the kernel over each 28x28 image, so it needs to start at row 0 and go up to row 25 (`28 - 3 = 25`) to cover all the positions where the 3x3 kernel can fit. This ensures that you can slide the 3x3 kernel over each row of each image in the batch.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### Convolution\n",
    "\n",
    "```python\n",
    "expanded_input = np.concatenate(sects,axis=1)\n",
    "es = expanded_input.shape\n",
    "flattened_input = expanded_input.reshape(es[0]*es[1],-1)\n",
    "\n",
    "kernel_output = flattened_input.dot(kernels)\n",
    "```\n",
    "\n",
    "The `expanded_input` holds all the sections of the image, flattened and concatenated连接. The dot product with `kernels` computes the convolution operation, creating the feature map.\n",
    "\n",
    "Certainly, let's break down these lines of code one by one:\n",
    "\n",
    "1. `expanded_input = np.concatenate(sects,axis=1)`: \n",
    "   - `sects` is a list of patches extracted from the input images. Each patch has dimensions `[batch_size, 1, kernel_rows, kernel_cols]`.\n",
    "   - The `np.concatenate` function concatenates连接 these patches along `axis=1`. After concatenation, you'll have a tensor where each \"column\" corresponds to a patch from the input images.\n",
    "   \n",
    "2. `es = expanded_input.shape`: \n",
    "   - This line stores the shape of `expanded_input` in `es`. Let's say, if you had 100 patches and your batch size is 128, `es` could look like `[128, 100, 3, 3]` (assuming 3x3 kernels).\n",
    "\n",
    "3. `flattened_input = expanded_input.reshape(es[0]*es[1],-1)`:\n",
    "   - The reshape operation is flattening the tensor along the first two dimensions (`batch_size` and number of patches).\n",
    "   - `es[0]*es[1]` would give you `128 * 100 = 12800` if we follow the earlier example. The `-1` in reshape means that the remaining dimensions are calculated automatically. In this case, the last two dimensions are 3x3, so they would be flattened into a single dimension of size `3 * 3 = 9`.\n",
    "\n",
    "4. `kernel_output = flattened_input.dot(kernels)`:\n",
    "   - Here, you have `flattened_input`, which has a shape like `[12800, 9]`, being matrix-multiplied (dot product) with `kernels`, which has a shape like `[9, num_kernels]` (let's say `[9, 16]` for 16 kernels).\n",
    "   - The output, `kernel_output`, will have a shape of `[12800, 16]`. Each row in `kernel_output` contains the activations produced by all 16 kernels for a specific patch in the input images.\n",
    "   \n",
    "Overall, these operations are aimed at preparing the input in a way that allows you to perform convolution operations effectively, applying multiple kernels over multiple patches and flattening them for easier calculations.\n",
    "\n",
    "### Activation and Dropout\n",
    "\n",
    "```python\n",
    "layer_1 = tanh(kernel_output.reshape(es[0],-1))\n",
    "dropout_mask = np.random.randint(2,size=layer_1.shape)\n",
    "layer_1 *= dropout_mask * 2\n",
    "```\n",
    "\n",
    "Here, the feature map goes through a tanh activation function. Then a dropout is applied, which is a regularization technique.\n",
    "\n",
    "### Backpropagation for CNN\n",
    "\n",
    "```python\n",
    "        layer_2_delta = (labels[batch_start:batch_end]-layer_2)\\\n",
    "                        / (batch_size * layer_2.shape[0])\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * \\\n",
    "                        tanh2deriv(layer_1)\n",
    "        layer_1_delta *= dropout_mask\n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        l1d_reshape = layer_1_delta.reshape(kernel_output.shape)\n",
    "        k_update = flattened_input.T.dot(l1d_reshape)\n",
    "        kernels -= alpha * k_update\n",
    "```\n",
    "\n",
    "In the backpropagation step, the gradients are calculated with respect to each kernel. Then, the kernels are updated.\n",
    "\n",
    "```python\n",
    "layer_2_delta = (labels[batch_start:batch_end]-layer_2) / (batch_size * layer_2.shape[0])\n",
    "```\n",
    "1. `layer_2_delta`: This is the error term for the output layer (`layer_2`). \n",
    "   - The error is computed as the difference between the true labels and the predicted values (`labels[batch_start:batch_end] - layer_2`).\n",
    "   - The division by `(batch_size * layer_2.shape[0])` is a normalization term. It averages the error over the number of instances in the batch.\n",
    "\n",
    "```python\n",
    "layer_1_delta = layer_2_delta.dot(weights_1_2.T) * tanh2deriv(layer_1)\n",
    "```\n",
    "2. `layer_1_delta`: This is the error term for the hidden layer (`layer_1`). \n",
    "   - It's calculated by taking the dot product of `layer_2_delta` and the transpose of `weights_1_2`. This backpropagates the error from the output layer to the hidden layer.\n",
    "   - This is then element-wise multiplied by `tanh2deriv(layer_1)`, which is the derivative of the activation function (tanh in this case) applied at `layer_1`. This gives you the gradient of the loss with respect to the activations in `layer_1`.\n",
    "\n",
    "```python\n",
    "layer_1_delta *= dropout_mask\n",
    "```\n",
    "3. This line applies the dropout mask to `layer_1_delta`.\n",
    "   - Dropout is used during training to randomly set some activations to zero, preventing overfitting. This line ensures that the same activations that were zeroed during the forward pass also have zero gradients during the backpropagation.\n",
    "\n",
    "```python\n",
    "weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "```\n",
    "4. This updates the weights between `layer_1` and `layer_2`.\n",
    "   - `alpha` is the learning rate, controlling how much we want to update the weights.\n",
    "   - `layer_1.T.dot(layer_2_delta)` calculates the gradient of the loss with respect to `weights_1_2`.\n",
    "\n",
    "```python\n",
    "l1d_reshape = layer_1_delta.reshape(kernel_output.shape)\n",
    "```\n",
    "5. The error term for `layer_1` (`layer_1_delta`) is reshaped back into the shape of `kernel_output`. This is done because you want to distribute this error back to each patch you originally extracted.\n",
    "\n",
    "```python\n",
    "k_update = flattened_input.T.dot(l1d_reshape)\n",
    "```\n",
    "6. `k_update`: This is the gradient for updating the kernels.\n",
    "   - The dot product is taken between the transpose of `flattened_input` and `l1d_reshape` to compute how much each kernel contributed to the overall error.\n",
    "\n",
    "```python\n",
    "kernels -= alpha * k_update\n",
    "```\n",
    "7. The kernels are updated.\n",
    "   - The gradients stored in `k_update` are used to update each kernel. The learning rate `alpha` controls the magnitude of this update.\n",
    "\n",
    "Each line of code in the backpropagation section plays a crucial role in updating the model's weights and biases, with the ultimate aim of minimizing the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5628b667-2264-4b9e-abde-4956d983dd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Acc:0.0288 Train-Acc:0.055\n",
      "I:1 Test-Acc:0.0273 Train-Acc:0.037\n",
      "I:2 Test-Acc:0.028 Train-Acc:0.037\n",
      "I:3 Test-Acc:0.0292 Train-Acc:0.04\n",
      "I:4 Test-Acc:0.0339 Train-Acc:0.046\n",
      "I:5 Test-Acc:0.0478 Train-Acc:0.068\n",
      "I:6 Test-Acc:0.076 Train-Acc:0.083\n",
      "I:7 Test-Acc:0.1316 Train-Acc:0.096\n",
      "I:8 Test-Acc:0.2137 Train-Acc:0.127\n",
      "I:9 Test-Acc:0.2941 Train-Acc:0.148\n",
      "I:10 Test-Acc:0.3563 Train-Acc:0.181\n",
      "I:11 Test-Acc:0.4023 Train-Acc:0.209\n",
      "I:12 Test-Acc:0.4358 Train-Acc:0.238\n",
      "I:13 Test-Acc:0.4473 Train-Acc:0.286\n",
      "I:14 Test-Acc:0.4389 Train-Acc:0.274\n",
      "I:15 Test-Acc:0.3951 Train-Acc:0.257\n",
      "I:16 Test-Acc:0.2222 Train-Acc:0.243\n",
      "I:17 Test-Acc:0.0613 Train-Acc:0.112\n",
      "I:18 Test-Acc:0.0266 Train-Acc:0.035\n",
      "I:19 Test-Acc:0.0127 Train-Acc:0.026\n",
      "I:20 Test-Acc:0.0133 Train-Acc:0.022\n",
      "I:21 Test-Acc:0.0185 Train-Acc:0.038\n",
      "I:22 Test-Acc:0.0363 Train-Acc:0.038\n",
      "I:23 Test-Acc:0.0928 Train-Acc:0.067\n",
      "I:24 Test-Acc:0.1994 Train-Acc:0.081\n",
      "I:25 Test-Acc:0.3086 Train-Acc:0.154\n",
      "I:26 Test-Acc:0.4276 Train-Acc:0.204\n",
      "I:27 Test-Acc:0.5323 Train-Acc:0.256\n",
      "I:28 Test-Acc:0.5919 Train-Acc:0.305\n",
      "I:29 Test-Acc:0.6324 Train-Acc:0.341\n",
      "I:30 Test-Acc:0.6608 Train-Acc:0.426\n",
      "I:31 Test-Acc:0.6815 Train-Acc:0.439\n",
      "I:32 Test-Acc:0.7048 Train-Acc:0.462\n",
      "I:33 Test-Acc:0.7171 Train-Acc:0.484\n",
      "I:34 Test-Acc:0.7313 Train-Acc:0.505\n",
      "I:35 Test-Acc:0.7355 Train-Acc:0.53\n",
      "I:36 Test-Acc:0.7417 Train-Acc:0.548\n",
      "I:37 Test-Acc:0.747 Train-Acc:0.534\n",
      "I:38 Test-Acc:0.7491 Train-Acc:0.55\n",
      "I:39 Test-Acc:0.7459 Train-Acc:0.562\n",
      "I:40 Test-Acc:0.7352 Train-Acc:0.54\n",
      "I:41 Test-Acc:0.7082 Train-Acc:0.496\n",
      "I:42 Test-Acc:0.6487 Train-Acc:0.456\n",
      "I:43 Test-Acc:0.5209 Train-Acc:0.353\n",
      "I:44 Test-Acc:0.3305 Train-Acc:0.234\n",
      "I:45 Test-Acc:0.2052 Train-Acc:0.174\n",
      "I:46 Test-Acc:0.2149 Train-Acc:0.136\n",
      "I:47 Test-Acc:0.2679 Train-Acc:0.171\n",
      "I:48 Test-Acc:0.3237 Train-Acc:0.172\n",
      "I:49 Test-Acc:0.3581 Train-Acc:0.186\n",
      "I:50 Test-Acc:0.4202 Train-Acc:0.21\n",
      "I:51 Test-Acc:0.5165 Train-Acc:0.223\n",
      "I:52 Test-Acc:0.6007 Train-Acc:0.262\n",
      "I:53 Test-Acc:0.6476 Train-Acc:0.308\n",
      "I:54 Test-Acc:0.676 Train-Acc:0.363\n",
      "I:55 Test-Acc:0.696 Train-Acc:0.402\n",
      "I:56 Test-Acc:0.7077 Train-Acc:0.434\n",
      "I:57 Test-Acc:0.7204 Train-Acc:0.441\n",
      "I:58 Test-Acc:0.7303 Train-Acc:0.475\n",
      "I:59 Test-Acc:0.7359 Train-Acc:0.475\n",
      "I:60 Test-Acc:0.7401 Train-Acc:0.525\n",
      "I:61 Test-Acc:0.7493 Train-Acc:0.517\n",
      "I:62 Test-Acc:0.7533 Train-Acc:0.517\n",
      "I:63 Test-Acc:0.7606 Train-Acc:0.538\n",
      "I:64 Test-Acc:0.7644 Train-Acc:0.554\n",
      "I:65 Test-Acc:0.7724 Train-Acc:0.57\n",
      "I:66 Test-Acc:0.7788 Train-Acc:0.586\n",
      "I:67 Test-Acc:0.7855 Train-Acc:0.595\n",
      "I:68 Test-Acc:0.7853 Train-Acc:0.591\n",
      "I:69 Test-Acc:0.7925 Train-Acc:0.605\n",
      "I:70 Test-Acc:0.7973 Train-Acc:0.64\n",
      "I:71 Test-Acc:0.8013 Train-Acc:0.621\n",
      "I:72 Test-Acc:0.8029 Train-Acc:0.626\n",
      "I:73 Test-Acc:0.8092 Train-Acc:0.631\n",
      "I:74 Test-Acc:0.8099 Train-Acc:0.638\n",
      "I:75 Test-Acc:0.8156 Train-Acc:0.661\n",
      "I:76 Test-Acc:0.8156 Train-Acc:0.639\n",
      "I:77 Test-Acc:0.8184 Train-Acc:0.65\n",
      "I:78 Test-Acc:0.8216 Train-Acc:0.67\n",
      "I:79 Test-Acc:0.8246 Train-Acc:0.675\n",
      "I:80 Test-Acc:0.8237 Train-Acc:0.666\n",
      "I:81 Test-Acc:0.8273 Train-Acc:0.673\n",
      "I:82 Test-Acc:0.8273 Train-Acc:0.704\n",
      "I:83 Test-Acc:0.8314 Train-Acc:0.674\n",
      "I:84 Test-Acc:0.8292 Train-Acc:0.686\n",
      "I:85 Test-Acc:0.8335 Train-Acc:0.699\n",
      "I:86 Test-Acc:0.8359 Train-Acc:0.694\n",
      "I:87 Test-Acc:0.8375 Train-Acc:0.704\n",
      "I:88 Test-Acc:0.8373 Train-Acc:0.697\n",
      "I:89 Test-Acc:0.8398 Train-Acc:0.704\n",
      "I:90 Test-Acc:0.8393 Train-Acc:0.687\n",
      "I:91 Test-Acc:0.8436 Train-Acc:0.705\n",
      "I:92 Test-Acc:0.8437 Train-Acc:0.711\n",
      "I:93 Test-Acc:0.8446 Train-Acc:0.721\n",
      "I:94 Test-Acc:0.845 Train-Acc:0.719\n",
      "I:95 Test-Acc:0.8469 Train-Acc:0.724\n",
      "I:96 Test-Acc:0.8476 Train-Acc:0.726\n",
      "I:97 Test-Acc:0.848 Train-Acc:0.718\n",
      "I:98 Test-Acc:0.8496 Train-Acc:0.719\n",
      "I:99 Test-Acc:0.85 Train-Acc:0.73\n",
      "I:100 Test-Acc:0.8511 Train-Acc:0.737\n",
      "I:101 Test-Acc:0.8503 Train-Acc:0.73\n",
      "I:102 Test-Acc:0.8504 Train-Acc:0.717\n",
      "I:103 Test-Acc:0.8528 Train-Acc:0.74\n",
      "I:104 Test-Acc:0.8532 Train-Acc:0.733\n",
      "I:105 Test-Acc:0.8537 Train-Acc:0.73\n",
      "I:106 Test-Acc:0.8568 Train-Acc:0.721\n",
      "I:107 Test-Acc:0.857 Train-Acc:0.75\n",
      "I:108 Test-Acc:0.8558 Train-Acc:0.731\n",
      "I:109 Test-Acc:0.8578 Train-Acc:0.744\n",
      "I:110 Test-Acc:0.8588 Train-Acc:0.754\n",
      "I:111 Test-Acc:0.8579 Train-Acc:0.732\n",
      "I:112 Test-Acc:0.8582 Train-Acc:0.747\n",
      "I:113 Test-Acc:0.8593 Train-Acc:0.747\n",
      "I:114 Test-Acc:0.8598 Train-Acc:0.751\n",
      "I:115 Test-Acc:0.8603 Train-Acc:0.74\n",
      "I:116 Test-Acc:0.86 Train-Acc:0.753\n",
      "I:117 Test-Acc:0.8588 Train-Acc:0.746\n",
      "I:118 Test-Acc:0.861 Train-Acc:0.741\n",
      "I:119 Test-Acc:0.8616 Train-Acc:0.731\n",
      "I:120 Test-Acc:0.8629 Train-Acc:0.753\n",
      "I:121 Test-Acc:0.8609 Train-Acc:0.743\n",
      "I:122 Test-Acc:0.8627 Train-Acc:0.752\n",
      "I:123 Test-Acc:0.8646 Train-Acc:0.76\n",
      "I:124 Test-Acc:0.8649 Train-Acc:0.766\n",
      "I:125 Test-Acc:0.8659 Train-Acc:0.752\n",
      "I:126 Test-Acc:0.868 Train-Acc:0.756\n",
      "I:127 Test-Acc:0.8648 Train-Acc:0.767\n",
      "I:128 Test-Acc:0.8662 Train-Acc:0.747\n",
      "I:129 Test-Acc:0.8669 Train-Acc:0.753\n",
      "I:130 Test-Acc:0.8694 Train-Acc:0.753\n",
      "I:131 Test-Acc:0.8692 Train-Acc:0.76\n",
      "I:132 Test-Acc:0.8658 Train-Acc:0.756\n",
      "I:133 Test-Acc:0.8666 Train-Acc:0.769\n",
      "I:134 Test-Acc:0.8692 Train-Acc:0.77\n",
      "I:135 Test-Acc:0.8681 Train-Acc:0.757\n",
      "I:136 Test-Acc:0.8705 Train-Acc:0.77\n",
      "I:137 Test-Acc:0.8706 Train-Acc:0.77\n",
      "I:138 Test-Acc:0.8684 Train-Acc:0.768\n",
      "I:139 Test-Acc:0.8664 Train-Acc:0.774\n",
      "I:140 Test-Acc:0.8666 Train-Acc:0.756\n",
      "I:141 Test-Acc:0.8705 Train-Acc:0.783\n",
      "I:142 Test-Acc:0.87 Train-Acc:0.775\n",
      "I:143 Test-Acc:0.8729 Train-Acc:0.769\n",
      "I:144 Test-Acc:0.8725 Train-Acc:0.776\n",
      "I:145 Test-Acc:0.8721 Train-Acc:0.772\n",
      "I:146 Test-Acc:0.8718 Train-Acc:0.765\n",
      "I:147 Test-Acc:0.8746 Train-Acc:0.777\n",
      "I:148 Test-Acc:0.8746 Train-Acc:0.77\n",
      "I:149 Test-Acc:0.8734 Train-Acc:0.778\n",
      "I:150 Test-Acc:0.873 Train-Acc:0.785\n",
      "I:151 Test-Acc:0.8732 Train-Acc:0.76\n",
      "I:152 Test-Acc:0.8727 Train-Acc:0.779\n",
      "I:153 Test-Acc:0.8754 Train-Acc:0.772\n",
      "I:154 Test-Acc:0.8729 Train-Acc:0.773\n",
      "I:155 Test-Acc:0.8758 Train-Acc:0.784\n",
      "I:156 Test-Acc:0.8732 Train-Acc:0.774\n",
      "I:157 Test-Acc:0.8743 Train-Acc:0.782\n",
      "I:158 Test-Acc:0.8762 Train-Acc:0.772\n",
      "I:159 Test-Acc:0.8755 Train-Acc:0.79\n",
      "I:160 Test-Acc:0.8751 Train-Acc:0.774\n",
      "I:161 Test-Acc:0.8749 Train-Acc:0.782\n",
      "I:162 Test-Acc:0.8744 Train-Acc:0.78\n",
      "I:163 Test-Acc:0.8765 Train-Acc:0.782\n",
      "I:164 Test-Acc:0.8738 Train-Acc:0.796\n",
      "I:165 Test-Acc:0.8753 Train-Acc:0.798\n",
      "I:166 Test-Acc:0.8767 Train-Acc:0.794\n",
      "I:167 Test-Acc:0.8746 Train-Acc:0.784\n",
      "I:168 Test-Acc:0.8769 Train-Acc:0.796\n",
      "I:169 Test-Acc:0.8758 Train-Acc:0.789\n",
      "I:170 Test-Acc:0.8764 Train-Acc:0.79\n",
      "I:171 Test-Acc:0.873 Train-Acc:0.791\n",
      "I:172 Test-Acc:0.8765 Train-Acc:0.797\n",
      "I:173 Test-Acc:0.8772 Train-Acc:0.789\n",
      "I:174 Test-Acc:0.8778 Train-Acc:0.781\n",
      "I:175 Test-Acc:0.8758 Train-Acc:0.799\n",
      "I:176 Test-Acc:0.8773 Train-Acc:0.785\n",
      "I:177 Test-Acc:0.8766 Train-Acc:0.796\n",
      "I:178 Test-Acc:0.8782 Train-Acc:0.803\n",
      "I:179 Test-Acc:0.8789 Train-Acc:0.794\n",
      "I:180 Test-Acc:0.8778 Train-Acc:0.794\n",
      "I:181 Test-Acc:0.8778 Train-Acc:0.8\n",
      "I:182 Test-Acc:0.8785 Train-Acc:0.791\n",
      "I:183 Test-Acc:0.8777 Train-Acc:0.787\n",
      "I:184 Test-Acc:0.8769 Train-Acc:0.781\n",
      "I:185 Test-Acc:0.8765 Train-Acc:0.786\n",
      "I:186 Test-Acc:0.8765 Train-Acc:0.793\n",
      "I:187 Test-Acc:0.8785 Train-Acc:0.796\n",
      "I:188 Test-Acc:0.879 Train-Acc:0.789\n",
      "I:189 Test-Acc:0.8763 Train-Acc:0.79\n",
      "I:190 Test-Acc:0.8774 Train-Acc:0.787\n",
      "I:191 Test-Acc:0.8766 Train-Acc:0.782\n",
      "I:192 Test-Acc:0.8803 Train-Acc:0.798\n",
      "I:193 Test-Acc:0.8781 Train-Acc:0.789\n",
      "I:194 Test-Acc:0.8795 Train-Acc:0.785\n",
      "I:195 Test-Acc:0.8791 Train-Acc:0.807\n",
      "I:196 Test-Acc:0.8778 Train-Acc:0.796\n",
      "I:197 Test-Acc:0.8783 Train-Acc:0.801\n",
      "I:198 Test-Acc:0.8778 Train-Acc:0.81\n",
      "I:199 Test-Acc:0.8771 Train-Acc:0.784\n",
      "I:200 Test-Acc:0.8776 Train-Acc:0.792\n",
      "I:201 Test-Acc:0.8784 Train-Acc:0.794\n",
      "I:202 Test-Acc:0.8787 Train-Acc:0.795\n",
      "I:203 Test-Acc:0.8803 Train-Acc:0.781\n",
      "I:204 Test-Acc:0.8798 Train-Acc:0.804\n",
      "I:205 Test-Acc:0.8779 Train-Acc:0.779\n",
      "I:206 Test-Acc:0.8788 Train-Acc:0.792\n",
      "I:207 Test-Acc:0.8764 Train-Acc:0.793\n",
      "I:208 Test-Acc:0.8792 Train-Acc:0.792\n",
      "I:209 Test-Acc:0.8798 Train-Acc:0.803\n",
      "I:210 Test-Acc:0.8788 Train-Acc:0.804\n",
      "I:211 Test-Acc:0.8793 Train-Acc:0.797\n",
      "I:212 Test-Acc:0.8764 Train-Acc:0.791\n",
      "I:213 Test-Acc:0.8801 Train-Acc:0.801\n",
      "I:214 Test-Acc:0.8814 Train-Acc:0.799\n",
      "I:215 Test-Acc:0.8806 Train-Acc:0.79\n",
      "I:216 Test-Acc:0.8799 Train-Acc:0.8\n",
      "I:217 Test-Acc:0.8803 Train-Acc:0.802\n",
      "I:218 Test-Acc:0.8782 Train-Acc:0.807\n",
      "I:219 Test-Acc:0.8818 Train-Acc:0.797\n",
      "I:220 Test-Acc:0.8793 Train-Acc:0.799\n",
      "I:221 Test-Acc:0.8789 Train-Acc:0.815\n",
      "I:222 Test-Acc:0.8791 Train-Acc:0.816\n",
      "I:223 Test-Acc:0.8793 Train-Acc:0.809\n",
      "I:224 Test-Acc:0.8814 Train-Acc:0.795\n",
      "I:225 Test-Acc:0.8798 Train-Acc:0.799\n",
      "I:226 Test-Acc:0.8805 Train-Acc:0.806\n",
      "I:227 Test-Acc:0.88 Train-Acc:0.808\n",
      "I:228 Test-Acc:0.8782 Train-Acc:0.801\n",
      "I:229 Test-Acc:0.8802 Train-Acc:0.814\n",
      "I:230 Test-Acc:0.8807 Train-Acc:0.8\n",
      "I:231 Test-Acc:0.8809 Train-Acc:0.798\n",
      "I:232 Test-Acc:0.8805 Train-Acc:0.82\n",
      "I:233 Test-Acc:0.8795 Train-Acc:0.794\n",
      "I:234 Test-Acc:0.8807 Train-Acc:0.806\n",
      "I:235 Test-Acc:0.8806 Train-Acc:0.808\n",
      "I:236 Test-Acc:0.8787 Train-Acc:0.802\n",
      "I:237 Test-Acc:0.8796 Train-Acc:0.81\n",
      "I:238 Test-Acc:0.8766 Train-Acc:0.805\n",
      "I:239 Test-Acc:0.8781 Train-Acc:0.792\n",
      "I:240 Test-Acc:0.8787 Train-Acc:0.809\n",
      "I:241 Test-Acc:0.8762 Train-Acc:0.802\n",
      "I:242 Test-Acc:0.8775 Train-Acc:0.811\n",
      "I:243 Test-Acc:0.8804 Train-Acc:0.814\n",
      "I:244 Test-Acc:0.8794 Train-Acc:0.804\n",
      "I:245 Test-Acc:0.8788 Train-Acc:0.801\n",
      "I:246 Test-Acc:0.8777 Train-Acc:0.795\n",
      "I:247 Test-Acc:0.8785 Train-Acc:0.808\n",
      "I:248 Test-Acc:0.8788 Train-Acc:0.803\n",
      "I:249 Test-Acc:0.8773 Train-Acc:0.813\n",
      "I:250 Test-Acc:0.8786 Train-Acc:0.808\n",
      "I:251 Test-Acc:0.8787 Train-Acc:0.803\n",
      "I:252 Test-Acc:0.8789 Train-Acc:0.812\n",
      "I:253 Test-Acc:0.8792 Train-Acc:0.804\n",
      "I:254 Test-Acc:0.8779 Train-Acc:0.815\n",
      "I:255 Test-Acc:0.8796 Train-Acc:0.811\n",
      "I:256 Test-Acc:0.8798 Train-Acc:0.806\n",
      "I:257 Test-Acc:0.88 Train-Acc:0.803\n",
      "I:258 Test-Acc:0.8776 Train-Acc:0.795\n",
      "I:259 Test-Acc:0.8798 Train-Acc:0.803\n",
      "I:260 Test-Acc:0.8799 Train-Acc:0.805\n",
      "I:261 Test-Acc:0.8789 Train-Acc:0.807\n",
      "I:262 Test-Acc:0.8784 Train-Acc:0.804\n",
      "I:263 Test-Acc:0.8792 Train-Acc:0.806\n",
      "I:264 Test-Acc:0.8777 Train-Acc:0.796\n",
      "I:265 Test-Acc:0.8785 Train-Acc:0.821\n",
      "I:266 Test-Acc:0.8794 Train-Acc:0.81\n",
      "I:267 Test-Acc:0.8783 Train-Acc:0.816\n",
      "I:268 Test-Acc:0.8777 Train-Acc:0.812\n",
      "I:269 Test-Acc:0.8791 Train-Acc:0.812\n",
      "I:270 Test-Acc:0.878 Train-Acc:0.813\n",
      "I:271 Test-Acc:0.8784 Train-Acc:0.82\n",
      "I:272 Test-Acc:0.8792 Train-Acc:0.821\n",
      "I:273 Test-Acc:0.8781 Train-Acc:0.823\n",
      "I:274 Test-Acc:0.8788 Train-Acc:0.816\n",
      "I:275 Test-Acc:0.8793 Train-Acc:0.82\n",
      "I:276 Test-Acc:0.8781 Train-Acc:0.829\n",
      "I:277 Test-Acc:0.8795 Train-Acc:0.809\n",
      "I:278 Test-Acc:0.875 Train-Acc:0.806\n",
      "I:279 Test-Acc:0.8795 Train-Acc:0.813\n",
      "I:280 Test-Acc:0.88 Train-Acc:0.816\n",
      "I:281 Test-Acc:0.8796 Train-Acc:0.819\n",
      "I:282 Test-Acc:0.8802 Train-Acc:0.809\n",
      "I:283 Test-Acc:0.8804 Train-Acc:0.811\n",
      "I:284 Test-Acc:0.8779 Train-Acc:0.808\n",
      "I:285 Test-Acc:0.8816 Train-Acc:0.82\n",
      "I:286 Test-Acc:0.8792 Train-Acc:0.822\n",
      "I:287 Test-Acc:0.8791 Train-Acc:0.817\n",
      "I:288 Test-Acc:0.8769 Train-Acc:0.814\n",
      "I:289 Test-Acc:0.8785 Train-Acc:0.807\n",
      "I:290 Test-Acc:0.8778 Train-Acc:0.817\n",
      "I:291 Test-Acc:0.8794 Train-Acc:0.82\n",
      "I:292 Test-Acc:0.8804 Train-Acc:0.824\n",
      "I:293 Test-Acc:0.8779 Train-Acc:0.812\n",
      "I:294 Test-Acc:0.8784 Train-Acc:0.816\n",
      "I:295 Test-Acc:0.877 Train-Acc:0.817\n",
      "I:296 Test-Acc:0.8767 Train-Acc:0.826\n",
      "I:297 Test-Acc:0.8774 Train-Acc:0.816\n",
      "I:298 Test-Acc:0.8774 Train-Acc:0.804\n",
      "I:299 Test-Acc:0.8774 Train-Acc:0.814"
     ]
    }
   ],
   "source": [
    "import numpy as np, sys\n",
    "np.random.seed(1)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "images, labels = (x_train[0:1000].reshape(1000,28*28) / 255,\n",
    "                  y_train[0:1000])\n",
    "\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels),10))\n",
    "for i,l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test),28*28) / 255\n",
    "test_labels = np.zeros((len(y_test),10))\n",
    "for i,l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh2deriv(output):\n",
    "    return 1 - (output ** 2)\n",
    "\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "\n",
    "alpha, iterations = (2, 300)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "batch_size = 128\n",
    "\n",
    "input_rows = 28\n",
    "input_cols = 28\n",
    "\n",
    "kernel_rows = 3\n",
    "kernel_cols = 3\n",
    "num_kernel = 16\n",
    "\n",
    "hidden_size = ((input_rows - kernel_rows) * (input_cols - kernel_cols)) * num_kernel\n",
    "\n",
    "kernels = 0.02*np.random.random((kernel_rows * kernel_cols, num_kernel)) - 0.01\n",
    "\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size, num_labels)) - 0.1\n",
    "\n",
    "\n",
    "def get_image_section(layer,row_from, row_to, col_from, col_to):\n",
    "    section = layer[:,row_from:row_to,col_from:col_to]\n",
    "    return section.reshape(-1,1,row_to-row_from, col_to-col_from)\n",
    "\n",
    "for j in range(iterations):\n",
    "    correct_cnt = 0\n",
    "    for i in range(int(len(images)/batch_size)):\n",
    "        batch_start, batch_end = ((i*batch_size),((i+1)*batch_size)) #Batching Initalizaiton\n",
    "        #Reshaping Input Layer\n",
    "        layer_0 = images[batch_start:batch_end]\n",
    "        layer_0 = layer_0.reshape(layer_0.shape[0],28,28)\n",
    "        \n",
    "        #Convolution Initialization\n",
    "        sects = list() #This empty list will hold the sections of the image to which the kernel (convolution filter) will be applied.\n",
    "        #Convolution Process\n",
    "        for row_start in range(layer_0.shape[1] - kernel_rows):\n",
    "            for col_start in range(layer_0.shape[2] - kernel_cols):\n",
    "                sect = get_image_section(layer_0, row_start, row_start + kernel_rows, col_start, col_start + kernel_cols)\n",
    "                sects.append(sect)\n",
    "\n",
    "        #Preparation for dot production\n",
    "        expanded_input = np.concatenate(sects,axis=1) #连接所有patches\n",
    "        es = expanded_input.shape #连接patches后的形状 (128,676,28,28)128batchSize 676pathes\n",
    "        flattened_input = expanded_input.reshape(es[0]*es[1],-1) #沿着batchSize128和patches676两个维度展平张量积即这个参数为128*676 -1表示剩下参数自动计算 \n",
    "\n",
    "        #Hidden Layer Activation and Dropout\n",
    "        kernel_output = flattened_input.dot(kernels)#kernels是[9,16] 点乘的结果[128*676*9,16]\n",
    "        layer_1 = tanh(kernel_output.reshape(es[0],-1)) #将patches结果按batchSize128展开 代入激活函数\n",
    "        drop_mask = np.random.randint(2,size=layer_1.shape) \n",
    "        layer_1 *= drop_mask * 2 #The hidden layer (layer_1) is activated using a tanh function after the convolution. Dropout is applied for regularization\n",
    "        layer_2 = softmax(layer_1.dot(weights_1_2)) #The output layer (layer_2) is activated using a softmax function to produce probabilities for each class label.\n",
    "\n",
    "        #Accuracy Counting\n",
    "        for k in range(batch_size):#counts the number of correct predictions within the batch.\n",
    "            labelset = labels[batch_start+k:batch_start+k+1]\n",
    "            _inc = int(np.argmax(layer_2[k:k+1]) == np.argmax(labelset))\n",
    "            correct_cnt += _inc \n",
    "\n",
    "        #Backpropogation\n",
    "        layer_2_delta = (labels[batch_start:batch_end] - layer_2) / (batch_size * layer_2.shape[0])\n",
    "        #The division by (batch_size * layer_2.shape[0]) is a normalization term. It averages the error over the number of instances in the batch.\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T)*tanh2deriv(layer_1)\n",
    "        layer_1_delta *= drop_mask\n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        #kernels其实就是weights01 因为是把patches当layer1的输入 所以后面调整kernels也要使用patches同形的量\n",
    "        l1d_reshape = layer_1_delta.reshape(kernel_output.shape) #把上面点乘加dropout后的量 转型为之前给layer1输入的量的形\n",
    "        k_update = flattened_input.T.dot(l1d_reshape) #给之前layer1的输入量点乘上面变形的结果（需要变化的量）\n",
    "        kernels -= alpha * k_update\n",
    "\n",
    "    #test 流程和training相似 先调整输入数据形于kernels相适应后只forward train \n",
    "    test_correct_cnt = 0\n",
    "\n",
    "    for i in range(len(test_images)):\n",
    "\n",
    "        layer_0 = test_images[i:i+1]\n",
    "#          layer_1 = tanh(np.dot(layer_0, weights_0_1))\n",
    "        layer_0 = layer_0.reshape(layer_0.shape[0], 28,28)\n",
    "\n",
    "        sects = list()\n",
    "        for row_start in range(layer_0.shape[1]-kernel_rows):\n",
    "            for col_start in range(layer_0.shape[2]-kernel_cols):\n",
    "                sect = get_image_section(layer_0, row_start, row_start+kernel_rows, col_start, col_start+kernel_cols)\n",
    "                sects.append(sect)\n",
    "                \n",
    "        expanded_input = np.concatenate(sects,axis=1)        \n",
    "        es = expanded_input.shape\n",
    "        flattened_input = expanded_input.reshape(es[0]*es[1],-1)\n",
    "\n",
    "        kernel_output = flattened_input.dot(kernels)\n",
    "        layer_1 = tanh(kernel_output.reshape(es[0],-1))\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "\n",
    "        test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "\n",
    "    if(j % 1 == 0):\n",
    "        sys.stdout.write(\"\\n\" + \"I:\" + str(j) + \" Test-Acc:\" + \\\n",
    "                         str(test_correct_cnt/float(len(test_images))) + \\\n",
    "                         \" Train-Acc:\" + str(correct_cnt/float(len(images)))) \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1477ffad-c02c-45a5-9d5e-21d9c953312a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Acc:0.0288 Train-Acc:0.055\n",
      "I:1 Test-Acc:0.0273 Train-Acc:0.037\n",
      "I:2 Test-Acc:0.028 Train-Acc:0.037\n",
      "I:3 Test-Acc:0.0292 Train-Acc:0.04\n",
      "I:4 Test-Acc:0.0339 Train-Acc:0.046\n",
      "I:5 Test-Acc:0.0478 Train-Acc:0.068\n",
      "I:6 Test-Acc:0.076 Train-Acc:0.083\n",
      "I:7 Test-Acc:0.1316 Train-Acc:0.096\n",
      "I:8 Test-Acc:0.2137 Train-Acc:0.127\n",
      "I:9 Test-Acc:0.2941 Train-Acc:0.148\n",
      "I:10 Test-Acc:0.3563 Train-Acc:0.181\n",
      "I:11 Test-Acc:0.4023 Train-Acc:0.209\n",
      "I:12 Test-Acc:0.4358 Train-Acc:0.238\n",
      "I:13 Test-Acc:0.4473 Train-Acc:0.286\n",
      "I:14 Test-Acc:0.4389 Train-Acc:0.274\n",
      "I:15 Test-Acc:0.3951 Train-Acc:0.257\n",
      "I:16 Test-Acc:0.2222 Train-Acc:0.243\n",
      "I:17 Test-Acc:0.0613 Train-Acc:0.112\n",
      "I:18 Test-Acc:0.0266 Train-Acc:0.035\n",
      "I:19 Test-Acc:0.0127 Train-Acc:0.026\n",
      "I:20 Test-Acc:0.0133 Train-Acc:0.022\n",
      "I:21 Test-Acc:0.0185 Train-Acc:0.038\n",
      "I:22 Test-Acc:0.0363 Train-Acc:0.038\n",
      "I:23 Test-Acc:0.0928 Train-Acc:0.067\n",
      "I:24 Test-Acc:0.1994 Train-Acc:0.081\n",
      "I:25 Test-Acc:0.3086 Train-Acc:0.154\n",
      "I:26 Test-Acc:0.4276 Train-Acc:0.204\n",
      "I:27 Test-Acc:0.5323 Train-Acc:0.256\n",
      "I:28 Test-Acc:0.5919 Train-Acc:0.305\n",
      "I:29 Test-Acc:0.6324 Train-Acc:0.341\n",
      "I:30 Test-Acc:0.6608 Train-Acc:0.426\n",
      "I:31 Test-Acc:0.6815 Train-Acc:0.439\n",
      "I:32 Test-Acc:0.7048 Train-Acc:0.462\n",
      "I:33 Test-Acc:0.7171 Train-Acc:0.484\n",
      "I:34 Test-Acc:0.7313 Train-Acc:0.505\n",
      "I:35 Test-Acc:0.7355 Train-Acc:0.53\n",
      "I:36 Test-Acc:0.7417 Train-Acc:0.548\n",
      "I:37 Test-Acc:0.747 Train-Acc:0.534\n",
      "I:38 Test-Acc:0.7491 Train-Acc:0.55\n",
      "I:39 Test-Acc:0.7459 Train-Acc:0.562\n",
      "I:40 Test-Acc:0.7352 Train-Acc:0.54\n",
      "I:41 Test-Acc:0.7082 Train-Acc:0.496\n",
      "I:42 Test-Acc:0.6487 Train-Acc:0.456\n",
      "I:43 Test-Acc:0.5209 Train-Acc:0.353\n",
      "I:44 Test-Acc:0.3305 Train-Acc:0.234\n",
      "I:45 Test-Acc:0.2052 Train-Acc:0.174\n",
      "I:46 Test-Acc:0.2149 Train-Acc:0.136\n",
      "I:47 Test-Acc:0.2679 Train-Acc:0.171\n",
      "I:48 Test-Acc:0.3237 Train-Acc:0.172\n",
      "I:49 Test-Acc:0.3581 Train-Acc:0.186\n",
      "I:50 Test-Acc:0.4202 Train-Acc:0.21\n",
      "I:51 Test-Acc:0.5165 Train-Acc:0.223\n",
      "I:52 Test-Acc:0.6007 Train-Acc:0.262\n",
      "I:53 Test-Acc:0.6476 Train-Acc:0.308\n",
      "I:54 Test-Acc:0.676 Train-Acc:0.363\n",
      "I:55 Test-Acc:0.696 Train-Acc:0.402\n",
      "I:56 Test-Acc:0.7077 Train-Acc:0.434\n",
      "I:57 Test-Acc:0.7204 Train-Acc:0.441\n",
      "I:58 Test-Acc:0.7303 Train-Acc:0.475\n",
      "I:59 Test-Acc:0.7359 Train-Acc:0.475\n",
      "I:60 Test-Acc:0.7401 Train-Acc:0.525\n",
      "I:61 Test-Acc:0.7493 Train-Acc:0.517\n",
      "I:62 Test-Acc:0.7533 Train-Acc:0.517\n",
      "I:63 Test-Acc:0.7606 Train-Acc:0.538\n",
      "I:64 Test-Acc:0.7644 Train-Acc:0.554\n",
      "I:65 Test-Acc:0.7724 Train-Acc:0.57\n",
      "I:66 Test-Acc:0.7788 Train-Acc:0.586\n",
      "I:67 Test-Acc:0.7855 Train-Acc:0.595\n",
      "I:68 Test-Acc:0.7853 Train-Acc:0.591\n",
      "I:69 Test-Acc:0.7925 Train-Acc:0.605\n",
      "I:70 Test-Acc:0.7973 Train-Acc:0.64\n",
      "I:71 Test-Acc:0.8013 Train-Acc:0.621\n",
      "I:72 Test-Acc:0.8029 Train-Acc:0.626\n",
      "I:73 Test-Acc:0.8092 Train-Acc:0.631\n",
      "I:74 Test-Acc:0.8099 Train-Acc:0.638\n",
      "I:75 Test-Acc:0.8156 Train-Acc:0.661\n",
      "I:76 Test-Acc:0.8156 Train-Acc:0.639\n",
      "I:77 Test-Acc:0.8184 Train-Acc:0.65\n",
      "I:78 Test-Acc:0.8216 Train-Acc:0.67\n",
      "I:79 Test-Acc:0.8246 Train-Acc:0.675\n",
      "I:80 Test-Acc:0.8237 Train-Acc:0.666\n",
      "I:81 Test-Acc:0.8273 Train-Acc:0.673\n",
      "I:82 Test-Acc:0.8273 Train-Acc:0.704\n",
      "I:83 Test-Acc:0.8314 Train-Acc:0.674\n",
      "I:84 Test-Acc:0.8292 Train-Acc:0.686\n",
      "I:85 Test-Acc:0.8335 Train-Acc:0.699\n",
      "I:86 Test-Acc:0.8359 Train-Acc:0.694\n",
      "I:87 Test-Acc:0.8375 Train-Acc:0.704\n",
      "I:88 Test-Acc:0.8373 Train-Acc:0.697\n",
      "I:89 Test-Acc:0.8398 Train-Acc:0.704\n",
      "I:90 Test-Acc:0.8393 Train-Acc:0.687\n",
      "I:91 Test-Acc:0.8436 Train-Acc:0.705\n",
      "I:92 Test-Acc:0.8437 Train-Acc:0.711\n",
      "I:93 Test-Acc:0.8446 Train-Acc:0.721\n",
      "I:94 Test-Acc:0.845 Train-Acc:0.719\n",
      "I:95 Test-Acc:0.8469 Train-Acc:0.724\n",
      "I:96 Test-Acc:0.8476 Train-Acc:0.726\n",
      "I:97 Test-Acc:0.848 Train-Acc:0.718\n",
      "I:98 Test-Acc:0.8496 Train-Acc:0.719\n",
      "I:99 Test-Acc:0.85 Train-Acc:0.73\n",
      "I:100 Test-Acc:0.8511 Train-Acc:0.737\n",
      "I:101 Test-Acc:0.8503 Train-Acc:0.73\n",
      "I:102 Test-Acc:0.8504 Train-Acc:0.717\n",
      "I:103 Test-Acc:0.8528 Train-Acc:0.74\n",
      "I:104 Test-Acc:0.8532 Train-Acc:0.733\n",
      "I:105 Test-Acc:0.8537 Train-Acc:0.73\n",
      "I:106 Test-Acc:0.8568 Train-Acc:0.721\n",
      "I:107 Test-Acc:0.857 Train-Acc:0.75\n",
      "I:108 Test-Acc:0.8558 Train-Acc:0.731\n",
      "I:109 Test-Acc:0.8578 Train-Acc:0.744\n",
      "I:110 Test-Acc:0.8588 Train-Acc:0.754\n",
      "I:111 Test-Acc:0.8579 Train-Acc:0.732\n",
      "I:112 Test-Acc:0.8582 Train-Acc:0.747\n",
      "I:113 Test-Acc:0.8593 Train-Acc:0.747\n",
      "I:114 Test-Acc:0.8598 Train-Acc:0.751\n",
      "I:115 Test-Acc:0.8603 Train-Acc:0.74\n",
      "I:116 Test-Acc:0.86 Train-Acc:0.753\n",
      "I:117 Test-Acc:0.8588 Train-Acc:0.746\n",
      "I:118 Test-Acc:0.861 Train-Acc:0.741\n",
      "I:119 Test-Acc:0.8616 Train-Acc:0.731\n",
      "I:120 Test-Acc:0.8629 Train-Acc:0.753\n",
      "I:121 Test-Acc:0.8609 Train-Acc:0.743\n",
      "I:122 Test-Acc:0.8627 Train-Acc:0.752\n",
      "I:123 Test-Acc:0.8646 Train-Acc:0.76\n",
      "I:124 Test-Acc:0.8649 Train-Acc:0.766\n",
      "I:125 Test-Acc:0.8659 Train-Acc:0.752\n",
      "I:126 Test-Acc:0.868 Train-Acc:0.756\n",
      "I:127 Test-Acc:0.8648 Train-Acc:0.767\n",
      "I:128 Test-Acc:0.8662 Train-Acc:0.747\n",
      "I:129 Test-Acc:0.8669 Train-Acc:0.753\n",
      "I:130 Test-Acc:0.8694 Train-Acc:0.753\n",
      "I:131 Test-Acc:0.8692 Train-Acc:0.76\n",
      "I:132 Test-Acc:0.8658 Train-Acc:0.756\n",
      "I:133 Test-Acc:0.8666 Train-Acc:0.769\n",
      "I:134 Test-Acc:0.8692 Train-Acc:0.77\n",
      "I:135 Test-Acc:0.8681 Train-Acc:0.757\n",
      "I:136 Test-Acc:0.8705 Train-Acc:0.77\n",
      "I:137 Test-Acc:0.8706 Train-Acc:0.77\n",
      "I:138 Test-Acc:0.8684 Train-Acc:0.768\n",
      "I:139 Test-Acc:0.8664 Train-Acc:0.774\n",
      "I:140 Test-Acc:0.8666 Train-Acc:0.756\n",
      "I:141 Test-Acc:0.8705 Train-Acc:0.783\n",
      "I:142 Test-Acc:0.87 Train-Acc:0.775\n",
      "I:143 Test-Acc:0.8729 Train-Acc:0.769\n",
      "I:144 Test-Acc:0.8725 Train-Acc:0.776\n",
      "I:145 Test-Acc:0.8721 Train-Acc:0.772\n",
      "I:146 Test-Acc:0.8718 Train-Acc:0.765\n",
      "I:147 Test-Acc:0.8746 Train-Acc:0.777\n",
      "I:148 Test-Acc:0.8746 Train-Acc:0.77\n",
      "I:149 Test-Acc:0.8734 Train-Acc:0.778\n",
      "I:150 Test-Acc:0.873 Train-Acc:0.785\n",
      "I:151 Test-Acc:0.8732 Train-Acc:0.76\n",
      "I:152 Test-Acc:0.8727 Train-Acc:0.779\n",
      "I:153 Test-Acc:0.8754 Train-Acc:0.772\n",
      "I:154 Test-Acc:0.8729 Train-Acc:0.773\n",
      "I:155 Test-Acc:0.8758 Train-Acc:0.784\n",
      "I:156 Test-Acc:0.8732 Train-Acc:0.774\n",
      "I:157 Test-Acc:0.8743 Train-Acc:0.782\n",
      "I:158 Test-Acc:0.8762 Train-Acc:0.772\n",
      "I:159 Test-Acc:0.8755 Train-Acc:0.79\n",
      "I:160 Test-Acc:0.8751 Train-Acc:0.774\n",
      "I:161 Test-Acc:0.8749 Train-Acc:0.782\n",
      "I:162 Test-Acc:0.8744 Train-Acc:0.78\n",
      "I:163 Test-Acc:0.8765 Train-Acc:0.782\n",
      "I:164 Test-Acc:0.8738 Train-Acc:0.796\n",
      "I:165 Test-Acc:0.8753 Train-Acc:0.798\n",
      "I:166 Test-Acc:0.8767 Train-Acc:0.794\n",
      "I:167 Test-Acc:0.8746 Train-Acc:0.784\n",
      "I:168 Test-Acc:0.8769 Train-Acc:0.796\n",
      "I:169 Test-Acc:0.8758 Train-Acc:0.789\n",
      "I:170 Test-Acc:0.8764 Train-Acc:0.79\n",
      "I:171 Test-Acc:0.873 Train-Acc:0.791\n",
      "I:172 Test-Acc:0.8765 Train-Acc:0.797\n",
      "I:173 Test-Acc:0.8772 Train-Acc:0.789\n",
      "I:174 Test-Acc:0.8778 Train-Acc:0.781\n",
      "I:175 Test-Acc:0.8758 Train-Acc:0.799\n",
      "I:176 Test-Acc:0.8773 Train-Acc:0.785\n",
      "I:177 Test-Acc:0.8766 Train-Acc:0.796\n",
      "I:178 Test-Acc:0.8782 Train-Acc:0.803\n",
      "I:179 Test-Acc:0.8789 Train-Acc:0.794\n",
      "I:180 Test-Acc:0.8778 Train-Acc:0.794\n",
      "I:181 Test-Acc:0.8778 Train-Acc:0.8\n",
      "I:182 Test-Acc:0.8785 Train-Acc:0.791\n",
      "I:183 Test-Acc:0.8777 Train-Acc:0.787\n",
      "I:184 Test-Acc:0.8769 Train-Acc:0.781\n",
      "I:185 Test-Acc:0.8765 Train-Acc:0.786\n",
      "I:186 Test-Acc:0.8765 Train-Acc:0.793\n",
      "I:187 Test-Acc:0.8785 Train-Acc:0.796\n",
      "I:188 Test-Acc:0.879 Train-Acc:0.789\n",
      "I:189 Test-Acc:0.8763 Train-Acc:0.79\n",
      "I:190 Test-Acc:0.8774 Train-Acc:0.787\n",
      "I:191 Test-Acc:0.8766 Train-Acc:0.782\n",
      "I:192 Test-Acc:0.8803 Train-Acc:0.798\n",
      "I:193 Test-Acc:0.8781 Train-Acc:0.789\n",
      "I:194 Test-Acc:0.8795 Train-Acc:0.785\n",
      "I:195 Test-Acc:0.8791 Train-Acc:0.807\n",
      "I:196 Test-Acc:0.8778 Train-Acc:0.796\n",
      "I:197 Test-Acc:0.8783 Train-Acc:0.801\n",
      "I:198 Test-Acc:0.8778 Train-Acc:0.81\n",
      "I:199 Test-Acc:0.8771 Train-Acc:0.784\n",
      "I:200 Test-Acc:0.8776 Train-Acc:0.792\n",
      "I:201 Test-Acc:0.8784 Train-Acc:0.794\n",
      "I:202 Test-Acc:0.8787 Train-Acc:0.795\n",
      "I:203 Test-Acc:0.8803 Train-Acc:0.781\n",
      "I:204 Test-Acc:0.8798 Train-Acc:0.804\n",
      "I:205 Test-Acc:0.8779 Train-Acc:0.779\n",
      "I:206 Test-Acc:0.8788 Train-Acc:0.792\n",
      "I:207 Test-Acc:0.8764 Train-Acc:0.793\n",
      "I:208 Test-Acc:0.8792 Train-Acc:0.792\n",
      "I:209 Test-Acc:0.8798 Train-Acc:0.803\n",
      "I:210 Test-Acc:0.8788 Train-Acc:0.804\n",
      "I:211 Test-Acc:0.8793 Train-Acc:0.797\n",
      "I:212 Test-Acc:0.8764 Train-Acc:0.791\n",
      "I:213 Test-Acc:0.8801 Train-Acc:0.801\n",
      "I:214 Test-Acc:0.8814 Train-Acc:0.799\n",
      "I:215 Test-Acc:0.8806 Train-Acc:0.79\n",
      "I:216 Test-Acc:0.8799 Train-Acc:0.8\n",
      "I:217 Test-Acc:0.8803 Train-Acc:0.802\n",
      "I:218 Test-Acc:0.8782 Train-Acc:0.807\n",
      "I:219 Test-Acc:0.8818 Train-Acc:0.797\n",
      "I:220 Test-Acc:0.8793 Train-Acc:0.799\n",
      "I:221 Test-Acc:0.8789 Train-Acc:0.815\n",
      "I:222 Test-Acc:0.8791 Train-Acc:0.816\n",
      "I:223 Test-Acc:0.8793 Train-Acc:0.809\n",
      "I:224 Test-Acc:0.8814 Train-Acc:0.795\n",
      "I:225 Test-Acc:0.8798 Train-Acc:0.799\n",
      "I:226 Test-Acc:0.8805 Train-Acc:0.806\n",
      "I:227 Test-Acc:0.88 Train-Acc:0.808\n",
      "I:228 Test-Acc:0.8782 Train-Acc:0.801\n",
      "I:229 Test-Acc:0.8802 Train-Acc:0.814\n",
      "I:230 Test-Acc:0.8807 Train-Acc:0.8\n",
      "I:231 Test-Acc:0.8809 Train-Acc:0.798\n",
      "I:232 Test-Acc:0.8805 Train-Acc:0.82\n",
      "I:233 Test-Acc:0.8795 Train-Acc:0.794\n",
      "I:234 Test-Acc:0.8807 Train-Acc:0.806\n",
      "I:235 Test-Acc:0.8806 Train-Acc:0.808\n",
      "I:236 Test-Acc:0.8787 Train-Acc:0.802\n",
      "I:237 Test-Acc:0.8796 Train-Acc:0.81\n",
      "I:238 Test-Acc:0.8766 Train-Acc:0.805\n",
      "I:239 Test-Acc:0.8781 Train-Acc:0.792\n",
      "I:240 Test-Acc:0.8787 Train-Acc:0.809\n",
      "I:241 Test-Acc:0.8762 Train-Acc:0.802\n",
      "I:242 Test-Acc:0.8775 Train-Acc:0.811\n",
      "I:243 Test-Acc:0.8804 Train-Acc:0.814\n",
      "I:244 Test-Acc:0.8794 Train-Acc:0.804\n",
      "I:245 Test-Acc:0.8788 Train-Acc:0.801\n",
      "I:246 Test-Acc:0.8777 Train-Acc:0.795\n",
      "I:247 Test-Acc:0.8785 Train-Acc:0.808\n",
      "I:248 Test-Acc:0.8788 Train-Acc:0.803\n",
      "I:249 Test-Acc:0.8773 Train-Acc:0.813\n",
      "I:250 Test-Acc:0.8786 Train-Acc:0.808\n",
      "I:251 Test-Acc:0.8787 Train-Acc:0.803\n",
      "I:252 Test-Acc:0.8789 Train-Acc:0.812\n",
      "I:253 Test-Acc:0.8792 Train-Acc:0.804\n",
      "I:254 Test-Acc:0.8779 Train-Acc:0.815\n",
      "I:255 Test-Acc:0.8796 Train-Acc:0.811\n",
      "I:256 Test-Acc:0.8798 Train-Acc:0.806\n",
      "I:257 Test-Acc:0.88 Train-Acc:0.803\n",
      "I:258 Test-Acc:0.8776 Train-Acc:0.795\n",
      "I:259 Test-Acc:0.8798 Train-Acc:0.803\n",
      "I:260 Test-Acc:0.8799 Train-Acc:0.805\n",
      "I:261 Test-Acc:0.8789 Train-Acc:0.807\n",
      "I:262 Test-Acc:0.8784 Train-Acc:0.804\n",
      "I:263 Test-Acc:0.8792 Train-Acc:0.806\n",
      "I:264 Test-Acc:0.8777 Train-Acc:0.796\n",
      "I:265 Test-Acc:0.8785 Train-Acc:0.821\n",
      "I:266 Test-Acc:0.8794 Train-Acc:0.81\n",
      "I:267 Test-Acc:0.8783 Train-Acc:0.816\n",
      "I:268 Test-Acc:0.8777 Train-Acc:0.812\n",
      "I:269 Test-Acc:0.8791 Train-Acc:0.812\n",
      "I:270 Test-Acc:0.878 Train-Acc:0.813\n",
      "I:271 Test-Acc:0.8784 Train-Acc:0.82\n",
      "I:272 Test-Acc:0.8792 Train-Acc:0.821\n",
      "I:273 Test-Acc:0.8781 Train-Acc:0.823\n",
      "I:274 Test-Acc:0.8788 Train-Acc:0.816\n",
      "I:275 Test-Acc:0.8793 Train-Acc:0.82\n",
      "I:276 Test-Acc:0.8781 Train-Acc:0.829\n",
      "I:277 Test-Acc:0.8795 Train-Acc:0.809\n",
      "I:278 Test-Acc:0.875 Train-Acc:0.806\n",
      "I:279 Test-Acc:0.8795 Train-Acc:0.813\n",
      "I:280 Test-Acc:0.88 Train-Acc:0.816\n",
      "I:281 Test-Acc:0.8796 Train-Acc:0.819\n",
      "I:282 Test-Acc:0.8802 Train-Acc:0.809\n",
      "I:283 Test-Acc:0.8804 Train-Acc:0.811\n",
      "I:284 Test-Acc:0.8779 Train-Acc:0.808\n",
      "I:285 Test-Acc:0.8816 Train-Acc:0.82\n",
      "I:286 Test-Acc:0.8792 Train-Acc:0.822\n",
      "I:287 Test-Acc:0.8791 Train-Acc:0.817\n",
      "I:288 Test-Acc:0.8769 Train-Acc:0.814\n",
      "I:289 Test-Acc:0.8785 Train-Acc:0.807\n",
      "I:290 Test-Acc:0.8778 Train-Acc:0.817\n",
      "I:291 Test-Acc:0.8794 Train-Acc:0.82\n",
      "I:292 Test-Acc:0.8804 Train-Acc:0.824\n",
      "I:293 Test-Acc:0.8779 Train-Acc:0.812\n",
      "I:294 Test-Acc:0.8784 Train-Acc:0.816\n",
      "I:295 Test-Acc:0.877 Train-Acc:0.817\n",
      "I:296 Test-Acc:0.8767 Train-Acc:0.826\n",
      "I:297 Test-Acc:0.8774 Train-Acc:0.816\n",
      "I:298 Test-Acc:0.8774 Train-Acc:0.804\n",
      "I:299 Test-Acc:0.8774 Train-Acc:0.814"
     ]
    }
   ],
   "source": [
    "import numpy as np, sys\n",
    "np.random.seed(1)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "images, labels = (x_train[0:1000].reshape(1000,28*28) / 255,\n",
    "                  y_train[0:1000])\n",
    "\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels),10))\n",
    "for i,l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test),28*28) / 255\n",
    "test_labels = np.zeros((len(y_test),10))\n",
    "for i,l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh2deriv(output):\n",
    "    return 1 - (output ** 2)\n",
    "\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "\n",
    "alpha, iterations = (2, 300)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "batch_size = 128\n",
    "\n",
    "input_rows = 28\n",
    "input_cols = 28\n",
    "\n",
    "kernel_rows = 3\n",
    "kernel_cols = 3\n",
    "num_kernels = 16\n",
    "\n",
    "hidden_size = ((input_rows - kernel_rows) * \n",
    "               (input_cols - kernel_cols)) * num_kernels\n",
    "\n",
    "# weights_0_1 = 0.02*np.random.random((pixels_per_image,hidden_size))-0.01\n",
    "kernels = 0.02*np.random.random((kernel_rows*kernel_cols,\n",
    "                                 num_kernels))-0.01\n",
    "\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size,\n",
    "                                    num_labels)) - 0.1\n",
    "\n",
    "\n",
    "\n",
    "def get_image_section(layer,row_from, row_to, col_from, col_to):\n",
    "    section = layer[:,row_from:row_to,col_from:col_to]\n",
    "    return section.reshape(-1,1,row_to-row_from, col_to-col_from)\n",
    "\n",
    "for j in range(iterations):\n",
    "    correct_cnt = 0\n",
    "    for i in range(int(len(images) / batch_size)):\n",
    "        batch_start, batch_end=((i * batch_size),((i+1)*batch_size))\n",
    "        layer_0 = images[batch_start:batch_end]\n",
    "        layer_0 = layer_0.reshape(layer_0.shape[0],28,28)\n",
    "        layer_0.shape\n",
    "\n",
    "        sects = list()\n",
    "        for row_start in range(layer_0.shape[1]-kernel_rows):\n",
    "            for col_start in range(layer_0.shape[2] - kernel_cols):\n",
    "                sect = get_image_section(layer_0,\n",
    "                                         row_start,\n",
    "                                         row_start+kernel_rows,\n",
    "                                         col_start,\n",
    "                                         col_start+kernel_cols)\n",
    "                sects.append(sect)\n",
    "\n",
    "        expanded_input = np.concatenate(sects,axis=1)\n",
    "        es = expanded_input.shape\n",
    "        flattened_input = expanded_input.reshape(es[0]*es[1],-1)\n",
    "\n",
    "        kernel_output = flattened_input.dot(kernels)\n",
    "        layer_1 = tanh(kernel_output.reshape(es[0],-1))\n",
    "        dropout_mask = np.random.randint(2,size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        layer_2 = softmax(np.dot(layer_1,weights_1_2))\n",
    "\n",
    "        for k in range(batch_size):\n",
    "            labelset = labels[batch_start+k:batch_start+k+1]\n",
    "            _inc = int(np.argmax(layer_2[k:k+1]) == \n",
    "                               np.argmax(labelset))\n",
    "            correct_cnt += _inc\n",
    "\n",
    "        layer_2_delta = (labels[batch_start:batch_end]-layer_2)\\\n",
    "                        / (batch_size * layer_2.shape[0])\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * \\\n",
    "                        tanh2deriv(layer_1)\n",
    "        layer_1_delta *= dropout_mask\n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        l1d_reshape = layer_1_delta.reshape(kernel_output.shape)\n",
    "        k_update = flattened_input.T.dot(l1d_reshape)\n",
    "        kernels -= alpha * k_update\n",
    "    \n",
    "    test_correct_cnt = 0\n",
    "\n",
    "    for i in range(len(test_images)):\n",
    "\n",
    "        layer_0 = test_images[i:i+1]\n",
    "#         layer_1 = tanh(np.dot(layer_0,weights_0_1))\n",
    "        layer_0 = layer_0.reshape(layer_0.shape[0],28,28)\n",
    "        layer_0.shape\n",
    "\n",
    "        sects = list()\n",
    "        for row_start in range(layer_0.shape[1]-kernel_rows):\n",
    "            for col_start in range(layer_0.shape[2] - kernel_cols):\n",
    "                sect = get_image_section(layer_0,\n",
    "                                         row_start,\n",
    "                                         row_start+kernel_rows,\n",
    "                                         col_start,\n",
    "                                         col_start+kernel_cols)\n",
    "                sects.append(sect)\n",
    "\n",
    "        expanded_input = np.concatenate(sects,axis=1)\n",
    "        es = expanded_input.shape\n",
    "        flattened_input = expanded_input.reshape(es[0]*es[1],-1)\n",
    "\n",
    "        kernel_output = flattened_input.dot(kernels)\n",
    "        layer_1 = tanh(kernel_output.reshape(es[0],-1))\n",
    "        layer_2 = np.dot(layer_1,weights_1_2)\n",
    "\n",
    "        test_correct_cnt += int(np.argmax(layer_2) == \n",
    "                                np.argmax(test_labels[i:i+1]))\n",
    "    if(j % 1 == 0):\n",
    "        sys.stdout.write(\"\\n\"+ \\\n",
    "         \"I:\" + str(j) + \\\n",
    "         \" Test-Acc:\"+str(test_correct_cnt/float(len(test_images)))+\\\n",
    "         \" Train-Acc:\" + str(correct_cnt/float(len(images))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb729ee-2283-4005-9353-923a50469068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
