{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92afc519-a22c-405e-9b49-7379fa0dbd4a",
   "metadata": {},
   "source": [
    "1. **Import Libraries**: \n",
    "    ```python\n",
    "    import sys, numpy as np\n",
    "    from keras.datasets import mnist\n",
    "    ```\n",
    "    - `sys`: Standard Python library for accessing the Python runtime environment.\n",
    "    - `numpy as np`: NumPy library for numerical operations.\n",
    "    - `mnist from keras.datasets`: Importing the MNIST dataset from Keras.\n",
    "    - `About the data`:<p>Before importing the data from MNIST, the data is already in matrix format. In Keras and many other machine learning libraries,<br>datasets like MNIST are usually stored in a format that is easy to load into memory as NumPy arrays or similar data structures.<br>This allows for quick and efficient manipulation of the data, which is essential for machine learning tasks.<br><br>It is quite common for machine learning datasets to be distributed in formats that are immediately usable for model training, such as NumPy arrays, CSV files, or other specialized formats.However, in some cases, especially in custom projects or when working with new datasets, you might have to deal with raw image files (.png, .jpg, etc.) or other types of unstructured data.<br><br>In such cases, you would use image processing libraries like PIL or OpenCV in Python to read the image files and convert them into NumPy arrays.Additionally, you might perform other preprocessing steps like resizing, normalization, or data augmentation, before using the data for training a machine learning model.So while mature datasets often come preprocessed and ready-to-use, real-world projects may require you to handle the rawdata yourself.</p><br>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda1f955-8fb8-4e5d-b297-e5a171b13802",
   "metadata": {},
   "source": [
    "2. **Load Data**:\n",
    "    ```python\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    ```\n",
    "    This line loads the MNIST dataset, separating it into training and test sets for both images (`x_train`, `x_test`) and labels (`y_train`, `y_test`).\n",
    "    <p>When you use the mnist.load_data() function from Keras, the MNIST data is already preprocessed and stored in NumPy arrays. The images are not in their original .png or .jpeg file formats; they are arrays that contain the pixel values of the images.The x_train and x_test arrays typically contain grayscale image data, where each entry is a 2D array of shape (28, 28) representing the pixel values of a single image. Each pixel value is an integer ranging from 0 to 255.<br><br>Similarly, y_train and y_test contain the labels for the training and test sets, respectively. These labels are also stored as integers in NumPy arrays.So, to summarize, when you load the MNIST data using mnist.load_data(), you get the images and labels in the form of NumPy arrays, not as raw image files.\n",
    "\n",
    "    The line `(x_train, y_train), (x_test, y_test) = mnist.load_data()` simply loads the MNIST dataset and separates it into training and testing sets, but it doesn't specify how many images are in each set. \n",
    "\n",
    "    In the standard MNIST dataset loaded via Keras, there are 60,000 training images and 10,000 testing images by default. The training images are stored in `x_train`, and their corresponding labels are stored in `y_train`. Similarly, the testing images are stored in `x_test`, and their labels are in `y_test`.</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cae3da",
   "metadata": {},
   "source": [
    "3. **Preprocess Images and Labels**:\n",
    "    ```python\n",
    "    images, labels = (x_train[0:1000].reshape(1000,28*28) / 255, y_train[0:1000])\n",
    "    ```\n",
    "    - Only the first 1000 images and labels are used.\n",
    "    - Images are reshaped from 28x28 to a flat vector of 28*28=784.\n",
    "    - Pixel values are normalized by dividing by 255.\n",
    "\n",
    "    <p>However, it's worth noting that the code snippet you provided only uses the first 1000 training images for its operations, as specified by this line: `images, labels = (x_train[0:1000].reshape(1000,28*28) / 255, y_train[0:1000])`.\n",
    "    \n",
    "    Q:\"Preprocess image and labels only the first 1000 images and labels are used\" what about the rest of image we don't use them at all? then why import the whole dataset?\n",
    "    The code snippet you provided is probably a simplified example meant for demonstration or testing purposes. In such cases, using a smaller subset of the dataset allows for quicker execution and debugging. However, in a real-world application, you would typically use the entire dataset for training to achieve better model performance.\n",
    "\n",
    "    Importing the whole dataset might be the default behavior of the `mnist.load_data()` function. The code then takes only the first 1000 images for demonstration, possibly to make the example run faster and to make it easier to understand for educational purposes.\n",
    "\n",
    "    In practice, if you only wanted a subset of the dataset, you could load the data once, create a subset, and then save that subset for future use. Alternatively, many libraries provide ways to load only a portion of the data, although the Keras `mnist.load_data()` function does not have this option.\n",
    "\n",
    "    So, in summary, the rest of the images aren't used in this specific example, but in a more comprehensive application, you would likely use all available data.\n",
    "    \n",
    "    Q:why Pixel values are normalized by dividing by 255?\n",
    "    Normalizing pixel values is a common preprocessing step in computer vision tasks that involve neural networks. In the 8-bit grayscale images commonly used in datasets like MNIST, pixel values range from 0 to 255. By dividing by 255, you ensure that all pixel values will lie in the range [0, 1].\n",
    "\n",
    "    Here are a few reasons why this normalization is beneficial:\n",
    "    1. **Numerical Stability**: Small input values are generally more numerically stable. Neural networks often involve many multiplicative operations, and having a large value at the start could lead to larger and larger values during training, which could cause numerical instability.\n",
    "\n",
    "    2. **Faster Convergence**: Many optimization algorithms converge faster when dealing with smaller numbers. This makes the training process more efficient.\n",
    "\n",
    "    3. **Consistency**: It's often easier to work with smaller, dimensionless numbers, especially when comparing the performance of different types of neural networks on the same data set.\n",
    "\n",
    "    4. **Weights Initialization**: Neural network weights are often initialized with small random values. The activations can grow very large if the input values are large, which can lead to issues like the vanishing or exploding gradients problem.\n",
    "\n",
    "    5. **Regularization**: Keeping input values small can also act as a form of implicit regularization, preventing any one feature from having too much influence on the training process.\n",
    "\n",
    "    Overall, normalizing inputs is a common best practice when working with neural networks.</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1095d",
   "metadata": {},
   "source": [
    "4. **One-hot Encoding for Labels**:\n",
    "    ```python\n",
    "    one_hot_labels = np.zeros((len(labels),10))\n",
    "    for i,l in enumerate(labels):\n",
    "        one_hot_labels[i][l] = 1\n",
    "    labels = one_hot_labels\n",
    "    ```\n",
    "    - Labels are converted to one-hot encoding. E.g., label 2 becomes [0,0,1,0,...,0].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5258fc6f",
   "metadata": {},
   "source": [
    "5. **Preprocess Test Images and Labels**:\n",
    "    ```python\n",
    "    test_images = x_test.reshape(len(x_test),28*28) / 255\n",
    "    test_labels = np.zeros((len(y_test),10))\n",
    "    for i,l in enumerate(y_test):\n",
    "        test_labels[i][l] = 1\n",
    "    ```\n",
    "    - Similar preprocessing is done for the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f31b6",
   "metadata": {},
   "source": [
    "6. **Initialize Parameters and Hyperparameters**:\n",
    "    ```python\n",
    "    np.random.seed(1)\n",
    "    relu = lambda x:(x>=0) * x \n",
    "    relu2deriv = lambda x: x>=0 \n",
    "    alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.005, 350, 40, 784, 10)\n",
    "    ```\n",
    "    - Random seed set for reproducibility.\n",
    "    - Two lambda functions defined for ReLU and its derivative.\n",
    "    - Hyperparameters like learning rate (`alpha`), number of iterations, hidden layer size, etc., are initialized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fafe07c",
   "metadata": {},
   "source": [
    "7. **Initialize Weights**:\n",
    "    ```python\n",
    "    weights_0_1 = 0.2*np.random.random((pixels_per_image,hidden_size)) - 0.1\n",
    "    weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1\n",
    "    ```\n",
    "    - Weights for the connections between input and hidden layer (weights_0_1), and between hidden and output layer (weights_1_2) are initialized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
