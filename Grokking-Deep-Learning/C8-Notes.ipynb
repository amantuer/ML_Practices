{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92afc519-a22c-405e-9b49-7379fa0dbd4a",
   "metadata": {},
   "source": [
    "1. **Import Libraries**: \n",
    "    ```python\n",
    "    import sys, numpy as np\n",
    "    from keras.datasets import mnist\n",
    "    ```\n",
    "    - `sys`: Standard Python library for accessing the Python runtime environment.\n",
    "    - `numpy as np`: NumPy library for numerical operations.\n",
    "    - `mnist from keras.datasets`: Importing the MNIST dataset from Keras.\n",
    "    - `About the data`:<p>Before importing the data from MNIST, the data is already in matrix format. In Keras and many other machine learning libraries,<br>datasets like MNIST are usually stored in a format that is easy to load into memory as NumPy arrays or similar data structures.<br>This allows for quick and efficient manipulation of the data, which is essential for machine learning tasks.<br><br>It is quite common for machine learning datasets to be distributed in formats that are immediately usable for model training, such as NumPy arrays, CSV files, or other specialized formats.However, in some cases, especially in custom projects or when working with new datasets, you might have to deal with raw image files (.png, .jpg, etc.) or other types of unstructured data.<br><br>In such cases, you would use image processing libraries like PIL or OpenCV in Python to read the image files and convert them into NumPy arrays.Additionally, you might perform other preprocessing steps like resizing, normalization, or data augmentation, before using the data for training a machine learning model.So while mature datasets often come preprocessed and ready-to-use, real-world projects may require you to handle the rawdata yourself.</p><br>      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda1f955-8fb8-4e5d-b297-e5a171b13802",
   "metadata": {},
   "source": [
    "2. **Load Data**:\n",
    "    ```python\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    ```\n",
    "    This line loads the MNIST dataset, separating it into training and test sets for both images (`x_train`, `x_test`) and labels (`y_train`, `y_test`).\n",
    "    <p>When you use the mnist.load_data() function from Keras, the MNIST data is already preprocessed and stored in NumPy arrays. The images are not in their original .png or .jpeg file formats; they are arrays that contain the pixel values of the images.The x_train and x_test arrays typically contain grayscale image data, where each entry is a 2D array of shape (28, 28) representing the pixel values of a single image. Each pixel value is an integer ranging from 0 to 255.<br><br>Similarly, y_train and y_test contain the labels for the training and test sets, respectively. These labels are also stored as integers in NumPy arrays.So, to summarize, when you load the MNIST data using mnist.load_data(), you get the images and labels in the form of NumPy arrays, not as raw image files.\n",
    "\n",
    "    The line `(x_train, y_train), (x_test, y_test) = mnist.load_data()` simply loads the MNIST dataset and separates it into training and testing sets, but it doesn't specify how many images are in each set. \n",
    "\n",
    "    In the standard MNIST dataset loaded via Keras, there are 60,000 training images and 10,000 testing images by default. The training images are stored in `x_train`, and their corresponding labels are stored in `y_train`. Similarly, the testing images are stored in `x_test`, and their labels are in `y_test`.</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cae3da",
   "metadata": {},
   "source": [
    "3. **Preprocess Images and Labels**:\n",
    "    ```python\n",
    "    images, labels = (x_train[0:1000].reshape(1000,28*28) / 255, y_train[0:1000])\n",
    "    ```\n",
    "    - Only the first 1000 images and labels are used.\n",
    "    - Images are reshaped from 28x28 to a flat vector of 28*28=784.\n",
    "    - Pixel values are normalized by dividing by 255.\n",
    "\n",
    "    However, it's worth noting that the code snippet you provided only uses the first 1000 training images for its operations, as specified by this line: `images, labels = (x_train[0:1000].reshape(1000,28*28) / 255, y_train[0:1000])`.\n",
    "    \n",
    "    Q:\"Preprocess image and labels only the first 1000 images and labels are used\" what about the rest of image we don't use them at all? then why import the whole dataset?\n",
    "    \n",
    "    The code snippet you provided is probably a simplified example meant for demonstration or testing purposes. In such cases, using a smaller subset of the dataset allows for quicker execution and debugging. However, in a real-world application, you would typically use the entire dataset for training to achieve better model performance.\n",
    "\n",
    "    Importing the whole dataset might be the default behavior of the `mnist.load_data()` function. The code then takes only the first 1000 images for demonstration, possibly to make the example run faster and to make it easier to understand for educational purposes.\n",
    "\n",
    "    In practice, if you only wanted a subset of the dataset, you could load the data once, create a subset, and then save that subset for future use. Alternatively, many libraries provide ways to load only a portion of the data, although the Keras `mnist.load_data()` function does not have this option.\n",
    "\n",
    "    So, in summary, the rest of the images aren't used in this specific example, but in a more comprehensive application, you would likely use all available data.\n",
    "    \n",
    "    Q:why Pixel values are normalized by dividing by 255?\n",
    "    \n",
    "    Normalizing pixel values is a common preprocessing step in computer vision tasks that involve neural networks. In the 8-bit grayscale images commonly used in datasets like MNIST, pixel values range from 0 to 255. By dividing by 255, you ensure that all pixel values will lie in the range [0, 1].\n",
    "\n",
    "    Here are a few reasons why this normalization is beneficial:\n",
    "    1. **Numerical Stability**: Small input values are generally more numerically stable. Neural networks often involve many multiplicative operations, and having a large value at the start could lead to larger and larger values during training, which could cause numerical instability.\n",
    "\n",
    "    2. **Faster Convergence**: Many optimization algorithms converge faster when dealing with smaller numbers. This makes the training process more efficient.\n",
    "\n",
    "    3. **Consistency**: It's often easier to work with smaller, dimensionless numbers, especially when comparing the performance of different types of neural networks on the same data set.\n",
    "\n",
    "    4. **Weights Initialization**: Neural network weights are often initialized with small random values. The activations can grow very large if the input values are large, which can lead to issues like the vanishing or exploding gradients problem.\n",
    "\n",
    "    5. **Regularization**: Keeping input values small can also act as a form of implicit regularization, preventing any one feature from having too much influence on the training process.\n",
    "\n",
    "    Overall, normalizing inputs is a common best practice when working with neural networks.\n",
    "\n",
    "    Q:two questions: 1. what's '0:1000' represent in x_trains[0:1000] ? 2. what's 1000 represent in reshape(1000,28*28)/255?\n",
    "\n",
    "    1. In Python, `x_train[0:1000]` uses slicing to select elements from the `x_train` array. The `0:1000` specifies that you want to start at the first element (index `0`) and go up to, but not include, the element at index `1000`. So, it will select the elements at indices `0, 1, 2, ..., 999`, effectively giving you the first 1000 elements in the `x_train` array.\n",
    "\n",
    "    2. In `reshape(1000, 28*28) / 255`, the `1000` indicates that the reshaped array should have 1000 rows. This corresponds to the number of images (each of size `28*28`) you are reshaping. The division by `255` is for normalization, scaling the pixel values to be between 0 and 1.\n",
    "\n",
    "    To summarize:\n",
    "    - `x_train[0:1000]` selects the first 1000 training samples.\n",
    "    - `reshape(1000, 28*28) / 255` reshapes those 1000 training samples each to be a 1D array of length `28*28` and scales the values to be between 0 and 1.\n",
    "\n",
    "    Q:reshape(1000, 28*28) means reshaped array should have 1000 rows and 784 columns?\n",
    "\n",
    "    Yes, that's correct. The `reshape(1000, 28*28)` operation will produce an array with 1000 rows and 784 columns. Each row corresponds to an image from the original dataset, and the 784 columns represent the pixel values of that image, which has been flattened from its original 28x28 shape to a 1D array of length 784."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1095d",
   "metadata": {},
   "source": [
    "4. **One-hot Encoding for Labels**:\n",
    "    ```python\n",
    "    one_hot_labels = np.zeros((len(labels),10))\n",
    "    for i,l in enumerate(labels):\n",
    "        one_hot_labels[i][l] = 1\n",
    "    labels = one_hot_labels\n",
    "    ```\n",
    "    - Labels are converted to one-hot encoding. E.g., label 2 becomes [0,0,1,0,...,0].\n",
    "\n",
    "    Q:what is one-hot encoding?\n",
    "\n",
    "    One-hot encoding is a method of converting categorical data variables so they can be provided to machine learning algorithms to improve predictions. In the context of machine learning, especially deep learning, one-hot encoding is often used for converting categorical labels into a format that can be provided to the algorithms without introducing errors.\n",
    "\n",
    "    For example, let's say you have a label set for digit recognition that consists of `[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]`. Using one-hot encoding, each of these labels would be converted into a vector of length 10, where 9 of the values in the vector are 0, and one of them is 1, at the index corresponding to the label value.\n",
    "\n",
    "    - The label `0` would be encoded as `[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]`\n",
    "    - The label `1` would be encoded as `[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]`\n",
    "    - The label `2` would be encoded as `[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]`\n",
    "    - ...\n",
    "    - The label `9` would be encoded as `[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]`\n",
    "\n",
    "    This type of encoding can help the algorithm distinguish between different categorical values, without mistakenly learning incorrect attributes from the numerical coding of these values.\n",
    "\n",
    "    In the MNIST example you provided earlier, the labels were one-hot encoded to represent each of the 10 possible digits (0 through 9). This makes it easier for the neural network to classify the images into one of the 10 digit classes.\n",
    "\n",
    "    Q:So if I have a label set for digit recogntion that consists of [0,1,2,.....,9999] then label 0 be encoded as [1,0,.....,0] vector of length 10000. label 9999 be encoded as [0,....,1] vector of length 10000. Is this correct?\n",
    "\n",
    "    Yes, that's correct. If you have a label set ranging from 0 to 9999, then each label would be represented by a one-hot encoded vector of length 10,000. For label 0, the vector would have a \"1\" at the first index and zeros everywhere else. Similarly, for label 9999, the vector would have a \"1\" at the last index (index 9999 if counting from 0) and zeros everywhere else. This results in a very high-dimensional representation, but it would be accurate for a categorical variable with 10,000 distinct categories.\n",
    "\n",
    "    Q:explain every line of code here:\n",
    "\n",
    "    Certainly, this piece of code converts the given labels into their one-hot encoded representations.\n",
    "    one_hot_labels = np.zeros((len(labels), 10))  # Create an array of zeros with dimensions (number of samples, number of classes)\n",
    "    ```python\n",
    "    for i, l in enumerate(labels):  # Loop over each sample and its corresponding label\n",
    "        one_hot_labels[i][l] = 1  # Set the index corresponding to the label to 1\n",
    "    labels = one_hot_labels  # Assign the one-hot encoded labels back to 'labels'\n",
    "    ```\n",
    "    1. `one_hot_labels = np.zeros((len(labels), 10))`: This line initializes a NumPy array filled with zeros. The dimensions are `(len(labels), 10)` where `len(labels)` is the number of samples and `10` is the number of categories or classes. In this case, it is designed for digit recognition, so there are 10 categories (0-9).\n",
    "\n",
    "    2. `for i, l in enumerate(labels)`: This line starts a loop that will go through each label in the `labels` array. The function `enumerate()` provides both the index `i` and the label value `l`.\n",
    "\n",
    "    3. `one_hot_labels[i][l] = 1`: Inside the loop, this line sets the value `1` at the index corresponding to the label `l` for the `i`-th sample. Since the labels are for digits (0-9), the index `l` directly corresponds to the digit label. For example, if `l=3`, then the one-hot encoded vector will have `1` at its 3rd index (counting from 0) and zeros everywhere else.\n",
    "\n",
    "    4. `labels = one_hot_labels`: This line replaces the original `labels` array with its one-hot encoded version stored in `one_hot_labels`.\n",
    "\n",
    "    After running this code, the original labels are transformed into their one-hot encoded forms, making them suitable for training a neural network for a classification problem.\n",
    "\n",
    "    Q:at the index corresponding to the label l for the i-th sample. label \"l\" is 'one' or 'L' ? \n",
    "\n",
    "    The label is \"l\" (lowercase 'L'), not \"one\" or '1'. In the loop `for i, l in enumerate(labels):`, the variable `l` is used to store the value of the label for the `i`-th sample. This `l` (lowercase 'L') corresponds to the digit that the `i`-th sample represents and is used to index into the one-hot encoded array to set the appropriate value to 1.\n",
    "\n",
    "    `i` and `l` are not the same in general.\n",
    "\n",
    "    - `i` is the index of the sample, ranging from 0 to `len(labels) - 1`.\n",
    "    - `l` is the actual label of the sample at index `i`.\n",
    "\n",
    "    The loop is enumerating over `labels`, so `i` gets the index and `l` gets the corresponding label. For example, if your `labels` array is `[2, 0, 4, 1]`:\n",
    "\n",
    "    - When `i = 0`, `l = 2`\n",
    "    - When `i = 1`, `l = 0`\n",
    "    - When `i = 2`, `l = 4`\n",
    "    - When `i = 3`, `l = 1`\n",
    "\n",
    "    Then the code `one_hot_labels[i][l] = 1` will do the following:\n",
    "\n",
    "    - For `i = 0`, it sets `one_hot_labels[0][2] = 1` (the 3rd element in the first row to 1, since Python is 0-indexed).\n",
    "    - For `i = 1`, it sets `one_hot_labels[1][0] = 1` (the 1st element in the second row to 1).\n",
    "    - And so on...\n",
    "\n",
    "    So `i` is the index in `one_hot_labels` where we want to set a `1`, and `l` is the position within that row where the `1` should be set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5258fc6f",
   "metadata": {},
   "source": [
    "5. **Preprocess Test Images and Labels**:\n",
    "    ```python\n",
    "    test_images = x_test.reshape(len(x_test),28*28) / 255\n",
    "    test_labels = np.zeros((len(y_test),10))\n",
    "    for i,l in enumerate(y_test):\n",
    "        test_labels[i][l] = 1\n",
    "    ```\n",
    "    - Similar preprocessing is done for the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f31b6",
   "metadata": {},
   "source": [
    "6. **Initialize Parameters and Hyperparameters**:\n",
    "    ```python\n",
    "    np.random.seed(1)\n",
    "    relu = lambda x:(x>=0) * x \n",
    "    relu2deriv = lambda x: x>=0 \n",
    "    alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.005, 350, 40, 784, 10)\n",
    "    ```\n",
    "    - Random seed set for reproducibility.\n",
    "    - Two lambda functions defined for ReLU and its derivative.\n",
    "    - Hyperparameters like learning rate (`alpha`), number of iterations, hidden layer size, etc., are initialized.\n",
    "\n",
    "    Q:how does this line works? \"np.random.seed(1)\" and is hidden_size is the value of hidden layer numbers?\n",
    "\n",
    "    1. `np.random.seed(1)`: This line sets the random seed for NumPy's random number generator to 1. The random seed is a starting point for the sequence of random numbers. When you set the seed, you ensure that the sequence of random numbers is reproducible. This is often useful in machine learning experiments where repeatability is important.\n",
    "\n",
    "    2. `hidden_size`: In this code, `hidden_size` refers to the number of neurons (or \"units\") in the hidden layer, not the number of hidden layers. If `hidden_size` is set to 4, for example, then the hidden layer would consist of 4 neurons.\n",
    "\n",
    "    Q:so hidden_size = 40 means one hidden layer with 40 neurons, am I correct?\n",
    "\n",
    "    Yes, that's correct. `hidden_size = 40` indicates that there is a single hidden layer in the neural network, and that hidden layer has 40 neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fafe07c",
   "metadata": {},
   "source": [
    "7. **Initialize Weights**:\n",
    "    ```python\n",
    "    weights_0_1 = 0.2*np.random.random((pixels_per_image,hidden_size)) - 0.1\n",
    "    weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1\n",
    "    ```\n",
    "    - Weights for the connections between input and hidden layer (weights_0_1), and between hidden and output layer (weights_1_2) are initialized.\n",
    "\n",
    "    Q:how does \"np.random.random()\" this works? and why the parameters passed to it in weights_0_1 and weights_1_2 are different?\n",
    "\n",
    "    The function `np.random.random(size)` generates an array of random floats in the range [0, 1) with the shape defined by `size`. For example, if `size=(3, 4)`, it will generate a 3x4 matrix with random float values between 0 and 1.\n",
    "\n",
    "    Here's a breakdown of the weights initialization:\n",
    "\n",
    "    1. `weights_0_1 = 0.2*np.random.random((pixels_per_image, hidden_size)) - 0.1`: This initializes the weights connecting the input layer to the hidden layer. The shape `(pixels_per_image, hidden_size)` tells us that each input neuron (pixel in the image) is connected to every neuron in the hidden layer. `0.2*np.random.random() - 0.1` scales these random weights to be in the range of [-0.1, 0.1).\n",
    "\n",
    "    2. `weights_1_2 = 0.2*np.random.random((hidden_size, num_labels)) - 0.1`: This initializes the weights connecting the hidden layer to the output layer. Again, every neuron in the hidden layer is connected to every output label. The weights are also scaled to be in the range [-0.1, 0.1].\n",
    "\n",
    "    The reason that the shapes of `weights_0_1` and `weights_1_2` are different is that they are connecting different layers with different numbers of neurons:\n",
    "\n",
    "    - `weights_0_1` is connecting an input layer of size `pixels_per_image` to a hidden layer of size `hidden_size`.\n",
    "  \n",
    "    - `weights_1_2` is connecting a hidden layer of size `hidden_size` to an output layer of size `num_labels`.\n",
    "\n",
    "    So their shapes reflect the architecture of the network and how the layers are interconnected.\n",
    "\n",
    "    Q:why it will generate a matrix with random float values between 0 and 1? why between 0 and 1? how it know we want it's between 0 and 1? we didn't pass 0 and 1 to it right?\n",
    "\n",
    "    The behavior of generating random floats between 0 and 1 is the default behavior of the function `np.random.random()`. It's designed to produce numbers in the range [0, 1) as per its specification. You don't have to explicitly pass the range [0, 1) to it; it's implicitly understood to be the default range for this function.\n",
    "\n",
    "    If you want to generate random numbers in a different range, you can scale and shift the output of `np.random.random()` yourself, like the code does: `0.2*np.random.random() - 0.1` scales and shifts the range to [-0.1, 0.1). \n",
    "\n",
    "    This is just a design choice made in the NumPy library for this particular function. Different functions in NumPy can generate random numbers in different ranges and types, for example:\n",
    "\n",
    "    - `np.random.randint(low, high)` generates random integers between `low` and `high`.\n",
    "  \n",
    "    - `np.random.uniform(low, high)` generates random floats between `low` and `high`.\n",
    "\n",
    "    The goal is to offer a suite of functions that can cover a wide variety of random number generation needs.\n",
    "\n",
    "    Q:what's the difference between 'np.random.uniform(low, high)' and 'np.random.random(low, high)'?\n",
    "\n",
    "    The function `np.random.random` does not actually take `low` and `high` arguments for specifying a range. It generates random floats in the half-open interval [0.0, 1.0). If you provide a tuple, like `(m, n)`, it will generate an array of shape `(m, n)` with random floats in the [0, 1) range.\n",
    "\n",
    "    On the other hand, `np.random.uniform(low, high)` generates random floats in the half-open interval `[low, high)`. The `low` and `high` parameters specify the range for the random numbers. If you want to produce an array, you would also need to specify the `size` parameter.\n",
    "\n",
    "    Here are some examples to illustrate the difference:\n",
    "\n",
    "    ```python\n",
    "    # Generate a single random number in the range [0, 1)\n",
    "    print(np.random.random())  # Output might be: 0.123456789\n",
    "\n",
    "    # Generate a single random number in the range [5, 10)\n",
    "    print(np.random.uniform(5, 10))  # Output might be: 7.123456789\n",
    "\n",
    "    # Generate an array of shape (2, 3) with random numbers in the range [0, 1)\n",
    "    print(np.random.random((2, 3)))  \n",
    "    # Output might be:\n",
    "    # [[0.1, 0.2, 0.3],\n",
    "    #  [0.4, 0.5, 0.6]]\n",
    "\n",
    "    # Generate an array of shape (2, 3) with random numbers in the range [5, 10)\n",
    "    print(np.random.uniform(5, 10, (2, 3)))  \n",
    "    # Output might be:\n",
    "    # [[5.1, 5.2, 5.3],\n",
    "    #  [9.4, 9.5, 9.6]]\n",
    "    ```\n",
    "\n",
    "    So, `np.random.uniform` provides more control over the range of the random numbers, whereas `np.random.random` is a more specialized function that provides numbers specifically in the [0, 1) range."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
