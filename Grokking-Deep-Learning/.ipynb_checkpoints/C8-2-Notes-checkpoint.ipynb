{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf7ec76-7def-47c5-9628-8955654d3022",
   "metadata": {},
   "source": [
    "**<em>Q:in the next part of this DL book it called this code as 'Drop out code' what's the reason?</em>**\n",
    "```python\n",
    "import numpy, sys\n",
    "np.random.seed(1)\n",
    "def relu(x):\n",
    "    return (x >= 0) * x # returns x if x > 0\n",
    "                        # returns 0 otherwise\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output >= 0 #returns 1 for input > 0\n",
    "\n",
    "alpha, iterations, hidden_size = (0.005, 300, 100)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image,hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations):\n",
    "    error, correct_cnt = (0.0,0)\n",
    "    for i in range(len(images)):\n",
    "        layer_0 = images[i:i+1]\n",
    "        layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        layer_2 = np.dot(layer_1,weights_1_2)\n",
    "\n",
    "        error += np.sum((labels[i:i+1] - layer_2) ** 2)\n",
    "        correct_cnt += int(np.argmax(layer_2) == np.argmax(labels[i:i+1]))\n",
    "        layer_2_delta = (labels[i:i+1] - layer_2)\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "        layer_1_delta *= dropout_mask\n",
    "\n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "    if(j%10 == 0):\n",
    "        test_error = 0.0\n",
    "        test_correct_cnt = 0\n",
    "\n",
    "        for i in range(len(test_images)):\n",
    "            layer_0 = test_images[i:i+1]\n",
    "            layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "            layer_2 = np.dot(layer_1, weights_1_2)\n",
    "\n",
    "            test_error += np.sum((test_labels[i:i+1] - layer_2) ** 2)\n",
    "            test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "\n",
    "        sys.stdout.write(\"\\n\" + \\\n",
    "                         \"I:\" + str(j) + \\\n",
    "                         \" Test-Err:\" + str(test_error/ float(len(test_images)))[0:5] +\\\n",
    "                         \" Test-Acc:\" + str(test_correct_cnt/ float(len(test_images)))+\\\n",
    "                         \" Train-Err:\" + str(error/ float(len(images)))[0:5] +\\\n",
    "                         \" Train-Acc:\" + str(correct_cnt/ float(len(images))))\n",
    "```\n",
    "\n",
    "A:The code you posted includes dropout, a regularization technique used to prevent overfitting in neural networks. In this particular code, dropout is implemented through the `dropout_mask`.\r\n",
    "\r\n",
    "Here's the relevant part where dropout is applied:\r\n",
    "\r\n",
    "```python\r\n",
    "dropout_mask = np.random.randint(2, size=layer_1.shape)\r\n",
    "layer_1 *= dropout_mask * 2\r\n",
    "```\r\n",
    "\r\n",
    "1. **`dropout_mask = np.random.randint(2, size=layer_1.shape)`**: Creates a mask where each element is either 0 or 1. The mask has the same shape as `layer_1`.\r\n",
    "\r\n",
    "2. **`layer_1 *= dropout_mask * 2`**: The `dropout_mask` is applied to `layer_1`. This essentially sets approximately half of the activations in `layer_1` to zero. The remaining activations are scaled by 2 to keep the expected sum of activations the same.\r\n",
    "\r\n",
    "This dropout mask is also later applied to `layer_1_delta`:\r\n",
    "\r\n",
    "```python\r\n",
    "layer_1_delta *= dropout_mask\r\n",
    "```\r\n",
    "\r\n",
    "This ensures that during backpropagation, the neurons that were \"dropped out\" (set to zero) during the forward pass will also have no influence on learning; their corresponding gradients will be zero.\r\n",
    "\r\n",
    "The rest of the code is similar to the previous example you provided. It does forward and backward passes, computes errors and updates weights. The key difference is the inclusion of dropout, which adds a form of regularization to the model.\r\n",
    "\r\n",
    "The dropout is not applied to the test set; it's only used during training. This is standard practice. Dropout is a form of regularization used to make the model generalize better from the training set to unseen data.\r\n",
    "\r\n",
    "So, the code is called 'Dropout code' because it incorporates dropout regularization in the neural network training process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
