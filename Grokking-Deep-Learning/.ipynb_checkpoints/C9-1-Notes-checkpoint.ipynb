{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75dd8bca-fbe7-43d6-8d18-5a08a19f7e30",
   "metadata": {},
   "source": [
    "**<h2>Chapter9 - Intro to Activation Functions - Modeling Probabilities</h2>**\n",
    "\n",
    "**<em>Q:should the new code is better than old one because using new three functions?</em>**\n",
    "\n",
    "A:The effectiveness of the new code versus the old one depends on various factors including the performance metrics you are interested in (accuracy, speed, generalization, etc.). Here's a breakdown:\n",
    "\n",
    "### Advantages of Using the New Functions:\n",
    "\n",
    "1. **Softmax**: It's often more appropriate for a classification task, especially when the classes are mutually exclusive. It provides probabilities for each class, making the model's outputs more interpretable.\n",
    "  \n",
    "2. **Tanh**: The tanh activation function is zero-centered, meaning that negative inputs will be mapped strongly negative and zero inputs will be near zero in the output. This can make the optimization process easier and potentially quicker.\n",
    "\n",
    "3. **tanh2deriv**: Given that tanh is used as an activation function, having a function to calculate its derivative can make backpropagation easier and possibly more effective.\n",
    "\n",
    "### Potential Disadvantages:\n",
    "\n",
    "1. **Softmax**: Computationally more expensive than sigmoid or no activation at all.\n",
    "\n",
    "2. **Tanh and tanh2deriv**: The tanh function and its derivative could potentially be computationally more expensive than simpler functions like the ReLU (Rectified Linear Unit) and its derivative, depending on the implementation and the hardware.\n",
    "\n",
    "3. **Learning Rate (`alpha = 2`)**: The high learning rate might cause the model to overshoot the optimal weights and fail to converge, or to generalize poorly.\n",
    "\n",
    "4. **Dropout Implementation**: The new code uses dropout but doesn't compensate for the dropped-out neurons during training (by multiplying by 2), which could potentially affect training dynamics.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Neither version is inherently \"better\"; they are just suited for different requirements. If you're interested in model interpretability and potentially better training dynamics, the new functions might offer advantages. However, you'll have to test both versions rigorously to conclude which is better empirically for your specific use-case.\n",
    "\n",
    "### Three New Functions:\n",
    "\n",
    "#### 1. tanh(x)\n",
    "This function returns the hyperbolic tangent of the input `x`. It's an activation function that squashes the outputs to lie in the range of -1 and 1.\n",
    "\n",
    "```python\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "```\n",
    "\n",
    "#### 2. tanh2deriv(output)\n",
    "This function returns the derivative of the tanh function given its output. It's used for backpropagation to adjust the weights.\n",
    "\n",
    "```python\n",
    "def tanh2deriv(output):\n",
    "    return 1 - (output ** 2)\n",
    "```\n",
    "\n",
    "#### 3. softmax(x)\n",
    "This function applies the softmax function to the input `x`. It turns raw scores (logits) into probabilities. The softmax function is often used for the output layer of a classifier.\n",
    "\n",
    "```python\n",
    "def softmax(x):\n",
    "  temp = np.exp(x)\n",
    "  return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "```\n",
    "\n",
    "The softmax function is a commonly used activation function in neural networks, especially in the output layer for multi-class classification problems. The softmax function takes an \\(N\\)-dimensional vector of real numbers as input and normalizes it into a probability distribution, that is, a vector of positive numbers between 0 and 1 that add up to 1.\n",
    "\n",
    "The formula for the softmax function \\( \\text{softmax}(\\vec{x}) \\) applied to a vector \\( \\vec{x} = [x_1, x_2, ..., x_N] \\) is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{N} e^{x_j}}\n",
    "\\end{equation*}\n",
    "\n",
    "Here's how the code implements this:\n",
    "\n",
    "1. **Exponential Calculation**: `np.exp(x)` computes the exponential指数 of each element \\( x_i \\) in the input vector \\( \\vec{x} \\). This transforms each value to be non-negative. Exponentiating also accentuates the differences between the elements.(指数运算也突出了元素之间的差异。) The larger an element \\( x_i \\) is, the larger \\( e^{x_i} \\) will be, especially compared to other, smaller elements.\n",
    "\n",
    "    ```python\n",
    "    temp = np.exp(x)\n",
    "    ```\n",
    "\n",
    "2. **Normalization**: `np.sum(temp, axis=1, keepdims=True)` computes the sum of these exponentials along the specified axis (in this case, axis 1 corresponds to summing along each row in a 2D array). This serves as the denominator for the softmax function, ensuring that the resulting probability distribution adds up to 1.\n",
    "规范化：np.sum(temp，axis=1，keepdims=True)计算指定轴上这些指数的总和（在这种情况下，轴1对应于在2D数组中沿每行求和）。这用作softmax函数的分母，确保生成的概率分布总和为1。\n",
    "\n",
    "    ```python\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "    ```\n",
    "\n",
    "The `axis=1, keepdims=True` ensures that the division is performed correctly. By setting `axis=1`, we sum along the columns (i.e., summing all the softmax scores for each sample in the batch). The `keepdims=True` ensures that the result has the same shape as the original array, making the division operation compatible.\n",
    "\n",
    "Certainly! Let's break down these three parameters (`temp`, `axis=1`, `keepdims=True`) and how they affect the function `softmax()`.\n",
    "\n",
    "### 1. `temp`\n",
    "\n",
    "The variable `temp` stores the exponentials of the input \\(x\\). Suppose \\(x\\) is a 2D array representing two examples with scores for three classes, i.e., \\(x = [[1, 2, 3], [1, 1, 1]]\\).\n",
    "\n",
    "Computing `temp = np.exp(x)` would give:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{temp} = \\left[ \\left[e^1, e^2, e^3\\right], \\left[e^1, e^1, e^1\\right] \\right] = \\left[ [2.72, 7.39, 20.08], [2.72, 2.72, 2.72] \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "Each element in the array is exponentiated, making them non-negative and emphasizing differences.\n",
    "\n",
    "### 2. `axis=1`\n",
    "\n",
    "The `axis=1` parameter specifies which axis to sum along. For a 2D array, `axis=0` would sum along columns, and `axis=1` would sum along rows.\n",
    "\n",
    "Continuing with our `temp` example:\n",
    "\n",
    "- Summing along `axis=1`: `np.sum(temp, axis=1)` would produce `[30.19, 8.16]`.\n",
    "\n",
    "This is essential for the softmax function because we want to normalize the scores for each example (each row) so that they form a probability distribution.\n",
    "\n",
    "### 3. `keepdims=True`\n",
    "\n",
    "Setting `keepdims=True` maintains the original dimensionality of the array after the sum operation. \n",
    "\n",
    "If you perform `np.sum(temp, axis=1, keepdims=True)`, it returns a 2D array `[[30.19], [8.16]]` instead of a 1D array `[30.19, 8.16]`.\n",
    "\n",
    "This is important for broadcasting rules in NumPy when we divide `temp` by this sum. If we didn't keep dimensions, NumPy would broadcast incorrectly, and we'd get an error or incorrect result.\n",
    "\n",
    "### Putting It All Together\n",
    "\n",
    "Finally, in `softmax()`:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{result} = \\frac{\\text{temp}}{\\text{np.sum(temp, axis=1, keepdims=True)}} = \\frac{\\left[ [2.72, 7.39, 20.08], [2.72, 2.72, 2.72] \\right]}{\\left[ [30.19], [8.16] \\right]} = \\left[ [0.09, 0.24, 0.67], [0.33, 0.33, 0.33] \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "Each row now sums to 1 and represents a probability distribution across the classes. The function has worked as intended!\n",
    "\n",
    "By implementing softmax this way, each row in the input 2D array \\( \\text{temp} \\) is transformed into a probability distribution over the classes (columns).\n",
    "\n",
    "**<em>Q:I mean the 't' in 'True' why need capitalization? in C++ we write \"true\" as \"true\" the 't' don't need capitalizaion. </em>**\n",
    "\n",
    "Ah, I see what you're asking. In Python, `True` and `False` are capitalized because they are built-in constants, and that's simply the convention the language designers chose. Python is case-sensitive, so `True` and `true` would be considered different (the latter would be considered a variable name and would throw an error if not defined).\n",
    "\n",
    "C++ and Python are different languages with different design philosophies and conventions. In C++, `true` and `false` are keywords and are all lowercase. However, Python chose to make its Boolean constants (`True` and `False`) start with a capital letter.\n",
    "\n",
    "There's no deep technical reason for this; it's simply a convention that you'll need to remember when working with Python. Different languages have different rules for syntax and naming, and this is one of those cases.\n",
    "\n",
    "在Python中，True和False是大写的，因为它们是内置常量，这只是语言设计者选择的约定。Python是区分大小写的，因此True和true被认为是不同的（后者被认为是变量名，如果未定义，则会引发错误）。\n",
    "\n",
    "C++和Python是不同的语言，具有不同的设计哲学和约定。在C++中，true和false是关键字，都是小写的。但是，Python选择使其布尔常量（True和False）以大写字母开头。\n",
    "\n",
    "这没有深层技术原因；这只是一种约定，你需要记住在使用Python时。不同的语言有不同的语法和命名规则，这是其中之一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d98cae5e-7cd8-475a-84fa-e67d689e7b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Acc:0.394 Train-Acc:0.156 Test-Err:2.0623 Train-Err:1.7959499953429915e-05\n",
      "I:10 Test-Acc:0.6867 Train-Acc:0.723 Test-Err:1.1351 Train-Err:1.6988449187755914e-05\n",
      "I:20 Test-Acc:0.7025 Train-Acc:0.732 Test-Err:1.0619 Train-Err:1.521588821935578e-05\n",
      "I:30 Test-Acc:0.734 Train-Acc:0.763 Test-Err:0.9422 Train-Err:1.3249866834745405e-05\n",
      "I:40 Test-Acc:0.7663 Train-Acc:0.794 Test-Err:0.8285 Train-Err:1.1221782527969216e-05\n",
      "I:50 Test-Acc:0.7913 Train-Acc:0.819 Test-Err:0.7367 Train-Err:1.0413538511490937e-05\n",
      "I:60 Test-Acc:0.8102 Train-Acc:0.849 Test-Err:0.666 Train-Err:9.225930193461022e-06\n",
      "I:70 Test-Acc:0.8228 Train-Acc:0.864 Test-Err:0.6202 Train-Err:8.408800041933423e-06\n",
      "I:80 Test-Acc:0.831 Train-Acc:0.867 Test-Err:0.5929 Train-Err:7.715483087549682e-06\n",
      "I:90 Test-Acc:0.8364 Train-Acc:0.885 Test-Err:0.5722 Train-Err:6.9307721515200216e-06\n",
      "I:100 Test-Acc:0.8407 Train-Acc:0.883 Test-Err:0.5538 Train-Err:6.504311898401096e-06\n",
      "I:110 Test-Acc:0.845 Train-Acc:0.891 Test-Err:0.5374 Train-Err:6.1418594199637175e-06\n",
      "I:120 Test-Acc:0.8481 Train-Acc:0.901 Test-Err:0.5242 Train-Err:5.582510796543297e-06\n",
      "I:130 Test-Acc:0.8505 Train-Acc:0.901 Test-Err:0.5153 Train-Err:5.05927399467817e-06\n",
      "I:140 Test-Acc:0.8526 Train-Acc:0.905 Test-Err:0.5057 Train-Err:5.314532486903257e-06\n",
      "I:150 Test-Acc:0.8555 Train-Acc:0.914 Test-Err:0.4927 Train-Err:4.732882806864228e-06\n",
      "I:160 Test-Acc:0.8577 Train-Acc:0.925 Test-Err:0.482 Train-Err:4.7495576482761835e-06\n",
      "I:170 Test-Acc:0.8596 Train-Acc:0.918 Test-Err:0.4766 Train-Err:4.494399371809181e-06\n",
      "I:180 Test-Acc:0.8619 Train-Acc:0.933 Test-Err:0.4681 Train-Err:4.155457717122059e-06\n",
      "I:190 Test-Acc:0.863 Train-Acc:0.933 Test-Err:0.4639 Train-Err:4.1722775894744e-06\n",
      "I:200 Test-Acc:0.8642 Train-Acc:0.926 Test-Err:0.4609 Train-Err:3.9841227915290795e-06\n",
      "I:210 Test-Acc:0.8653 Train-Acc:0.931 Test-Err:0.4547 Train-Err:3.5958833403581546e-06\n",
      "I:220 Test-Acc:0.8668 Train-Acc:0.93 Test-Err:0.4517 Train-Err:3.7299028543159522e-06\n",
      "I:230 Test-Acc:0.8672 Train-Acc:0.937 Test-Err:0.45 Train-Err:3.470508547575517e-06\n",
      "I:240 Test-Acc:0.8681 Train-Acc:0.938 Test-Err:0.4454 Train-Err:3.530156070791518e-06\n",
      "I:250 Test-Acc:0.8687 Train-Acc:0.937 Test-Err:0.4433 Train-Err:3.477504059058854e-06\n",
      "I:260 Test-Acc:0.8684 Train-Acc:0.945 Test-Err:0.4457 Train-Err:3.2561158656342336e-06\n",
      "I:270 Test-Acc:0.8703 Train-Acc:0.951 Test-Err:0.44 Train-Err:3.1169795332447647e-06\n",
      "I:280 Test-Acc:0.8699 Train-Acc:0.949 Test-Err:0.4394 Train-Err:2.9534299803403746e-06\n",
      "I:290 Test-Acc:0.8701 Train-Acc:0.94 Test-Err:0.4406 Train-Err:2.7816062716529986e-06"
     ]
    }
   ],
   "source": [
    "import numpy as np, sys\n",
    "np.random.seed(1)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "images, labels = (x_train[0:1000].reshape(1000,28*28) / 255, y_train[0:1000])\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels),10))\n",
    "for i,l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test),28*28) / 255\n",
    "test_labels = np.zeros((len(y_test),10))\n",
    "for i,l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tan2deriv(output):\n",
    "    return 1 - (output ** 2)\n",
    "\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "\n",
    "alpha, iterations, hidden_size = (2, 300, 100)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "batch_size = 100\n",
    "\n",
    "weights_0_1 = 0.02*np.random.random((pixels_per_image,hidden_size))-0.01\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations):\n",
    "    correct_cnt = 0\n",
    "    for i in range(int(len(images) / batch_size)):\n",
    "        batch_start, batch_end=((i * batch_size),((i+1)*batch_size))\n",
    "        layer_0 = images[batch_start:batch_end]\n",
    "        layer_1 = tanh(layer_0.dot(weights_0_1))\n",
    "        dropout_mask = np.random.randint(2,size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        layer_2 = softmax(layer_1.dot(weights_1_2))\n",
    "        \n",
    "        for k in range(batch_size):\n",
    "            correct_cnt += int(np.argmax(layer_2[k:k+1]) == np.argmax(labels[batch_start+k:batch_start+k+1]))\n",
    "\n",
    "        layer_2_delta = (labels[batch_start:batch_end]-layer_2) / (batch_size * layer_2.shape[0])\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * tan2deriv(layer_1)\n",
    "        layer_1_delta *= dropout_mask\n",
    "\n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "    test_correct_cnt = 0\n",
    "    test_total_error = 0\n",
    "\n",
    "    for i in range(len(test_images)):\n",
    "\n",
    "        layer_0 = test_images[i:i+1]\n",
    "        layer_1 = tanh(np.dot(layer_0,weights_0_1))\n",
    "        layer_2 = np.dot(layer_1,weights_1_2)\n",
    "\n",
    "        test_total_error += np.sum(np.abs(np.argmax(layer_2) - np.argmax(test_labels[i:i+1])))\n",
    "        test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "        \n",
    "    total_error = np.sum(np.abs(layer_2_delta))\n",
    "    avg_error = total_error / float(len(images))\n",
    "\n",
    "    test_avg_error = test_total_error / float(len(test_images))\n",
    "    \n",
    "    if(j % 10 == 0):\n",
    "        sys.stdout.write(\"\\n\"+ \\\n",
    "         \"I:\" + str(j) + \\\n",
    "         \" Test-Acc:\"+str(test_correct_cnt/float(len(test_images)))+\\\n",
    "         \" Train-Acc:\" + str(correct_cnt/float(len(images)))+\\\n",
    "         \" Test-Err:\" + str(test_avg_error) +\\\n",
    "         \" Train-Err:\" + str(avg_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f40d19a-502f-4d98-9183-309ace8d5be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Acc:0.072 Train-Acc:0.088 Test-Err:2.9287 Train-Err:1.8015329284054318e-05\n",
      "I:10 Test-Acc:0.0721 Train-Acc:0.069 Test-Err:2.9281 Train-Err:1.7998444927999157e-05\n",
      "I:20 Test-Acc:0.0722 Train-Acc:0.087 Test-Err:2.9279 Train-Err:1.8002056144353965e-05\n",
      "I:30 Test-Acc:0.0723 Train-Acc:0.068 Test-Err:2.9279 Train-Err:1.79944096441887e-05\n",
      "I:40 Test-Acc:0.0724 Train-Acc:0.066 Test-Err:2.9277 Train-Err:1.801123729156478e-05\n",
      "I:50 Test-Acc:0.0725 Train-Acc:0.093 Test-Err:2.9279 Train-Err:1.8003343919530262e-05\n",
      "I:60 Test-Acc:0.0726 Train-Acc:0.087 Test-Err:2.9269 Train-Err:1.8000515106158395e-05\n",
      "I:70 Test-Acc:0.0729 Train-Acc:0.082 Test-Err:2.9271 Train-Err:1.8001228860941025e-05\n",
      "I:80 Test-Acc:0.0729 Train-Acc:0.072 Test-Err:2.9275 Train-Err:1.8002585215358698e-05\n",
      "I:90 Test-Acc:0.0729 Train-Acc:0.083 Test-Err:2.9275 Train-Err:1.8001949170079207e-05\n",
      "I:100 Test-Acc:0.0729 Train-Acc:0.081 Test-Err:2.9275 Train-Err:1.8001920579155582e-05\n",
      "I:110 Test-Acc:0.073 Train-Acc:0.076 Test-Err:2.9276 Train-Err:1.800171388638528e-05\n",
      "I:120 Test-Acc:0.0731 Train-Acc:0.079 Test-Err:2.9269 Train-Err:1.800048753711417e-05\n",
      "I:130 Test-Acc:0.0731 Train-Acc:0.074 Test-Err:2.9269 Train-Err:1.7995353676478962e-05\n",
      "I:140 Test-Acc:0.0733 Train-Acc:0.084 Test-Err:2.9264 Train-Err:1.8010561942429765e-05\n",
      "I:150 Test-Acc:0.0734 Train-Acc:0.088 Test-Err:2.9263 Train-Err:1.7989999177013924e-05\n",
      "I:160 Test-Acc:0.0736 Train-Acc:0.07 Test-Err:2.9259 Train-Err:1.8002868682349674e-05\n",
      "I:170 Test-Acc:0.0736 Train-Acc:0.081 Test-Err:2.9259 Train-Err:1.8015844774226625e-05\n",
      "I:180 Test-Acc:0.0736 Train-Acc:0.078 Test-Err:2.9259 Train-Err:1.8001298462130687e-05\n",
      "I:190 Test-Acc:0.0736 Train-Acc:0.084 Test-Err:2.9261 Train-Err:1.8000838395696692e-05\n",
      "I:200 Test-Acc:0.0737 Train-Acc:0.09 Test-Err:2.9255 Train-Err:1.7989936295574085e-05\n",
      "I:210 Test-Acc:0.0738 Train-Acc:0.081 Test-Err:2.9248 Train-Err:1.799126638338961e-05\n",
      "I:220 Test-Acc:0.0738 Train-Acc:0.085 Test-Err:2.9252 Train-Err:1.799838036497354e-05\n",
      "I:230 Test-Acc:0.074 Train-Acc:0.091 Test-Err:2.924 Train-Err:1.7992913278884582e-05\n",
      "I:240 Test-Acc:0.074 Train-Acc:0.085 Test-Err:2.9241 Train-Err:1.8001670732587658e-05\n",
      "I:250 Test-Acc:0.074 Train-Acc:0.075 Test-Err:2.9253 Train-Err:1.8001504972396766e-05\n",
      "I:260 Test-Acc:0.074 Train-Acc:0.103 Test-Err:2.9257 Train-Err:1.7994610219903742e-05\n",
      "I:270 Test-Acc:0.074 Train-Acc:0.089 Test-Err:2.9258 Train-Err:1.7996405391359277e-05\n",
      "I:280 Test-Acc:0.074 Train-Acc:0.082 Test-Err:2.9258 Train-Err:1.7994815352916003e-05\n",
      "I:290 Test-Acc:0.0742 Train-Acc:0.082 Test-Err:2.9245 Train-Err:1.800748568963562e-05"
     ]
    }
   ],
   "source": [
    "#edit alpha=0.0001\n",
    "import numpy as np, sys\n",
    "np.random.seed(1)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "images, labels = (x_train[0:1000].reshape(1000,28*28) / 255, y_train[0:1000])\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels),10))\n",
    "for i,l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test),28*28) / 255\n",
    "test_labels = np.zeros((len(y_test),10))\n",
    "for i,l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tan2deriv(output):\n",
    "    return 1 - (output ** 2)\n",
    "\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "\n",
    "alpha, iterations, hidden_size = (0.0001, 300, 100)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "batch_size = 100\n",
    "\n",
    "weights_0_1 = 0.02*np.random.random((pixels_per_image,hidden_size))-0.01\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations):\n",
    "    correct_cnt = 0\n",
    "    for i in range(int(len(images) / batch_size)):\n",
    "        batch_start, batch_end=((i * batch_size),((i+1)*batch_size))\n",
    "        layer_0 = images[batch_start:batch_end]\n",
    "        layer_1 = tanh(layer_0.dot(weights_0_1))\n",
    "        dropout_mask = np.random.randint(2,size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        layer_2 = softmax(layer_1.dot(weights_1_2))\n",
    "        \n",
    "        for k in range(batch_size):\n",
    "            correct_cnt += int(np.argmax(layer_2[k:k+1]) == np.argmax(labels[batch_start+k:batch_start+k+1]))\n",
    "\n",
    "        layer_2_delta = (labels[batch_start:batch_end]-layer_2) / (batch_size * layer_2.shape[0])\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * tan2deriv(layer_1)\n",
    "        layer_1_delta *= dropout_mask\n",
    "\n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "    test_correct_cnt = 0\n",
    "    test_total_error = 0\n",
    "\n",
    "    for i in range(len(test_images)):\n",
    "\n",
    "        layer_0 = test_images[i:i+1]\n",
    "        layer_1 = tanh(np.dot(layer_0,weights_0_1))\n",
    "        layer_2 = np.dot(layer_1,weights_1_2)\n",
    "\n",
    "        test_total_error += np.sum(np.abs(np.argmax(layer_2) - np.argmax(test_labels[i:i+1])))\n",
    "        test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "        \n",
    "    total_error = np.sum(np.abs(layer_2_delta))\n",
    "    avg_error = total_error / float(len(images))\n",
    "\n",
    "    test_avg_error = test_total_error / float(len(test_images))\n",
    "    \n",
    "    if(j % 10 == 0):\n",
    "        sys.stdout.write(\"\\n\"+ \\\n",
    "         \"I:\" + str(j) + \\\n",
    "         \" Test-Acc:\"+str(test_correct_cnt/float(len(test_images)))+\\\n",
    "         \" Train-Acc:\" + str(correct_cnt/float(len(images)))+\\\n",
    "         \" Test-Err:\" + str(test_avg_error) +\\\n",
    "         \" Train-Err:\" + str(avg_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cce85db-0f86-4899-b8ca-2caf56c88c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
