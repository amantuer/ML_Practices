{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e65d33fa-b16e-4c64-967e-0f90ef70387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: -0.19999999999999996 Error:0.03999999999999998\n",
      "Pred: -0.15999999999999992 Error:0.025599999999999973\n",
      "Pred: -0.1279999999999999 Error:0.01638399999999997\n",
      "Pred: -0.10239999999999982 Error:0.010485759999999964\n",
      "Pred: -0.08191999999999977 Error:0.006710886399999962\n",
      "Pred: -0.06553599999999982 Error:0.004294967295999976\n",
      "Pred: -0.05242879999999994 Error:0.002748779069439994\n",
      "Pred: -0.04194304000000004 Error:0.0017592186044416036\n",
      "Pred: -0.03355443200000008 Error:0.0011258999068426293\n",
      "Pred: -0.02684354560000002 Error:0.0007205759403792803\n",
      "Pred: -0.021474836479999926 Error:0.0004611686018427356\n",
      "Pred: -0.01717986918399994 Error:0.0002951479051793508\n",
      "Pred: -0.013743895347199997 Error:0.00018889465931478573\n",
      "Pred: -0.010995116277759953 Error:0.00012089258196146188\n",
      "Pred: -0.008796093022207963 Error:7.737125245533561e-05\n",
      "Pred: -0.007036874417766459 Error:4.951760157141604e-05\n",
      "Pred: -0.0056294995342132115 Error:3.169126500570676e-05\n",
      "Pred: -0.004503599627370569 Error:2.028240960365233e-05\n",
      "Pred: -0.003602879701896544 Error:1.298074214633813e-05\n",
      "Pred: -0.002882303761517324 Error:8.307674973656916e-06\n"
     ]
    }
   ],
   "source": [
    "#Creating a Matrix or Two in Python\n",
    "import numpy as np\n",
    "weights = np.array([0.5,0.48,-0.7])\n",
    "alpha = 0.1\n",
    "\n",
    "streetlights = np.array( [ [ 1, 0, 1 ],\n",
    "                           [ 0, 1, 1 ],\n",
    "                           [ 0, 0, 1 ],\n",
    "                           [ 1, 1, 1 ],\n",
    "                           [ 0, 1, 1 ],\n",
    "                           [ 1, 0, 1 ] ] )\n",
    "\n",
    "walk_vs_stop = np.array( [ 0, 1, 0, 1, 1, 0 ] )\n",
    "\n",
    "input = streetlights[0] # [1,0,1]\n",
    "goal_prediction = walk_vs_stop[0] # equals 0... i.e. \"stop\"\n",
    "\n",
    "for iteration in range(20):\n",
    "    pred = input.dot(weights)\n",
    "    error = (pred - goal_prediction)**2\n",
    "    delta = pred - goal_prediction\n",
    "    weights_delta = input * delta\n",
    "    weights -= alpha * weights_delta\n",
    "    print(\"Pred: \" + str(pred) + \" Error:\" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9fcc12-787a-4f41-8043-d25fb4e1041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 4 3]\n",
      "[2 3 4 4]\n",
      "[0.  0.5 1.  0.5]\n",
      "[0.5 1.5 2.5 1.5]\n"
     ]
    }
   ],
   "source": [
    "#vector,scalar calculation\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([0,1,2,1])\n",
    "b = np.array([2,2,2,3])\n",
    "\n",
    "print(b*a) #elementwise multiplication\n",
    "print(b+a) #elementwise addication\n",
    "print(0.5*a) #vector-scalar multiplication\n",
    "print(0.5+a) #vector-scalar addication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d76f301-7aff-49ed-b6d2-01d8ad059a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predication: -0.19999999999999996 Weights:[ 0.52  0.48 -0.68]\n",
      "Predication: -0.19999999999999996 Weights:[ 0.52  0.6  -0.56]\n",
      "Predication: -0.5599999999999999 Weights:[ 0.52   0.6   -0.504]\n",
      "Predication: 0.6160000000000001 Weights:[ 0.5584  0.6384 -0.4656]\n",
      "Predication: 0.17279999999999995 Weights:[ 0.5584   0.72112 -0.38288]\n",
      "Predication: 0.17552 Weights:[ 0.540848  0.72112  -0.400432]\n",
      "Error_of_all: 2.6561231104\n",
      "\n",
      "Predication: 0.14041599999999999 Weights:[ 0.5268064  0.72112   -0.4144736]\n",
      "Predication: 0.3066464 Weights:[ 0.5268064   0.79045536 -0.34513824]\n",
      "Predication: -0.34513824 Weights:[ 0.5268064   0.79045536 -0.31062442]\n",
      "Predication: 1.006637344 Weights:[ 0.52614267  0.78979163 -0.31128815]\n",
      "Predication: 0.4785034751999999 Weights:[ 0.52614267  0.84194128 -0.2591385 ]\n",
      "Predication: 0.26700416768 Weights:[ 0.49944225  0.84194128 -0.28583891]\n",
      "Error_of_all: 0.9628701776715985\n",
      "\n",
      "Predication: 0.213603334144 Weights:[ 0.47808192  0.84194128 -0.30719925]\n",
      "Predication: 0.5347420299776 Weights:[ 0.47808192  0.88846708 -0.26067345]\n",
      "Predication: -0.26067345110016 Weights:[ 0.47808192  0.88846708 -0.23460611]\n",
      "Predication: 1.1319428845096962 Weights:[ 0.46488763  0.87527279 -0.24780039]\n",
      "Predication: 0.6274723921901568 Weights:[ 0.46488763  0.91252555 -0.21054763]\n",
      "Predication: 0.25433999330650114 Weights:[ 0.43945363  0.91252555 -0.23598163]\n",
      "Error_of_all: 0.5509165866836797\n",
      "\n",
      "Predication: 0.20347199464520088 Weights:[ 0.41910643  0.91252555 -0.25632883]\n",
      "Predication: 0.6561967149569552 Weights:[ 0.41910643  0.94690588 -0.2219485 ]\n",
      "Predication: -0.221948503950995 Weights:[ 0.41910643  0.94690588 -0.19975365]\n",
      "Predication: 1.166258650532124 Weights:[ 0.40248056  0.93028001 -0.21637952]\n",
      "Predication: 0.7139004922542389 Weights:[ 0.40248056  0.95888996 -0.18776957]\n",
      "Predication: 0.21471099528371604 Weights:[ 0.38100946  0.95888996 -0.20924067]\n",
      "Error_of_all: 0.36445836852222424\n",
      "\n",
      "Predication: 0.17176879622697283 Weights:[ 0.36383258  0.95888996 -0.22641755]\n",
      "Predication: 0.7324724146523222 Weights:[ 0.36383258  0.98564272 -0.19966479]\n",
      "Predication: -0.19966478845083285 Weights:[ 0.36383258  0.98564272 -0.17969831]\n",
      "Predication: 1.1697769945341199 Weights:[ 0.34685488  0.96866502 -0.19667601]\n",
      "Predication: 0.7719890116601171 Weights:[ 0.34685488  0.99146612 -0.17387491]\n",
      "Predication: 0.17297997428859369 Weights:[ 0.32955689  0.99146612 -0.19117291]\n",
      "Error_of_all: 0.2516768662079895\n",
      "\n",
      "Predication: 0.13838397943087496 Weights:[ 0.31571849  0.99146612 -0.20501131]\n",
      "Predication: 0.7864548139561468 Weights:[ 0.31571849  1.01282064 -0.18365679]\n",
      "Predication: -0.1836567869927348 Weights:[ 0.31571849  1.01282064 -0.16529111]\n",
      "Predication: 1.163248019006011 Weights:[ 0.29939369  0.99649584 -0.18161591]\n",
      "Predication: 0.8148799260629888 Weights:[ 0.29939369  1.01500784 -0.1631039 ]\n",
      "Predication: 0.1362897844408577 Weights:[ 0.28576471  1.01500784 -0.17673288]\n",
      "Error_of_all: 0.17797575048089034\n",
      "\n",
      "Predication: 0.10903182755268614 Weights:[ 0.27486153  1.01500784 -0.18763606]\n",
      "Predication: 0.8273717796510367 Weights:[ 0.27486153  1.03227067 -0.17037324]\n",
      "Predication: -0.17037324196481937 Weights:[ 0.27486153  1.03227067 -0.15333592]\n",
      "Predication: 1.1537962739591756 Weights:[ 0.2594819   1.01689104 -0.16871555]\n",
      "Predication: 0.8481754931254761 Weights:[ 0.2594819   1.03207349 -0.15353309]\n",
      "Predication: 0.1059488041691444 Weights:[ 0.24888702  1.03207349 -0.16412797]\n",
      "Error_of_all: 0.12864460733422164\n",
      "\n",
      "Predication: 0.0847590433353155 Weights:[ 0.24041111  1.03207349 -0.17260388]\n",
      "Predication: 0.859469609749935 Weights:[ 0.24041111  1.04612653 -0.15855084]\n",
      "Predication: -0.1585508402022421 Weights:[ 0.24041111  1.04612653 -0.14269576]\n",
      "Predication: 1.1438418857156731 Weights:[ 0.22602693  1.03174234 -0.15707994]\n",
      "Predication: 0.8746623946770374 Weights:[ 0.22602693  1.0442761  -0.14454618]\n",
      "Predication: 0.08148074110264475 Weights:[ 0.21787885  1.0442761  -0.15269426]\n",
      "Error_of_all: 0.09511036950476208\n",
      "\n",
      "Predication: 0.06518459288211581 Weights:[ 0.21136039  1.0442761  -0.15921272]\n",
      "Predication: 0.8850633823431538 Weights:[ 0.21136039  1.05576976 -0.14771906]\n",
      "Predication: -0.14771905585408038 Weights:[ 0.21136039  1.05576976 -0.13294715]\n",
      "Predication: 1.1341830033853888 Weights:[ 0.19794209  1.04235146 -0.14636545]\n",
      "Predication: 0.8959860107828534 Weights:[ 0.19794209  1.05275286 -0.13596405]\n",
      "Predication: 0.0619780399014222 Weights:[ 0.19174429  1.05275286 -0.14216186]\n",
      "Error_of_all: 0.07194564247043436\n",
      "\n",
      "Predication: 0.04958243192113776 Weights:[ 0.18678604  1.05275286 -0.1471201 ]\n",
      "Predication: 0.9056327614440267 Weights:[ 0.18678604  1.06218958 -0.13768338]\n",
      "Predication: -0.13768337501215525 Weights:[ 0.18678604  1.06218958 -0.12391504]\n",
      "Predication: 1.1250605910610996 Weights:[ 0.17427999  1.04968353 -0.1364211 ]\n",
      "Predication: 0.9132624284442169 Weights:[ 0.17427999  1.05835728 -0.12774734]\n",
      "Predication: 0.04653264583708144 Weights:[ 0.16962672  1.05835728 -0.1324006 ]\n",
      "Error_of_all: 0.05564914990717743\n",
      "\n",
      "Predication: 0.03722611666966513 Weights:[ 0.16590411  1.05835728 -0.13612322]\n",
      "Predication: 0.922234066504699 Weights:[ 0.16590411  1.06613388 -0.12834662]\n",
      "Predication: -0.12834662236261596 Weights:[ 0.16590411  1.06613388 -0.11551196]\n",
      "Predication: 1.116526024487899 Weights:[ 0.15425151  1.05448127 -0.12716456]\n",
      "Predication: 0.9273167105424409 Weights:[ 0.15425151  1.0617496  -0.11989623]\n",
      "Predication: 0.03435527296969987 Weights:[ 0.15081598  1.0617496  -0.12333176]\n",
      "Error_of_all: 0.04394763937673939\n",
      "\n",
      "Predication: 0.027484218375759886 Weights:[ 0.14806756  1.0617496  -0.12608018]\n",
      "Predication: 0.9356694192994068 Weights:[ 0.14806756  1.06818266 -0.11964712]\n",
      "Predication: -0.11964712469387503 Weights:[ 0.14806756  1.06818266 -0.10768241]\n",
      "Predication: 1.1085678053734553 Weights:[ 0.13721078  1.05732588 -0.11853919]\n",
      "Predication: 0.9387866868342218 Weights:[ 0.13721078  1.06344721 -0.11241786]\n",
      "Predication: 0.024792915481941458 Weights:[ 0.13473149  1.06344721 -0.11489715]\n",
      "Error_of_all: 0.035357967050948465\n",
      "\n",
      "Predication: 0.019834332385553155 Weights:[ 0.13274805  1.06344721 -0.11688059]\n",
      "Predication: 0.946566624680628 Weights:[ 0.13274805  1.06879055 -0.11153725]\n",
      "Predication: -0.11153724870006754 Weights:[ 0.13274805  1.06879055 -0.10038352]\n",
      "Predication: 1.1011550767549563 Weights:[ 0.12263254  1.05867504 -0.11049903]\n",
      "Predication: 0.948176009263518 Weights:[ 0.12263254  1.06385744 -0.10531663]\n",
      "Predication: 0.017315912033043404 Weights:[ 0.12090095  1.06385744 -0.10704822]\n",
      "Error_of_all: 0.02890700056547436\n",
      "\n",
      "Predication: 0.013852729626434732 Weights:[ 0.11951568  1.06385744 -0.1084335 ]\n",
      "Predication: 0.9554239432448665 Weights:[ 0.11951568  1.06831505 -0.10397589]\n",
      "Predication: -0.10397589092234266 Weights:[ 0.11951568  1.06831505 -0.0935783 ]\n",
      "Predication: 1.0942524239871314 Weights:[ 0.11009044  1.0588898  -0.10300354]\n",
      "Predication: 0.9558862588907013 Weights:[ 0.11009044  1.06330118 -0.09859217]\n",
      "Predication: 0.011498267782398985 Weights:[ 0.10894061  1.06330118 -0.099742  ]\n",
      "Error_of_all: 0.023951660591138853\n",
      "\n",
      "Predication: 0.009198614225919194 Weights:[ 0.10802075  1.06330118 -0.10066186]\n",
      "Predication: 0.9626393189117293 Weights:[ 0.10802075  1.06703725 -0.09692579]\n",
      "Predication: -0.09692579020989642 Weights:[ 0.10802075  1.06703725 -0.08723321]\n",
      "Predication: 1.087824783849832 Weights:[ 0.09923827  1.05825477 -0.09601569]\n",
      "Predication: 0.9622390773804066 Weights:[ 0.09923827  1.06203086 -0.0922396 ]\n",
      "Predication: 0.006998674002545002 Weights:[ 0.0985384   1.06203086 -0.09293946]\n",
      "Error_of_all: 0.020063105176016144\n",
      "\n",
      "Predication: 0.005598939202035996 Weights:[ 0.09797851  1.06203086 -0.09349936]\n",
      "Predication: 0.9685315005838672 Weights:[ 0.09797851  1.06517771 -0.09035251]\n",
      "Predication: -0.09035250869077546 Weights:[ 0.09797851  1.06517771 -0.08131726]\n",
      "Predication: 1.0818389613301889 Weights:[ 0.08979461  1.05699381 -0.08950115]\n",
      "Predication: 0.9674926590701334 Weights:[ 0.08979461  1.06024455 -0.08625042]\n",
      "Predication: 0.003544193999268516 Weights:[ 0.08944019  1.06024455 -0.08660484]\n",
      "Error_of_all: 0.016952094519447087\n",
      "\n",
      "Predication: 0.0028353551994148157 Weights:[ 0.08915666  1.06024455 -0.08688837]\n",
      "Predication: 0.9733561723362383 Weights:[ 0.08915666  1.06290893 -0.08422399]\n",
      "Predication: -0.0842239920152223 Weights:[ 0.08915666  1.06290893 -0.07580159]\n",
      "Predication: 1.0762639960116431 Weights:[ 0.08153026  1.05528253 -0.08342799]\n",
      "Predication: 0.9718545378681842 Weights:[ 0.08153026  1.05809708 -0.08061345]\n",
      "Predication: 0.0009168131382832068 Weights:[ 0.08143858  1.05809708 -0.08070513]\n",
      "Error_of_all: 0.014420818295271236\n",
      "\n",
      "Predication: 0.0007334505106265654 Weights:[ 0.08136523  1.05809708 -0.08077847]\n",
      "Predication: 0.9773186039296565 Weights:[ 0.08136523  1.06036522 -0.07851033]\n",
      "Predication: -0.07851033295953944 Weights:[ 0.08136523  1.06036522 -0.0706593 ]\n",
      "Predication: 1.0710711494147542 Weights:[ 0.07425812  1.0532581  -0.07776641]\n",
      "Predication: 0.9754916865567282 Weights:[ 0.07425812  1.05570893 -0.07531558]\n",
      "Predication: -0.0010574652271341245 Weights:[ 0.07436386  1.05570893 -0.07520984]\n",
      "Error_of_all: 0.012331739998443648\n",
      "\n",
      "Predication: -0.0008459721817072885 Weights:[ 0.07444846  1.05570893 -0.07512524]\n",
      "Predication: 0.9805836929862668 Weights:[ 0.07444846  1.05765056 -0.07318361]\n",
      "Predication: -0.07318360881847627 Weights:[ 0.07444846  1.05765056 -0.06586525]\n",
      "Predication: 1.066233777045345 Weights:[ 0.06782508  1.05102719 -0.07248863]\n",
      "Predication: 0.9785385598617921 Weights:[ 0.06782508  1.05317333 -0.07034248]\n",
      "Predication: -0.0025173975573930946 Weights:[ 0.06807682  1.05317333 -0.07009074]\n",
      "Error_of_all: 0.010587393171639842\n",
      "\n",
      "Predication: -0.002013918045914484 Weights:[ 0.06827822  1.05317333 -0.06988935]\n",
      "Predication: 0.9832839794497644 Weights:[ 0.06827822  1.05484493 -0.06821775]\n",
      "Predication: -0.06821774801198803 Weights:[ 0.06827822  1.05484493 -0.06139597]\n",
      "Predication: 1.0617271739912904 Weights:[ 0.0621055   1.04867221 -0.06756869]\n",
      "Predication: 0.9811035235627523 Weights:[ 0.0621055   1.05056186 -0.06567904]\n",
      "Predication: -0.0035735447350425317 Weights:[ 0.06246285  1.05056186 -0.06532169]\n",
      "Error_of_all: 0.009117233405426495\n",
      "\n",
      "Predication: -0.002858835788034024 Weights:[ 0.06274874  1.05056186 -0.0650358 ]\n",
      "Predication: 0.9855260569025094 Weights:[ 0.06274874  1.05200926 -0.06358841]\n",
      "Predication: -0.06358841060413677 Weights:[ 0.06274874  1.05200926 -0.05722957]\n",
      "Predication: 1.05752842286588 Weights:[ 0.05699589  1.04625641 -0.06298241]\n",
      "Predication: 0.9832740020092452 Weights:[ 0.05699589  1.04792901 -0.06130981]\n",
      "Predication: -0.004313918034364962 Weights:[ 0.05742729  1.04792901 -0.06087842]\n",
      "Error_of_all: 0.00786904226904208\n",
      "\n",
      "Predication: -0.003451134427491974 Weights:[ 0.0577724   1.04792901 -0.06053331]\n",
      "Predication: 0.9873957068535818 Weights:[ 0.0577724   1.04918944 -0.05927288]\n",
      "Predication: -0.059272877470408075 Weights:[ 0.0577724   1.04918944 -0.05334559]\n",
      "Predication: 1.0536162524729626 Weights:[ 0.05241077  1.04382782 -0.05870721]\n",
      "Predication: 0.9851206027353137 Weights:[ 0.05241077  1.04531576 -0.05721928]\n",
      "Predication: -0.004808501248434842 Weights:[ 0.05289162  1.04531576 -0.05673843]\n",
      "Error_of_all: 0.006803273214640502\n",
      "\n",
      "Predication: -0.0038468009987478735 Weights:[ 0.0532763   1.04531576 -0.05635375]\n",
      "Predication: 0.9889620124129692 Weights:[ 0.0532763   1.04641956 -0.05524995]\n",
      "Predication: -0.05524994626077355 Weights:[ 0.0532763   1.04641956 -0.04972495]\n",
      "Predication: 1.049970908776931 Weights:[ 0.04827921  1.04142247 -0.05472204]\n",
      "Predication: 0.9867004228010665 Weights:[ 0.04827921  1.04275242 -0.05339208]\n",
      "Predication: -0.005112871449710697 Weights:[ 0.0487905   1.04275242 -0.0528808 ]\n",
      "Error_of_all: 0.005889303541837786\n",
      "\n",
      "Predication: -0.004090297159768559 Weights:[ 0.04919953  1.04275242 -0.05247177]\n",
      "Predication: 0.9902806551018011 Weights:[ 0.04919953  1.04372436 -0.05149983]\n",
      "Predication: -0.051499833441728114 Weights:[ 0.04919953  1.04372436 -0.04634985]\n",
      "Predication: 1.0465740376293469 Weights:[ 0.04454213  1.03906695 -0.05100725]\n",
      "Predication: 0.9880596998997442 Weights:[ 0.04454213  1.04026098 -0.04981322]\n",
      "Predication: -0.0052710974096659285 Weights:[ 0.04506924  1.04026098 -0.04928611]\n",
      "Error_of_all: 0.0051029252561172675\n",
      "\n",
      "Predication: -0.004216877927732746 Weights:[ 0.04549092  1.04026098 -0.04886443]\n",
      "Predication: 0.9913965574535352 Weights:[ 0.04549092  1.04112133 -0.04800408]\n",
      "Predication: -0.048004082062078055 Weights:[ 0.04549092  1.04112133 -0.04320367]\n",
      "Predication: 1.043408578143574 Weights:[ 0.04115007  1.03678047 -0.04754453]\n",
      "Predication: 0.9892359385403211 Weights:[ 0.04115007  1.03785688 -0.04646813]\n",
      "Predication: -0.005318059364078823 Weights:[ 0.04168187  1.03785688 -0.04593632]\n",
      "Error_of_all: 0.004424644608684828\n",
      "\n",
      "Predication: -0.0042544474912630525 Weights:[ 0.04210732  1.03785688 -0.04551087]\n",
      "Predication: 0.992346001517791 Weights:[ 0.04210732  1.03862228 -0.04474547]\n",
      "Predication: -0.044745474990504665 Weights:[ 0.04210732  1.03862228 -0.04027093]\n",
      "Predication: 1.0404586655589985 Weights:[ 0.03806145  1.03457641 -0.04431679]\n",
      "Predication: 0.9902596156014837 Weights:[ 0.03806145  1.03555045 -0.04334276]\n",
      "Predication: -0.005281305317687134 Weights:[ 0.03858958  1.03555045 -0.04281463]\n",
      "Error_of_all: 0.0038385124412518303\n",
      "\n",
      "Predication: -0.0042250442541497055 Weights:[ 0.03901209  1.03555045 -0.04239212]\n",
      "Predication: 0.9931583274383705 Weights:[ 0.03901209  1.03623462 -0.04170795]\n",
      "Predication: -0.041707953394155776 Weights:[ 0.03901209  1.03623462 -0.03753716]\n",
      "Predication: 1.0377095425371112 Weights:[ 0.03524113  1.03246366 -0.04130811]\n",
      "Predication: 0.9911555487826897 Weights:[ 0.03524113  1.03334811 -0.04042367]\n",
      "Predication: -0.005182536193432452 Weights:[ 0.03575938  1.03334811 -0.03990541]\n",
      "Error_of_all: 0.0033313054558089675\n",
      "\n",
      "Predication: -0.004146028954745959 Weights:[ 0.03617399  1.03334811 -0.03949081]\n",
      "Predication: 0.9938572955409696 Weights:[ 0.03617399  1.03396238 -0.03887654]\n",
      "Predication: -0.03887654022599941 Weights:[ 0.03617399  1.03396238 -0.03498889]\n",
      "Predication: 1.0351474779634813 Weights:[ 0.03265924  1.03044763 -0.03850363]\n",
      "Predication: 0.9919439948626794 Weights:[ 0.03265924  1.03125323 -0.03769803]\n",
      "Predication: -0.00503879377425797 Weights:[ 0.03316312  1.03125323 -0.03719415]\n",
      "Error_of_all: 0.0028919416227737734\n",
      "\n",
      "Predication: -0.004031035019406375 Weights:[ 0.03356622  1.03125323 -0.03679105]\n",
      "Predication: 0.9944621787695098 Weights:[ 0.03356622  1.03180701 -0.03623727]\n",
      "Predication: -0.03623726848360008 Weights:[ 0.03356622  1.03180701 -0.03261354]\n",
      "Predication: 1.032759692455092 Weights:[ 0.03029025  1.02853104 -0.03588951]\n",
      "Predication: 0.9926415313729495 Weights:[ 0.03029025  1.02926689 -0.03515366]\n",
      "Predication: -0.004863410672429416 Weights:[ 0.03077659  1.02926689 -0.03466732]\n",
      "Error_of_all: 0.002511053608117256\n",
      "\n",
      "Predication: -0.003890728537943533 Weights:[ 0.03116567  1.02926689 -0.03427825]\n",
      "Predication: 0.9949886390193969 Weights:[ 0.03116567  1.02976803 -0.03377711]\n",
      "Predication: -0.03377711399894662 Weights:[ 0.03116567  1.02976803 -0.0303994 ]\n",
      "Predication: 1.0305342898820642 Weights:[ 0.02811224  1.0267146  -0.03345283]\n",
      "Predication: 0.9932617646389992 Weights:[ 0.02811224  1.02738842 -0.03277901]\n",
      "Predication: -0.004666769772712614 Weights:[ 0.02857892  1.02738842 -0.03231233]\n",
      "Error_of_all: 0.0021806703520253884\n",
      "\n",
      "Predication: -0.003733415818170091 Weights:[ 0.02895226  1.02738842 -0.03193899]\n",
      "Predication: 0.9954494302702878 Weights:[ 0.02895226  1.02784348 -0.03148393]\n",
      "Predication: -0.03148393251909879 Weights:[ 0.02895226  1.02784348 -0.02833554]\n",
      "Predication: 1.0284601943056741 Weights:[ 0.02610624  1.02499746 -0.03118156]\n",
      "Predication: 0.9938158986070053 Weights:[ 0.02610624  1.02561587 -0.03056315]\n",
      "Predication: -0.004456911151490314 Weights:[ 0.02655193  1.02561587 -0.03011746]\n",
      "Error_of_all: 0.0018939739123713475\n",
      "\n",
      "Predication: -0.003565528921192253 Weights:[ 0.02690848  1.02561587 -0.0297609 ]\n",
      "Predication: 0.9958549628928723 Weights:[ 0.02690848  1.02603037 -0.0293464 ]\n",
      "Predication: -0.029346400840475826 Weights:[ 0.02690848  1.02603037 -0.02641176]\n",
      "Predication: 1.0265270918125804 Weights:[ 0.02425577  1.02337766 -0.02906447]\n",
      "Predication: 0.9943131920358295 Weights:[ 0.02425577  1.02394634 -0.02849579]\n",
      "Predication: -0.004240016908292479 Weights:[ 0.02467977  1.02394634 -0.02807179]\n",
      "Error_of_all: 0.0016451096996342332\n",
      "\n",
      "Predication: -0.0033920135266339822 Weights:[ 0.02501898  1.02394634 -0.02773259]\n",
      "Predication: 0.9962137566721563 Weights:[ 0.02501898  1.02432497 -0.02735396]\n",
      "Predication: -0.02735396176499221 Weights:[ 0.02501898  1.02432497 -0.02461857]\n",
      "Predication: 1.0247253767906936 Weights:[ 0.02254644  1.02185243 -0.0270911 ]\n",
      "Predication: 0.9947613261560856 Weights:[ 0.02254644  1.0223763  -0.02656724]\n",
      "Predication: -0.004020798285770878 Weights:[ 0.02294852  1.0223763  -0.02616516]\n",
      "Error_of_all: 0.0014290353984827077\n",
      "\n",
      "Predication: -0.003216638628616701 Weights:[ 0.02327018  1.0223763  -0.02584349]\n",
      "Predication: 0.9965328046163073 Weights:[ 0.02327018  1.02272302 -0.02549677]\n",
      "Predication: -0.025496772653362886 Weights:[ 0.02327018  1.02272302 -0.0229471 ]\n",
      "Predication: 1.0230461022472208 Weights:[ 0.02096557  1.02041841 -0.02525171]\n",
      "Predication: 0.9951667005089379 Weights:[ 0.02096557  1.02090174 -0.02476838]\n",
      "Predication: -0.0038028045995257484 Weights:[ 0.02134585  1.02090174 -0.0243881 ]\n",
      "Error_of_all: 0.0012413985592149145\n",
      "\n",
      "Predication: -0.003042243679620596 Weights:[ 0.02165008  1.02090174 -0.02408387]\n",
      "Predication: 0.996817865235065 Weights:[ 0.02165008  1.02121995 -0.02376566]\n",
      "Predication: -0.023765657359234325 Weights:[ 0.02165008  1.02121995 -0.02138909]\n",
      "Predication: 1.0214809338160067 Weights:[ 0.01950198  1.01907186 -0.02353719]\n",
      "Predication: 0.995534671160774 Weights:[ 0.01950198  1.01951839 -0.02309065]\n",
      "Predication: -0.0035886696105582767 Weights:[ 0.01986085  1.01951839 -0.02273179]\n",
      "Error_of_all: 0.0010784359268087556\n",
      "\n",
      "Predication: -0.0028709356884466207 Weights:[ 0.02014794  1.01951839 -0.02244469]\n",
      "Predication: 0.9970736974585198 Weights:[ 0.02014794  1.01981102 -0.02215206]\n",
      "Predication: -0.022152061336940452 Weights:[ 0.02014794  1.01981102 -0.01993686]\n",
      "Predication: 1.0200221071408409 Weights:[ 0.01814573  1.01780881 -0.02193907]\n",
      "Predication: 0.9958697426723416 Weights:[ 0.01814573  1.01822183 -0.02152604]\n",
      "Predication: -0.0033803078583175654 Weights:[ 0.01848376  1.01822183 -0.02118801]\n",
      "Error_of_all: 0.0009368896209360312\n",
      "\n",
      "Predication: -0.0027042462866540516 Weights:[ 0.01875419  1.01822183 -0.02091758]\n",
      "Predication: 0.9973042495523706 Weights:[ 0.01875419  1.01849141 -0.02064801]\n",
      "Predication: -0.02064800972530455 Weights:[ 0.01875419  1.01849141 -0.01858321]\n",
      "Predication: 1.018662388355171 Weights:[ 0.01688795  1.01662517 -0.02044945]\n",
      "Predication: 0.9961757229433927 Weights:[ 0.01688795  1.0170076  -0.02006702]\n",
      "Predication: -0.0031790709774033414 Weights:[ 0.01720586  1.0170076  -0.01974911]\n",
      "Error_of_all: 0.0008139366504753339\n",
      "\n",
      "Predication: -0.002543256781922673 Weights:[ 0.01746018  1.0170076  -0.01949479]\n",
      "Predication: 0.9975128111306469 Weights:[ 0.01746018  1.01725632 -0.01924607]\n",
      "Predication: -0.019246068219762574 Weights:[ 0.01746018  1.01725632 -0.01732146]\n",
      "Predication: 1.0173950374076535 Weights:[ 0.01572068  1.01551681 -0.01906097]\n",
      "Predication: 0.9964558482449631 Weights:[ 0.01572068  1.01587123 -0.01870655]\n",
      "Predication: -0.0029858720226535913 Weights:[ 0.01601927  1.01587123 -0.01840796]\n",
      "Error_of_all: 0.0007071291752624441\n",
      "\n",
      "Predication: -0.002388697618122871 Weights:[ 0.01625813  1.01587123 -0.01816909]\n",
      "Predication: 0.9977021355600483 Weights:[ 0.01625813  1.01610102 -0.01793931]\n",
      "Predication: -0.01793930655497516 Weights:[ 0.01625813  1.01610102 -0.01614538]\n",
      "Predication: 1.0162137740080082 Weights:[ 0.01463676  1.01447964 -0.01776675]\n",
      "Predication: 0.9967128843019345 Weights:[ 0.01463676  1.01480835 -0.01743804]\n",
      "Predication: -0.0028012842268006904 Weights:[ 0.01491689  1.01480835 -0.01715791]\n",
      "Error_of_all: 0.0006143435674831474\n",
      "\n",
      "Predication: -0.0022410273814405524 Weights:[ 0.01514099  1.01480835 -0.01693381]\n",
      "Predication: 0.9978745386023716 Weights:[ 0.01514099  1.0150209  -0.01672126]\n",
      "Predication: -0.016721264429884947 Weights:[ 0.01514099  1.0150209  -0.01504914]\n",
      "Predication: 1.0151127459893812 Weights:[ 0.01362971  1.01350962 -0.01656041]\n",
      "Predication: 0.9969492081270097 Weights:[ 0.01362971  1.0138147  -0.01625533]\n",
      "Predication: -0.0026256193329783125 Weights:[ 0.01389228  1.0138147  -0.01599277]\n",
      "Error_of_all: 0.00053373677328488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Learning the whole dataset!\n",
    "import numpy as np\n",
    "\n",
    "weights = np.array([0.5,0.48,-0.7])\n",
    "alpha = 0.1\n",
    "\n",
    "streetlights = np.array( [[ 1, 0, 1 ],\n",
    "                          [ 0, 1, 1 ],\n",
    "                          [ 0, 0, 1 ],\n",
    "                          [ 1, 1, 1 ],\n",
    "                          [ 0, 1, 1 ],\n",
    "                          [ 1, 0, 1 ] ] )\n",
    "\n",
    "walk_vs_stop = np.array( [ 0, 1, 0, 1, 1, 0 ] )\n",
    "\n",
    "input = streetlights[0] # [1,0,1]\n",
    "goal_prediction = walk_vs_stop[0] # equals 0... i.e. \"stop\"\n",
    "\n",
    "for i in range(40):\n",
    "    error_for_all_lights = 0\n",
    "    for row_index in range(len(walk_vs_stop)):\n",
    "        input = streetlights[row_index]\n",
    "        goal_prediction = walk_vs_stop[row_index]\n",
    "        \n",
    "        pred = input.dot(weights)\n",
    "        error = (pred - goal_prediction)**2\n",
    "        error_for_all_lights += error\n",
    "        delta = pred - goal_prediction\n",
    "        weight_delta = input * delta\n",
    "        weights = weights - (alpha * weight_delta)\n",
    "        print(\"Predication: \" + str(pred) + \" Weights:\" + str(weights))\n",
    "    print(\"Error_of_all: \" + str(error_for_all_lights) +\"\\n\")\n",
    "\"\"\"\n",
    "Q:In the following code I know we should expect the total error goes down, but is there a changing pattern of weights values that we should expect? \n",
    "\n",
    "In a gradient descent algorithm like the one in your code, you should expect that the weights will continuously adjust in a direction that minimizes the total error. \n",
    "The specific pattern of the weight changes will depend on the learning rate (alpha), the features (input), and the targets (goal_prediction), among other factors.\n",
    "\n",
    "Here are some things to note:\n",
    "\n",
    "Learning Rate (alpha): A smaller learning rate would mean slower convergence, so the changes in the weights would be more gradual. \n",
    "A larger learning rate might make the weights change more dramatically, but be careful not to set it too high, or the model might overshoot the minimum error.\n",
    "\n",
    "Features (input) and Targets (goal_prediction): The nature of the data will heavily influence the changes in weights. Features with larger ranges or variances could cause more significant weight updates.\n",
    "\n",
    "Gradient (Delta): The gradient indicates the direction and magnitude of the change in the weights. If the model is far from an optimal solution, you might see larger deltas initially, which gradually get smaller as the model approaches the minimum error.\n",
    "\n",
    "Global vs Local Minima: In complex models (though not in this simple example), you also have to consider that the weights may get stuck in local minima. However, this is less of an issue with convex optimization problems like linear regression.\n",
    "\n",
    "Iteration: In your code, you run 40 iterations of updating each weight with every row in your dataset (this is essentially a form of batch gradient descent). \n",
    "Over time, you should notice the \"Error_for_all_lights\" value decreasing, indicating that your model is improving its predictions.\n",
    "\n",
    "So, while you might not be able to predict the exact sequence of weight changes, you can generally expect them to move in a direction that reduces error. \n",
    "By monitoring the error and the weight updates, you'll have a good sense of whether your model is learning as intended.\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14e62bbd-95d9-4cf8-8ae4-478a94f3d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our First \"Deep\" Neural Network\n",
    "\"\"\"\n",
    "反向传播（Backpropagation）是一种优化算法，用于最小化神经网络的误差函数。它是监督式学习中最常用的算法之一。反向传播的主要思想是通过计算梯度的反向传播来优化权重，从而减少误差。\n",
    "\n",
    "在一个典型的前馈神经网络中，反向传播涉及以下几个主要步骤：\n",
    "\n",
    "前向传播：输入通过每一层进行运算，最终产生输出。\n",
    "计算损失（Loss）：使用损失函数比较模型的预测和实际标签，生成一个损失值。\n",
    "反向传播：计算损失函数相对于每个权重的梯度。这是通过从输出层开始，逐层向后计算梯度来完成的。\n",
    "权重更新：根据计算出的梯度来更新权重，通常使用梯度下降或其变种。\n",
    "为什么反向传播有效？\n",
    "计算效率：反向传播算法可以有效地计算梯度，这通常是优化神经网络最复杂的部分。\n",
    "通用性：它可以用于任何可微分的损失函数和激活函数，这使得它非常灵活。\n",
    "局部更新：只有与误差直接相关的权重会被更新，这使得网络的训练更加高效。\n",
    "自动化微分：反向传播算法自动地计算需要更新的权重，减少了手动微分的需要。\n",
    "总体来说，反向传播是一种强大而灵活的工具，它是现代深度学习的基础之一。\n",
    "\n",
    "Q:why by convection layer_1_delta is equal to multipled result of these two parts?\n",
    "A:layer_1_delta 的计算方式来自于链式法则（Chain Rule），这是反向传播算法（Backpropagation）的基础。下面我们分解一下为什么需要这两个部分：\n",
    "\n",
    "反向传播的误差项（layer_2_delta.dot(weights_1_2.T)）： 这一项将输出层的“误差”传回到隐藏层。它是输出层 delta 的加权和，其中权重是隐藏层和输出层之间的连接权重。这实际上是在说：“隐藏层中的每个神经元有多么不准确，考虑到它对第2层误差的影响。”\n",
    "\n",
    "激活函数的导数（relu2deriv(layer_1)）： 这一项在反向传播中引入了非线性。这个导数衡量第1层输出对其输入变化的敏感性。换句话说，如果这个导数很低，意味着在前向传播中，该神经元不是很活跃，因此其误差项应该被“减弱”。相反，如果导数很高，说明神经元是活跃的，误差项应该有更多的影响。\n",
    "\n",
    "这两个部分组合给出了隐藏层的最终 delta，也就是 layer_1_delta。在数学上，对于第 1 层中的每个神经元 j，它看起来像这样：\n",
    "\n",
    "layer_1_delta[j]=(∑klayer_2_delta[k]×weights_1_2[j][k])×relu2deriv(layer_1[j])\n",
    "\n",
    "其中∑k是对所有第2层中的神经元k的求和。\n",
    "\n",
    "通过逐元素乘法（⊙）结合这两个方面：传播的误差和激活函数的敏感性。这种组合对于网络能够有效地从数据中学习是至关重要的，它同时考虑了网络有多么不正确（误差传播）以及它对自己的不正确性有多么自信（激活的导数）。\n",
    "\n",
    "隐藏层的 delta 实际上是基于链式法则（Chain Rule）计算的。链式法则是微积分中一个非常重要的规则，用于求复合函数的导数。在机器学习和深度学习的上下文中，链式法则用于反向传播（Backpropagation）算法中，以计算损失函数（或目标函数）关于模型参数（权重和偏置）的梯度。\n",
    "\n",
    "简单来说，如果你有一个复合函数，比如f(g(h(x)))，链式法则可以用于计算这个复合函数关于x的导数，即df/dx​。对于一般形式f(g(x))，链式法则给出：df/dx= df/dx ⋅ dg/dx\n",
    "​ \n",
    "在多层神经网络中，损失函数 L 是关于多个参数（比如权重 w 和偏置 b）的复合函数。反向传播算法利用链式法则从输出层向输入层逐层计算这些参数的梯度，以便进行梯度下降优化。\n",
    "\n",
    "具体到隐藏层的 delta，它就是用于反向传播中梯度的局部累积。每一层的 delta 被用于计算前一层的 delta，这就是链式法则在反向传播中的应用。这样做是为了有效地计算损失函数关于每一层权重的偏导数，从而进行参数更新。\n",
    "\n",
    "Q:why only calculation of the hidden layer weights need the activation funciton?\n",
    "A:In neural networks, activation functions like ReLU are typically applied to the output of each neuron in the hidden layers. \n",
    "These activation functions introduce non-linearity into the system, which allows the neural network to learn from the error and make adjustments, enabling it to model complex relationships in the data.\n",
    "\n",
    "For the output layer, it is not always necessary to apply an activation function, depending on what kind of problem you are solving. \n",
    "For regression problems, you might not use an activation function at all in the output layer. For binary classification, you might use the sigmoid function, and for multi-class classification, you might use the softmax function.\n",
    "\n",
    "In your backpropagation code, the calculation of the hidden layer deltas (layer_1_delta) incorporates the derivative of the activation function (relu2deriv(layer_1)), which is crucial for the weight updates. \n",
    "This is because the gradients need to be scaled by how much the neurons in the hidden layer were \"responsible\" for the final output, which the activation function's derivative helps determine.\n",
    "\n",
    "The output layer doesn't usually have this scaling by the derivative of the activation function because its output is directly compared to the target variable to compute the loss. \n",
    "In many cases, especially for problems like regression, the output layer is linear (i.e., no activation function), so the derivative is essentially 1, and we don't explicitly have to multiply by it when calculating output layer deltas.\n",
    "\n",
    "Q:from the angle of derivative, how does the two parts of multiplation comes from? it should be some parts of derivative of some function right? layer_1_delta = layer_2_delta.dot(weights_1_2.T)*relu2deriv(layer_1)\n",
    "A:在反向传播中，layer_1_delta 本质上是损失函数相对于隐藏层神经元输出（激活函数之前）的梯度。这是通过使用微积分的链式法则来推导出来的。\n",
    "\n",
    "表达式 layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1) 包括两个主要部分：\n",
    "\n",
    "layer_2_delta.dot(weights_1_2.T)：这个术语将第二层的 '错误' 反向传播到第一层。从数学上讲，它是根据第一层到第二层的权重（weights_1_2）计算第二层梯度（layer_2_delta）的加权和。这给你一个关于第一层中的每个神经元对第二层误差有多大贡献的感觉，因此你应该如何调整第一层中的每个神经元。\n",
    "\n",
    "relu2deriv(layer_1)：这个术语是第一层应用的 ReLU 激活函数的导数。它与上面的项按元素乘法进行缩放，以反映每个第一层神经元有多少“激活”。用更简单的术语来说，如果第一层中的一个神经元输出接近 0（因此没有太多的激活），ReLU 的导数将为 0，从而有效地取消了基于第二层误差对该神经元进行任何调整。\n",
    "\n",
    "这两个术语的乘法给你提供了用于更新第 0 层和第 1 层之间权重的梯度，有效地实现了链式法则，以将误差反向传播通过网络。\n",
    "\n",
    "Q:So basically the calculation method of layer_1_delta is not directly using some math formulas, it's actually using ML concepts to do the calculation, but inside the ML concepts it use Calculus chain rule formula.Am I right?\n",
    "A:Yes, you are correct. The formula for calculating layer_1_delta does not directly stem from a mathematical theorem but is based on the practical needs and design of machine learning algorithms, particularly backpropagation. \n",
    "Inside this machine learning context, the calculus chain rule is employed to efficiently compute gradients for weight updates across layers. \n",
    "The chain rule allows us to propagate the error information backward through the neural network, effectively adjusting each layer's parameters based on its contribution to the final error. \n",
    "So, it's a blend of machine learning concepts that are underpinned by mathematical principles, particularly calculus.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "streetlights = np.array( [[ 1, 0, 1 ],\n",
    "                          [ 0, 1, 1 ],\n",
    "                          [ 0, 0, 1 ],\n",
    "                          [ 1, 1, 1 ]] )\n",
    "\n",
    "walk_vs_stop = np.array([[1, 1, 0, 0]]).T\n",
    "\n",
    "input = streetlights[0] # [1,0,1]\n",
    "goal_prediction = walk_vs_stop[0] # equals 0... i.e. \"stop\"\n",
    "\n",
    "alpha = 0.2\n",
    "hidden_size = 4\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def relu(x):\n",
    "    return (x>0)*x #returns x if x > 0\n",
    "                   #return 0 otherwise\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output>0 #return 1 for input>0\n",
    "                    #return 0 otherwise\n",
    "\n",
    "\n",
    "weights_0_1 = 2*np.random.random((3,hidden_size)) - 1\n",
    "weights_1_2 = 2*np.random.random((hidden_size,1)) - 1\n",
    "\n",
    "for iteration in range(60):\n",
    "    layer_2_error = 0\n",
    "    for i in range(len(streetlights)):\n",
    "        layer_0 = streetlights[i:i+1]\n",
    "        layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "        layer_2 = relu(np.dot(layer_1,weights_1_2))\n",
    "\n",
    "        layer_2_error += np.sum((layer_2 - walk_vs_stop[i:i+1])**2)\n",
    "\n",
    "        layer_2_delta = (layer_2 - walk_vs_stop[i:i+1])\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T)*relu2deriv(layer_1)\n",
    "\n",
    "        weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta)\n",
    "    if(iteration % 10 == 9):\n",
    "       print(\"Error: \" + str(layer_2_error))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf841a5-19f4-4199-a209-96b532f563ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
