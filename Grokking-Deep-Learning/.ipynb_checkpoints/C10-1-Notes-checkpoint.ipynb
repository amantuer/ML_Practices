{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3552aa6-e72d-411b-a18a-1eeff216e444",
   "metadata": {},
   "source": [
    "**<h2>Chapter10 - Intro to Convolutional Neural Networks - Learning Edges and Corners.ipynb</h2>**\n",
    "\n",
    "![Deep Learning Algorithm Structure](https://showme.redstarplugin.com/d/d:MvQXLjhj)\n",
    "\n",
    "Certainly! This code is implementing a basic CNN (Convolutional Neural Network) in NumPy. CNNs are especially useful for image-related tasks. The following parts are new compared to the previous version of the code and are relevant to the implementation of a CNN:\n",
    "\n",
    "### Kernel Initialization\n",
    "\n",
    "```python\n",
    "kernel_rows = 3\n",
    "kernel_cols = 3\n",
    "num_kernels = 16\n",
    "```\n",
    "\n",
    "Here, you initialize the kernels (also known as filters). Each kernel is a 3x3 matrix, and there are 16 such kernels. These kernels are what will \"slide over\" the input image to produce feature maps.\n",
    "\n",
    "\n",
    "```python\n",
    "hidden_size = ((input_rows - kernel_rows) *  (input_cols - kernel_cols)) * num_kernels\n",
    "```\n",
    "\n",
    "\n",
    "hidden_size变量用于定义卷积操作后的后续层的维度。让我们分解一下为什么要这样计算：\n",
    "\n",
    "卷积图像的空间维度：当您用3x3内核卷积28x28图像时，输出特征图的尺寸将为（28-3 +1）x（28-3 +1）= 26x26。这是一种计算卷积层输出尺寸的基本公式，不带填充和步幅：O =（W-K +1）x（H-K +1），其中W和H是输入层的宽度和高度，K是内核的大小。在您的情况下，宽度和高度都减小到26。\n",
    "\n",
    "多个内核：该代码使用16个不同的内核（num_kernels设置为16）。当与输入图像卷积时，每个内核都会产生自己的26x26特征图。\n",
    "\n",
    "展平输出：为了将这个卷积层连接到后续的全连接层，我们通常会展平输出。每个26x26输出将被展平为大小为676（26 x 26 = 676）的1D数组。\n",
    "\n",
    "连接多个展平输出：由于有16个这样的展平输出（每个内核一个），因此总大小将为16 x 676 = 10816。\n",
    "\n",
    "因此，hidden_size的计算方式为（（input_rows-kernel_rows）（input_cols-kernel_cols）） num_kernels =（26 * 26）* 16 = 10816。这将是通过完全连接层之前卷积并通过的第一层中每个特征向量的大小。\n",
    "\n",
    "The `hidden_size` variable is used to define the dimensions of the subsequent layers after the convolutional operation. Let's break down why it is calculated the way it is:\n",
    "\n",
    "1. **Spatial Dimensions of Convolved Image**: When you convolve a 28x28 image with a 3x3 kernel, the dimensions of the output feature map would be `(28 - 3 + 1) x (28 - 3 + 1)` = 26x26. This is a basic formula for calculating the dimensions of the output of a convolutional layer without padding and stride: `O = (W - K + 1) x (H - K + 1)`, where `W` and `H` are the width and height of the input layer, and `K` is the size of the kernel. In your case, both the width and the height are reduced to 26.\n",
    "\n",
    "2. **Multiple Kernels**: The code uses 16 different kernels (`num_kernels` is set to 16). Each of these kernels will produce its own 26x26 feature map when convolved with the input image.\n",
    "\n",
    "3. **Flattening the Output**: To connect this convolved layer to subsequent fully-connected layers, we usually flatten the output. Each 26x26 output would be flattened to a 1D array of size 676 (`26 x 26 = 676`).\n",
    "\n",
    "4. **Concatenating Multiple Flattened Outputs**: Since there are 16 such flattened outputs (one for each kernel), the overall size would be `16 x 676 = 10816`.\n",
    "\n",
    "\n",
    "Therefore, the `hidden_size` is calculated as `((input_rows - kernel_rows) * (input_cols - kernel_cols)) * num_kernels = (26 * 26) * 16 = 10816`. This will be the size of each feature vector in `layer_1` after the convolution and before being passed through the fully-connected layer.\n",
    "\n",
    "```python\n",
    "kernels = 0.02*np.random.random((kernel_rows*kernel_cols, num_kernels))-0.01\n",
    "```\n",
    "\n",
    "**<em>Q:why there is only 'weights_1_2' be used not 'weights_0_1' be used in this code?**</em>\n",
    "\n",
    "In a typical neural network with fully connected layers, weights between each layer, often denoted as `weights_0_1`, `weights_1_2`, etc., are used to map from one layer to the next. However, in the provided code, convolutional operations have replaced the fully connected layer that would have otherwise existed between `layer_0` (input layer) and `layer_1` (hidden layer). This is a key feature of Convolutional Neural Networks (CNNs).\n",
    "\n",
    "Here's how it works in the given code:\n",
    "\n",
    "1. The input images (stored in `layer_0`) are not mapped to `layer_1` using a weight matrix `weights_0_1`. Instead, they are convolved with a set of kernels. These kernels effectively replace `weights_0_1`. \n",
    "\n",
    "2. After convolution and activation, the output (stored in `layer_1`) is then mapped to `layer_2` (output layer) using a weight matrix, which is `weights_1_2`.\n",
    "\n",
    "So, in summary, `weights_0_1` is effectively replaced by the set of kernels used for the convolutional operation, and that's why only `weights_1_2` appears in the code.\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### Image Section Extraction\n",
    "\n",
    "```python\n",
    "def get_image_section(layer,row_from, row_to, col_from, col_to):\n",
    "    section = layer[:,row_from:row_to,col_from:col_to]\n",
    "    return section.reshape(-1,1,row_to-row_from, col_to-col_from)\n",
    "```\n",
    "\n",
    "This function extracts a section of the image to apply the kernel. It reshapes the extracted section so that it can be mu\n",
    "\n",
    "Certainly! The code snippet is from the function `get_image_section`, which extracts a specific region from the 2D input array `layer`. This region is defined by the rows `[row_from:row_to]` and columns `[col_from:col_to]`. Let's break down each line:\n",
    "\n",
    "### Line 1: Extracting the Region\n",
    "\n",
    "```python\n",
    "section = layer[:,row_from:row_to,col_from:col_to]\n",
    "```\n",
    "\n",
    "- `layer`: This is a 3D array where the first dimension usually represents the batch size, the second represents rows, and the third represents columns of the image.\n",
    "  \n",
    "- `[:, row_from:row_to, col_from:col_to]`: This is NumPy slicing syntax, which is extracting a section of the array `layer`.\n",
    "  - `:` means to take all elements along the first dimension (usually the batch dimension in this case).\n",
    "  - `row_from:row_to` means to take all rows from `row_from` to `row_to-1`.\n",
    "  - `col_from:col_to` means to take all columns from `col_from` to `col_to-1`.\n",
    "\n",
    "Certainly! The code snippet is from the function `get_image_section`, which extracts a specific region from the 2D input array `layer`. This region is defined by the rows `[row_from:row_to]` and columns `[col_from:col_to]`. Let's break down each line:\n",
    "\n",
    "### Line 1: Extracting the Region\n",
    "\n",
    "```python\n",
    "section = layer[:,row_from:row_to,col_from:col_to]\n",
    "```\n",
    "\n",
    "- `layer`: This is a 3D array where the first dimension usually represents the batch size, the second represents rows, and the third represents columns of the image.\n",
    "  \n",
    "- `[:, row_from:row_to, col_from:col_to]`: This is NumPy slicing syntax, which is extracting a section of the array `layer`.\n",
    "  - `:` means to take all elements along the first dimension (usually the batch dimension in this case).\n",
    "  - `row_from:row_to` means to take all rows from `row_from` to `row_to-1`.\n",
    "  - `col_from:col_to` means to take all columns from `col_from` to `col_to-1`.\n",
    "\n",
    "\n",
    "### Line 2: Reshaping the Region\n",
    "\n",
    "```python\n",
    "return section.reshape(-1,1,row_to-row_from, col_to-col_from)\n",
    "```\n",
    "\n",
    "- `reshape(-1, 1, row_to-row_from, col_to-col_from)`: This reshapes the `section` array. Here's what each argument does:\n",
    "  - `-1`: The size of this dimension is automatically calculated. This usually would be `batch_size * num_of_sections`, where `num_of_sections` is the number of distinct sections you're taking from each image in the batch.\n",
    "  - `1`: Adds an extra dimension. This is useful for keeping the shape consistent with potential future operations, like concatenation.\n",
    "  - `row_to-row_from`: The height of the section. It's the difference between `row_to` and `row_from`.\n",
    "  - `col_to-col_from`: The width of the section. It's the difference between `col_to` and `col_from`.\n",
    "\n",
    "This reshaped section is then returned by the function. The reshaping is necessary for future operations, such as dot products with the kernels. The shape after reshaping essentially stacks all the individual sections and makes it ready for subsequent matrix operations.\n",
    "ltiplied by the kernel.\n",
    "\n",
    "### The `-1` in `reshape`\n",
    "\n",
    "When using `-1` as a dimension in `numpy.reshape`, it's essentially a \"wildcard\" that tells NumPy to automatically calculate the size of that dimension. The size is determined such that the total number of elements in the array remains unchanged.\n",
    "\n",
    "For example, suppose you have an array of shape `(2, 3, 4)`; this array has `2 * 3 * 4 = 24` total elements. If you reshape it with dimensions `(2, -1, 2)`, NumPy will automatically calculate that the `-1` should be `6`, because `2 * 6 * 2 = 24`. So, the shape becomes `(2, 6, 2)`.\n",
    "\n",
    "In your code, the `-1` allows the function to be more flexible with respect to the batch size and the number of sections you're taking from each image. It ensures that the reshaping works correctly, regardless of these sizes, as long as the total number of elements remains constant.\n",
    "\n",
    "### The `1` in `reshape`\n",
    "\n",
    "Adding an extra dimension with size `1` can be useful for several reasons:\n",
    "\n",
    "1. **Broadcasting:** NumPy allows broadcasting of dimensions with size `1` when performing array operations. This can be useful if you want to perform element-wise operations between arrays that mostly have matching dimensions, except for one that is missing or has a size of `1`.\n",
    "\n",
    "2. **Shape Compatibility:** Some machine learning libraries and functions require input arrays to have a specific number of dimensions. Adding a dimension with size `1` allows the array to meet this requirement without changing the overall semantics of the data.\n",
    "\n",
    "3. **Concatenation and Stacking:** If you plan to concatenate or stack arrays along a new dimension, initializing that dimension with a size of `1` is useful.\n",
    "\n",
    "4. **Clearer Semantics:** Sometimes an extra dimension is added to make it clear what each dimension represents. For example, a 3D array where the first dimension represents different samples, the second represents the height, and the third represents the width can be made 4D to indicate that there's only one \"channel\" (like grayscale in image data), even though this channel contains only a single slice.\n",
    "\n",
    "In your code, the extra dimension helps mainly with shape compatibility and potential future concatenation operations.\n",
    "\n",
    "Certainly! Adding an extra dimension with size `1` is often done in NumPy using the `reshape` method. Here's a simple example:\n",
    "\n",
    "### Original Array\n",
    "\n",
    "Let's say we have a 1D array with shape `(4,)` and it looks like this:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "arr = np.array([1, 2, 3, 4])\n",
    "print(\"Original array:\", arr)\n",
    "print(\"Shape:\", arr.shape)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Original array: [1 2 3 4]\n",
    "Shape: (4,)\n",
    "```\n",
    "\n",
    "### Adding an Extra Dimension\n",
    "\n",
    "Now, we'll add an extra dimension with size `1`:\n",
    "\n",
    "#### Row Vector\n",
    "\n",
    "For making it a row vector:\n",
    "\n",
    "```python\n",
    "reshaped_arr = arr.reshape((1, 4))\n",
    "print(\"Reshaped array:\", reshaped_arr)\n",
    "print(\"Shape:\", reshaped_arr.shape)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Reshaped array: [[1 2 3 4]]\n",
    "Shape: (1, 4)\n",
    "```\n",
    "\n",
    "#### Column Vector\n",
    "\n",
    "For making it a column vector:\n",
    "\n",
    "```python\n",
    "reshaped_arr = arr.reshape((4, 1))\n",
    "print(\"Reshaped array:\", reshaped_arr)\n",
    "print(\"Shape:\", reshaped_arr.shape)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Reshaped array: [[1]\n",
    "                 [2]\n",
    "                 [3]\n",
    "                 [4]]\n",
    "Shape: (4, 1)\n",
    "```\n",
    "\n",
    "In both of these reshaped arrays, we added an extra dimension with size `1`. The reshaped array still represents the same data but in a form that may be more suitable for certain types of operations.\n",
    "\n",
    "For example, this kind of reshaping is often useful when you're working with machine learning libraries that expect inputs to have a certain number of dimensions.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "```python\n",
    "        batch_start, batch_end=((i * batch_size),((i+1)*batch_size))\n",
    "```\n",
    "\n",
    "This sets up the indices for batching the dataset. batch_start and batch_end determine the beginning and end of each batch.\n",
    "\n",
    "```python\n",
    "        layer_0 = images[batch_start:batch_end]\n",
    "        layer_0 = layer_0.reshape(layer_0.shape[0],28,28)\n",
    "```\n",
    "\n",
    "layer_0 contains the batch of input images. They are reshaped to 28x28 pixels each.\n",
    "In the reshaped version `layer_0.reshape(layer_0.shape[0], 28, 28)`, `layer_0` becomes a 3D array or tensor, not a 1D matrix. Each of the 1000 elements along the first dimension is a 2D matrix of shape `28x28`, representing an image. Here's the breakdown:\n",
    "\n",
    "1. Initially, `layer_0` is a 2D array with shape `(1000, 784)`, representing 1000 images where each image is a 1D array of 784 pixels.\n",
    "2. After the reshape operation, `layer_0` becomes a 3D array with shape `(1000, 28, 28)`. Now it represents 1000 images, where each image is a 2D array (or matrix) of shape `28x28`.\n",
    "\n",
    "So, in the reshaped `layer_0`, each image has been reorganized from a 1D array of 784 pixels into a 2D array of `28x28` pixels, which can be easier to work with or visualize, especially if the images are meant to be 28 pixels in height and 28 pixels in width.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### Feature Map Creation\n",
    "\n",
    "```python\n",
    "sects = list()\n",
    "for row_start in range(layer_0.shape[1]-kernel_rows):\n",
    "    for col_start in range(layer_0.shape[2] - kernel_cols):\n",
    "        sect = get_image_section(layer_0,\n",
    "                                 row_start,\n",
    "                                 row_start+kernel_rows,\n",
    "                                 col_start,\n",
    "                                 col_start+kernel_cols)\n",
    "        sects.append(sect)\n",
    "```\n",
    "\n",
    "This loop iterates over the image, and at each iteration, it calls `get_image_section` to obtain a section of the image. These sections are what the kernel will be applied to.\n",
    "\n",
    "The line `sects = list()` initializes an empty list and assigns it to the variable `sects`. This is preparation for populating this list later in the code.\n",
    "This empty list will hold the sections of the image to which the kernel (convolution filter) will be applied.\n",
    "\n",
    "In the context of the given code, `sects` is meant to hold \"sections\" of the input images that are processed through the convolutional kernels. Specifically, the code loops through different positions of each image in the mini-batch to extract subsections of the image, defined by the `kernel_rows` and `kernel_cols`. These subsections are then appended to the `sects` list.\n",
    "\n",
    "Here's a simplified explanation:\n",
    "\n",
    "- `sects` starts as an empty list.\n",
    "- The code extracts subsections of the image and appends them to `sects`.\n",
    "- Eventually, `sects` contains all the different portions of the input image that the convolutional kernel will scan through.\n",
    "\n",
    "By initializing `sects` as an empty list, you ensure that it's ready to accept these subsections as they are computed.\n",
    "\n",
    "In one iteration of 'i', sects will hold all patches that extracted from 128(batch size) images.\n",
    "\n",
    "For a single 28x28 image and a 3x3 kernel, you can slide the kernel across the image in strides of 1 pixel both horizontally and vertically. \n",
    "\n",
    "The number of ways you can fit a 3x3 kernel into a 28x28 image can be calculated as follows:\n",
    "\n",
    "1. Horizontally: You can start the kernel at columns 0 to 25, making it 26 possible starting positions.\n",
    "2. Vertically: You can start the kernel at rows 0 to 25, making it 26 possible starting positions as well.\n",
    "\n",
    "So for each image, you'll have 26 (horizontal positions) x 26 (vertical positions) = 676 patches.\n",
    "\n",
    "Therefore, for a single 28x28 image with a 3x3 kernel, the list `sects` will hold 676 patches.\n",
    "\n",
    "**<em>Q:why this for loop \"for row_start in range(layer_0.shape[1]-kernel_rows):\" start with 'layer_0.shape[1]' not 'layer_0.shape[0]'?</em>**\n",
    "\n",
    "In the code, `layer_0` is reshaped as `[batch_size, 28, 28]`, where `batch_size` is the number of images in a mini-batch, and 28x28 is the dimension of each image.\n",
    "\n",
    "When you look at `layer_0.shape[1]`, it refers to the number of rows in each image, which is 28 in this case. Similarly, `layer_0.shape[2]` would refer to the number of columns in each image, which is also 28.\n",
    "\n",
    "The reason for using `layer_0.shape[1]` instead of `layer_0.shape[0]` is that the for-loop is iterating over the rows of each individual image, not over the batch size. It's moving the kernel over each 28x28 image, so it needs to start at row 0 and go up to row 25 (`28 - 3 = 25`) to cover all the positions where the 3x3 kernel can fit. This ensures that you can slide the 3x3 kernel over each row of each image in the batch.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### Convolution\n",
    "\n",
    "```python\n",
    "expanded_input = np.concatenate(sects,axis=1)\n",
    "es = expanded_input.shape\n",
    "flattened_input = expanded_input.reshape(es[0]*es[1],-1)\n",
    "\n",
    "kernel_output = flattened_input.dot(kernels)\n",
    "```\n",
    "\n",
    "The `expanded_input` holds all the sections of the image, flattened and concatenated连接. The dot product with `kernels` computes the convolution operation, creating the feature map.\n",
    "\n",
    "Certainly, let's break down these lines of code one by one:\n",
    "\n",
    "1. `expanded_input = np.concatenate(sects,axis=1)`: \n",
    "   - `sects` is a list of patches extracted from the input images. Each patch has dimensions `[batch_size, 1, kernel_rows, kernel_cols]`.\n",
    "   - The `np.concatenate` function concatenates连接 these patches along `axis=1`. After concatenation, you'll have a tensor where each \"column\" corresponds to a patch from the input images.\n",
    "   \n",
    "2. `es = expanded_input.shape`: \n",
    "   - This line stores the shape of `expanded_input` in `es`. Let's say, if you had 100 patches and your batch size is 128, `es` could look like `[128, 100, 3, 3]` (assuming 3x3 kernels).\n",
    "\n",
    "3. `flattened_input = expanded_input.reshape(es[0]*es[1],-1)`:\n",
    "   - The reshape operation is flattening the tensor along the first two dimensions (`batch_size` and number of patches).\n",
    "   - `es[0]*es[1]` would give you `128 * 100 = 12800` if we follow the earlier example. The `-1` in reshape means that the remaining dimensions are calculated automatically. In this case, the last two dimensions are 3x3, so they would be flattened into a single dimension of size `3 * 3 = 9`.\n",
    "\n",
    "4. `kernel_output = flattened_input.dot(kernels)`:\n",
    "   - Here, you have `flattened_input`, which has a shape like `[12800, 9]`, being matrix-multiplied (dot product) with `kernels`, which has a shape like `[9, num_kernels]` (let's say `[9, 16]` for 16 kernels).\n",
    "   - The output, `kernel_output`, will have a shape of `[12800, 16]`. Each row in `kernel_output` contains the activations produced by all 16 kernels for a specific patch in the input images.\n",
    "   \n",
    "Overall, these operations are aimed at preparing the input in a way that allows you to perform convolution operations effectively, applying multiple kernels over multiple patches and flattening them for easier calculations.\n",
    "\n",
    "### Activation and Dropout\n",
    "\n",
    "```python\n",
    "layer_1 = tanh(kernel_output.reshape(es[0],-1))\n",
    "dropout_mask = np.random.randint(2,size=layer_1.shape)\n",
    "layer_1 *= dropout_mask * 2\n",
    "```\n",
    "\n",
    "Here, the feature map goes through a tanh activation function. Then a dropout is applied, which is a regularization technique.\n",
    "\n",
    "### Backpropagation for CNN\n",
    "\n",
    "```python\n",
    "        layer_2_delta = (labels[batch_start:batch_end]-layer_2)\\\n",
    "                        / (batch_size * layer_2.shape[0])\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * \\\n",
    "                        tanh2deriv(layer_1)\n",
    "        layer_1_delta *= dropout_mask\n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        l1d_reshape = layer_1_delta.reshape(kernel_output.shape)\n",
    "        k_update = flattened_input.T.dot(l1d_reshape)\n",
    "        kernels -= alpha * k_update\n",
    "```\n",
    "\n",
    "In the backpropagation step, the gradients are calculated with respect to each kernel. Then, the kernels are updated.\n",
    "\n",
    "```python\n",
    "layer_2_delta = (labels[batch_start:batch_end]-layer_2) / (batch_size * layer_2.shape[0])\n",
    "```\n",
    "1. `layer_2_delta`: This is the error term for the output layer (`layer_2`). \n",
    "   - The error is computed as the difference between the true labels and the predicted values (`labels[batch_start:batch_end] - layer_2`).\n",
    "   - The division by `(batch_size * layer_2.shape[0])` is a normalization term. It averages the error over the number of instances in the batch.\n",
    "\n",
    "```python\n",
    "layer_1_delta = layer_2_delta.dot(weights_1_2.T) * tanh2deriv(layer_1)\n",
    "```\n",
    "2. `layer_1_delta`: This is the error term for the hidden layer (`layer_1`). \n",
    "   - It's calculated by taking the dot product of `layer_2_delta` and the transpose of `weights_1_2`. This backpropagates the error from the output layer to the hidden layer.\n",
    "   - This is then element-wise multiplied by `tanh2deriv(layer_1)`, which is the derivative of the activation function (tanh in this case) applied at `layer_1`. This gives you the gradient of the loss with respect to the activations in `layer_1`.\n",
    "\n",
    "```python\n",
    "layer_1_delta *= dropout_mask\n",
    "```\n",
    "3. This line applies the dropout mask to `layer_1_delta`.\n",
    "   - Dropout is used during training to randomly set some activations to zero, preventing overfitting. This line ensures that the same activations that were zeroed during the forward pass also have zero gradients during the backpropagation.\n",
    "\n",
    "```python\n",
    "weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "```\n",
    "4. This updates the weights between `layer_1` and `layer_2`.\n",
    "   - `alpha` is the learning rate, controlling how much we want to update the weights.\n",
    "   - `layer_1.T.dot(layer_2_delta)` calculates the gradient of the loss with respect to `weights_1_2`.\n",
    "\n",
    "```python\n",
    "l1d_reshape = layer_1_delta.reshape(kernel_output.shape)\n",
    "```\n",
    "5. The error term for `layer_1` (`layer_1_delta`) is reshaped back into the shape of `kernel_output`. This is done because you want to distribute this error back to each patch you originally extracted.\n",
    "\n",
    "```python\n",
    "k_update = flattened_input.T.dot(l1d_reshape)\n",
    "```\n",
    "6. `k_update`: This is the gradient for updating the kernels.\n",
    "   - The dot product is taken between the transpose of `flattened_input` and `l1d_reshape` to compute how much each kernel contributed to the overall error.\n",
    "\n",
    "```python\n",
    "kernels -= alpha * k_update\n",
    "```\n",
    "7. The kernels are updated.\n",
    "   - The gradients stored in `k_update` are used to update each kernel. The learning rate `alpha` controls the magnitude of this update.\n",
    "\n",
    "Each line of code in the backpropagation section plays a crucial role in updating the model's weights and biases, with the ultimate aim of minimizing the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5628b667-2264-4b9e-abde-4956d983dd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, sys\n",
    "np.random.seed(1)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "images, labels = (x_train[0:1000].reshape(1000,28*28) / 255,\n",
    "                  y_train[0:1000])\n",
    "\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels),10))\n",
    "for i,l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test),28*28) / 255\n",
    "test_labels = np.zeros((len(y_test),10))\n",
    "for i,l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh2deriv(output):\n",
    "    return 1 - (output ** 2)\n",
    "\n",
    "def softmax(x):\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
    "\n",
    "alpha, iterations = (2, 300)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "batch_size = 128\n",
    "\n",
    "input_rows = 28\n",
    "input_cols = 28\n",
    "\n",
    "kernal_rows = 3\n",
    "kernal_cols = 3\n",
    "num_kernal = 16\n",
    "\n",
    "hidden_size = ((input_rows - kernal_rows) * (input_cols - kernal_cols)) * num_kernal\n",
    "\n",
    "kernels = 0.02*np.random.random((kernel_rows*kernel_cols,num_kernels)) - 0.01\n",
    "\n",
    "weights_1_2 = 0.02*np.random.random((hidden_size, num_labels)) - 0.01\n",
    "\n",
    "\n",
    "def get_image_section(layer,row_from, row_to, col_from, col_to):\n",
    "    section = layer[:,row_from:row_to,col_from:col_to]\n",
    "    return section.reshape(-1,1,row_to-row_from, col_to-col_from)\n",
    "\n",
    "for j in range(iterations):\n",
    "    corrent_cnt = 0\n",
    "    for i in range(int(len(images)/batch_size)):\n",
    "        batch_start, batch_end = ((i*batch_size),((i+1)*batch_size)) #Batching Initalizaiton\n",
    "        #Reshaping Input Layer\n",
    "        layer_0 = images[batch_start:batch_end]\n",
    "        layer_0 = layer_0.reshape(layer_0.shape[0],28,28)\n",
    "        \n",
    "        #Convolution Initialization\n",
    "        sects = list() #This empty list will hold the sections of the image to which the kernel (convolution filter) will be applied.\n",
    "        #Convolution Process\n",
    "        for row_start in range(layer_0.shape[1] - kernel_rows):\n",
    "            for col_start in range(layer_0.shape[2] - kernel_rows):\n",
    "                sect = get_image_section(layer_0, row_start, row_star + kernel_rows, col_start, col_start + kernel_cols)\n",
    "                sects.append()\n",
    "\n",
    "        #Preparation for dot production\n",
    "        expanded_input = np.concatenate(sects,axis=1) #连接所有patches\n",
    "        es = expanded_input.shape #连接patches后的形状 (128,676,28,28)128batchSize 676pathes\n",
    "        flattened_input = expanded_input.reshape(es[0]*es[1],-1) #沿着batchSize128和patches676两个维度展平张量积即这个参数为128*676 -1表示剩下参数自动计算 \n",
    "\n",
    "        #Hidden Layer Activation and Dropout\n",
    "        kernal_output = flattened_input.dot(kernels)#kernels是[9,16] 点乘的结果[128*676*9,16]\n",
    "        layer_1 = tanh(kernal_output.reshape(es[0],-1)) #将patches结果按batchSize128展开 代入激活函数\n",
    "        drop_mask = np.random.randint(2,size=layer_1.shape) \n",
    "        layer_1 *= drop_mask #The hidden layer (layer_1) is activated using a tanh function after the convolution. Dropout is applied for regularization\n",
    "        layer_2 = softmax(layer_1.dot(weights_1_2)) #The output layer (layer_2) is activated using a softmax function to produce probabilities for each class label.\n",
    "\n",
    "        #Accuracy Counting\n",
    "        for k in range(batch_size):#counts the number of correct predictions within the batch.\n",
    "            labelset = labels[batch_start+k:batch_start+k+1]\n",
    "            _inc = int(np.argmax(layer_2[k:k+1]) == np.argmax(labelset))\n",
    "            correct_cnt += _inc \n",
    "\n",
    "        #Backpropogation\n",
    "        layer_2_delta = (labels[batch_start:batch_end] - layer_2) / (batch_size * layer_2.shape[0])\n",
    "        #The division by (batch_size * layer_2.shape[0]) is a normalization term. It averages the error over the number of instances in the batch.\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T)*tanh2deriv(layer_1)\n",
    "        layer_1_delta *= drop_mask\n",
    "        weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta)\n",
    "        kernals =\n",
    "\n",
    "        #test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
