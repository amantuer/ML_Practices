{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf737db-9d24-4311-850e-ad43e00191c8",
   "metadata": {},
   "source": [
    "**<em>Q:what's opinion do you have for me to edit the code to get 'Train-Acc' go down below 1.0?</em>**\n",
    "\n",
    "Getting the \"Train-Acc\" (Training Accuracy) below 1.0 seems a bit confusing, as accuracy is typically expressed as a value between 0 and 1, representing the fraction of correctly classified samples. A value greater than 1 for accuracy would indeed be incorrect.\r\n",
    "\r\n",
    "However, if you're looking for ways to improve the code's performance, there are several avenues you might explore:\r\n",
    "\r\n",
    "1. **Learning Rate Tuning**: You might try to adjust the learning rate `alpha` to see how the model performs. Sometimes a lower or higher learning rate can make a big difference.\r\n",
    "\r\n",
    "2. **Batch Size**: Instead of updating weights for each image, you could accumulate the gradient over a mini-batch of images and then update the weights. This often leads to more stable and better generalizing models.\r\n",
    "\r\n",
    "3. **Regularization**: Consider adding regularization techniques like dropout, L1 or L2 regularization to reduce overfitting, if it's occurring.\r\n",
    "\r\n",
    "4. **More Hidden Layers or Neurons**: Try adding more layers or neurons in the hidden layer, although be cautious as this could lead to overfitting as well.\r\n",
    "\r\n",
    "5. **Activation Functions**: You are using ReLU (Rectified Linear Units) as the activation function, which is good for many tasks. But, you might also want to experiment with other activation functions like tanh, sigmoid, or more advanced ones like LeakyReLU or Swish.\r\n",
    "\r\n",
    "6. **Early Stopping**: Instead of running for a fixed number of iterations, you can stop the training when the validation loss stops decreasing or starts to increase. \r\n",
    "\r\n",
    "7. **Weight Initialization**: The choice of weight initialization can also have a significant effect on training dynamics. You might want to try using other weight initialization methods.\r\n",
    "\r\n",
    "8. **Normalization**: Normalizing the inputs (if not already done) or using techniques like batch normalization within the network could also be beneficial.\r\n",
    "\r\n",
    "9. **Gradient Clipping**: If you're experiencing exploding gradients (although that's less common in shallow networks), you could clip gradients during backpropagation to keep them within reasonable bounds.\r\n",
    "\r\n",
    "Given that you're focusing on machine learning for your PhD, diving deep into these kinds of optimizations can offer valuable insights and could make for interesting experiments to understand the behavior of n\n",
    "\n",
    "**<em>Q:Is there some place have some notes like pattern of adjusting parameters and results that I can learn from?</em>**\n",
    "\n",
    "Certainly, there are multiple resources where you can learn about the strategies for hyperparameter tuning and optimization in machine learning. These can be especially valuable as you're focusing on machine learning for your PhD research. Here are some:\r\n",
    "\r\n",
    "1. **Books and Academic Papers**:\r\n",
    "   - \"Practical Statistics for Data Scientists\" by Andrew Bruce and Peter Bruce\r\n",
    "   - \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (this one you might already be familiar with)\r\n",
    "   - Research papers often discuss hyperparameter settings and justifications for those settings.\r\n",
    "\r\n",
    "2. **Online Courses and Tutorials**:\r\n",
    "   - Coursera, edX, and Udacity offer courses on machine learning and deep learning, often covering hyperparameter tuning.\r\n",
    "   - Fast.ai courses are practical and often discuss the intuition behind hyperparameter settings.\r\n",
    "\r\n",
    "3. **Forums and Blogs**:\r\n",
    "   - Websites like Medium, Towards Data Science, and Analytics Vidhya frequently feature articles on hyperparameter tuning.\r\n",
    "   - Forums like Reddit's r/MachineLearning and Stack Overflow contain threads discussing various optimization strategies.\r\n",
    "\r\n",
    "4. **GitHub Repositories**:\r\n",
    "   - Many researchers and engineers share their code along with insights into the choices they made for hyperparameters.\r\n",
    "  \r\n",
    "5. **Arxiv Papers**:\r\n",
    "   - Researchers often post preprints of their papers on arXiv, where they explain the hyperparameter choices in detail.\r\n",
    "  \r\n",
    "6. **Official Documentation**:\r\n",
    "   - Libraries like TensorFlow and PyTorch often include guides on best practices for setting hyperparameters.\r\n",
    "\r\n",
    "7. **Hyperparameter Optimization Libraries**: \r\n",
    "    - Libraries like Optuna, Hyperopt, and Scikit-learn's GridSearchCV or RandomizedSearchCV offer automated ways to perform hyperparameter tuning, and their documentation often includes useful tips.\r\n",
    "  \r\n",
    "8. **Machine Learning Competitions**: \r\n",
    "   - Websites like Kaggle feature competitions where you can see various kernels/notebooks submitted by others, which can provide insights into effective hyperparameter tuning.\r\n",
    "\r\n",
    "9. **Meetups and Conferences**: \r\n",
    "    - Attending machine learning meetups, workshops, or conferences can provide you with the latest practical knowledge and networking opportunities to learn from experts in the field.\r\n",
    "\r\n",
    "Each of these resources can provide you with a different perspective on hyperparameter tuning and can be a good supplement to your PhD research.eural networks better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9777d1b5-f2c1-498b-9782-dbd03f06f2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
