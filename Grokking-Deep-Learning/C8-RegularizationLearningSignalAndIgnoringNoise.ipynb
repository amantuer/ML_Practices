{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "875b2f49-428c-409c-8586-4a73c9531b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I:0 Train-Err:0.722 Train-Acc:0.537 Test-Err:0.601 Test-Acc:0.6488\n",
      " I:10 Train-Err:0.312 Train-Acc:0.901 Test-Err:0.420 Test-Acc:0.8114\n",
      " I:20 Train-Err:0.260 Train-Acc:0.937 Test-Err:0.414 Test-Acc:0.8111\n",
      " I:30 Train-Err:0.232 Train-Acc:0.946 Test-Err:0.417 Test-Acc:0.8066\n",
      " I:40 Train-Err:0.215 Train-Acc:0.956 Test-Err:0.426 Test-Acc:0.8019\n",
      " I:50 Train-Err:0.204 Train-Acc:0.966 Test-Err:0.437 Test-Acc:0.7982\n",
      " I:60 Train-Err:0.194 Train-Acc:0.967 Test-Err:0.448 Test-Acc:0.7921\n",
      " I:70 Train-Err:0.186 Train-Acc:0.975 Test-Err:0.458 Test-Acc:0.7864\n",
      " I:80 Train-Err:0.179 Train-Acc:0.979 Test-Err:0.466 Test-Acc:0.7817\n",
      " I:90 Train-Err:0.172 Train-Acc:0.981 Test-Err:0.474 Test-Acc:0.7758\n",
      " I:100 Train-Err:0.166 Train-Acc:0.984 Test-Err:0.482 Test-Acc:0.7706\n",
      " I:110 Train-Err:0.161 Train-Acc:0.984 Test-Err:0.489 Test-Acc:0.7686\n",
      " I:120 Train-Err:0.157 Train-Acc:0.986 Test-Err:0.496 Test-Acc:0.766\n",
      " I:130 Train-Err:0.153 Train-Acc:0.999 Test-Err:0.502 Test-Acc:0.7622\n",
      " I:140 Train-Err:0.149 Train-Acc:0.991 Test-Err:0.508 Test-Acc:0.758\n",
      " I:150 Train-Err:0.145 Train-Acc:0.991 Test-Err:0.513 Test-Acc:0.7558\n",
      " I:160 Train-Err:0.141 Train-Acc:0.992 Test-Err:0.518 Test-Acc:0.7553\n",
      " I:170 Train-Err:0.138 Train-Acc:0.992 Test-Err:0.524 Test-Acc:0.751\n",
      " I:180 Train-Err:0.135 Train-Acc:0.995 Test-Err:0.528 Test-Acc:0.7505\n",
      " I:190 Train-Err:0.132 Train-Acc:0.995 Test-Err:0.533 Test-Acc:0.7482\n",
      " I:200 Train-Err:0.130 Train-Acc:0.998 Test-Err:0.538 Test-Acc:0.7464\n",
      " I:210 Train-Err:0.127 Train-Acc:0.998 Test-Err:0.544 Test-Acc:0.7446\n",
      " I:220 Train-Err:0.125 Train-Acc:0.998 Test-Err:0.552 Test-Acc:0.7416\n",
      " I:230 Train-Err:0.123 Train-Acc:0.998 Test-Err:0.560 Test-Acc:0.7372\n",
      " I:240 Train-Err:0.121 Train-Acc:0.998 Test-Err:0.569 Test-Acc:0.7344\n",
      " I:250 Train-Err:0.120 Train-Acc:0.999 Test-Err:0.577 Test-Acc:0.7316\n",
      " I:260 Train-Err:0.118 Train-Acc:0.999 Test-Err:0.585 Test-Acc:0.729\n",
      " I:270 Train-Err:0.117 Train-Acc:0.999 Test-Err:0.593 Test-Acc:0.7259\n",
      " I:280 Train-Err:0.115 Train-Acc:0.999 Test-Err:0.600 Test-Acc:0.723\n",
      " I:290 Train-Err:0.114 Train-Acc:0.999 Test-Err:0.607 Test-Acc:0.7196\n",
      " I:300 Train-Err:0.113 Train-Acc:0.999 Test-Err:0.614 Test-Acc:0.7183\n",
      " I:310 Train-Err:0.112 Train-Acc:0.999 Test-Err:0.622 Test-Acc:0.7165\n",
      " I:320 Train-Err:0.111 Train-Acc:0.999 Test-Err:0.629 Test-Acc:0.7133\n",
      " I:330 Train-Err:0.110 Train-Acc:0.999 Test-Err:0.637 Test-Acc:0.7125\n",
      " I:340 Train-Err:0.109 Train-Acc:1.099 Test-Err:0.645 Test-Acc:0.71\n",
      " I:349 Train-Err:0.108 Train-Acc:1.0 Test-Err:0.653 Test-Acc:0.7073\n"
     ]
    }
   ],
   "source": [
    "#try adjust parameters to get right result for one hour after that move on!\n",
    "#330 after will jump across 0.99 to 1.099, but add if part for test it go down to 1.0\n",
    "import sys, numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "images, labels = (x_train[0:1000].reshape(1000,28*28)/255, y_train[0:1000])\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels),10))\n",
    "for i,l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test),28*28) / 255\n",
    "test_labels = np.zeros((len(y_test),10))\n",
    "for i,l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "\n",
    "np.random.seed(1)\n",
    "relu = lambda x:(x>=0)*x \n",
    "relu2deriv = lambda x: x>=0\n",
    "alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.005,350,40,784,10)\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image,hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size, num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations): #backprogation\n",
    "    error, correct_cnt = (0.0,0)\n",
    "    for i in range(len(images)):\n",
    "        layer_0 = images[i:i+1]\n",
    "        layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "        layer_2 = layer_1.dot(weights_1_2)\n",
    "\n",
    "        error += np.sum((labels[i:i+1] - layer_2)**2)\n",
    "        correct_cnt += int(np.argmax(layer_2) == np.argmax(labels[i:i+1]))\n",
    "\n",
    "        layer_2_delta = labels[i:i+1] - layer_2\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "\n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "    sys.stdout.write(\"\\r I:\" + str(j) + \" Train-Err:\" + str(error/float(len(images)))[0:5] + \" Train-Acc:\" + str(correct_cnt/float(len(images))))\n",
    "    if(j % 10 == 0 or j == iterations - 1):\n",
    "        error, correct_cnt = (0.0,0)\n",
    "        for i in range(len(test_images)):\n",
    "            layer_0 = test_images[i:i+1]\n",
    "            layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "            layer_2 = layer_1.dot(weights_1_2)\n",
    "\n",
    "            error += np.sum((layer_2 - test_labels[i:i+1])**2)\n",
    "            correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "        \n",
    "        sys.stdout.write(\" Test-Err:\" + str(error/float(len(test_images)))[0:5] + \" Test-Acc:\" + str(correct_cnt/float(len(test_images))))\n",
    "        print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e53e4a33-fe7d-448a-9a40-67332cc21692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I:0 Train-Err:0.322 Train-Acc:0.8601666666666666 Test-Err:0.292 Test-Acc:0.8615\n",
      " I:10 Train-Err:0.279 Train-Acc:0.8789333333333333 Test-Err:0.306 Test-Acc:0.8613\n",
      " I:20 Train-Err:0.306 Train-Acc:0.8572166666666666 Test-Err:0.325 Test-Acc:0.8475\n",
      " I:30 Train-Err:0.352 Train-Acc:0.8256833333333333 Test-Err:0.388 Test-Acc:0.8144\n",
      " I:40 Train-Err:0.421 Train-Acc:0.7727166666666667 Test-Err:0.428 Test-Acc:0.7777\n",
      " I:50 Train-Err:0.414 Train-Acc:0.7897533333333333 Test-Err:0.401 Test-Acc:0.799\n",
      " I:60 Train-Err:0.475 Train-Acc:0.7414166666666666 Test-Err:0.462 Test-Acc:0.7396\n",
      " I:70 Train-Err:0.520 Train-Acc:0.7009533333333334 Test-Err:0.483 Test-Acc:0.7231\n",
      " I:80 Train-Err:0.589 Train-Acc:0.6476166666666666 Test-Err:0.521 Test-Acc:0.6745\n",
      " I:90 Train-Err:0.632 Train-Acc:0.5842833333333334 Test-Err:0.551 Test-Acc:0.6083\n",
      " I:100 Train-Err:0.716 Train-Acc:0.564256666666667 Test-Err:0.627 Test-Acc:0.5902\n",
      " I:110 Train-Err:0.733 Train-Acc:0.5159166666666667 Test-Err:0.660 Test-Acc:0.5328\n",
      " I:120 Train-Err:0.942 Train-Acc:0.40595666666666676 Test-Err:0.705 Test-Acc:0.4065\n",
      " I:122 Train-Err:0.967 Train-Acc:0.39225666666666666"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amant\\AppData\\Local\\Temp\\ipykernel_25448\\2770537421.py:36: RuntimeWarning: overflow encountered in square\n",
      "  error += np.sum((labels[i:i+1] - layer_2)**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I:130 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:140 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:150 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:160 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:170 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:180 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:190 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:200 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:210 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:220 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:230 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:240 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:250 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:260 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:270 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:280 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:290 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:300 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:310 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:320 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:330 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:340 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n",
      " I:349 Train-Err:nan Train-Acc:0.09871666666666666 Test-Err:nan Test-Acc:0.098\n"
     ]
    }
   ],
   "source": [
    "#try adjust parameters to get right result for one hour after that move on!\n",
    "#whole mnist dataset training on 3090 takes 1h\n",
    "\"\"\"\n",
    " I:122 Train-Err:0.967 Train-Acc:0.39225666666666666\n",
    "C:\\Users\\amant\\AppData\\Local\\Temp\\ipykernel_25448\\2770537421.py:36: RuntimeWarning: overflow encountered in square\n",
    "  error += np.sum((labels[i:i+1] - layer_2)**2)\n",
    "  after iteration 122 get overflow error now considering adjust\n",
    "\"\"\"\n",
    "import sys, numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "images, labels = (x_train.reshape(60000,28*28) / 255, y_train)\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels),10))\n",
    "for i,l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test),28*28) / 255\n",
    "test_labels = np.zeros((len(y_test),10))\n",
    "for i,l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "\n",
    "np.random.seed(1)\n",
    "relu = lambda x:(x>=0)*x \n",
    "relu2deriv = lambda x: x>=0\n",
    "alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.005,350,40,784,10)\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image,hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size, num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations): #backprogation\n",
    "    error, correct_cnt = (0.0,0)\n",
    "    for i in range(len(images)):\n",
    "        layer_0 = images[i:i+1]\n",
    "        layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "        layer_2 = layer_1.dot(weights_1_2)\n",
    "\n",
    "        error += np.sum((labels[i:i+1] - layer_2)**2)\n",
    "        correct_cnt += int(np.argmax(layer_2) == np.argmax(labels[i:i+1]))\n",
    "\n",
    "        layer_2_delta = labels[i:i+1] - layer_2\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "\n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "    sys.stdout.write(\"\\r I:\" + str(j) + \" Train-Err:\" + str(error/float(len(images)))[0:5] + \" Train-Acc:\" + str(correct_cnt/float(len(images))))\n",
    "    if(j % 10 == 0 or j == iterations - 1):\n",
    "        error, correct_cnt = (0.0,0)\n",
    "        for i in range(len(test_images)):\n",
    "            layer_0 = test_images[i:i+1]\n",
    "            layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "            layer_2 = layer_1.dot(weights_1_2)\n",
    "\n",
    "            error += np.sum((layer_2 - test_labels[i:i+1])**2)\n",
    "            correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "        \n",
    "        sys.stdout.write(\" Test-Err:\" + str(error/float(len(test_images)))[0:5] + \" Test-Acc:\" + str(correct_cnt/float(len(test_images))))\n",
    "        print()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca67b2e-e77c-478f-a4fc-c967dd489c91",
   "metadata": {},
   "source": [
    "From the output you provided, it seems like the error is growing over time and then diverging into NaN values. The warning from Python indicates that an overflow encountered in square calculations, which is likely the source of your NaN issue. Here are some things to look at:\n",
    "\n",
    "### Addressing the Overflow and NaN Issues\n",
    "1. **Learning Rate**: The alpha value is set to 0.005. You may want to try a smaller value like 0.001 or even 0.0001 to make the model more stable.\n",
    "  \n",
    "2. **Weight Initialization**: Your weight initialization looks fine, but you might want to experiment with different schemes like Glorot initialization.\n",
    "\n",
    "3. **Activation Functions and Derivatives**: Make sure the activation functions and their derivatives are implemented correctly.\n",
    "\n",
    "4. **Numerical Stability**: Overflow happens because numbers get too large for Python to handle. This might occur when squaring an already large error.\n",
    "\n",
    "5. **Batch Size**: You're using online learning (updating weights after each example). Sometimes using mini-batch or batch learning can stabilize training.\n",
    "\n",
    "6. **Feature Scaling**: Ensure that your input data and labels are appropriately scaled. The division by 255 is a good normalization step for the images, but double-check that your labels are also appropriately scaled.\n",
    "\n",
    "### Debugging Tips:\n",
    "- Print or log the values of weights, activations, and gradients during training to see if anything stands out.\n",
    "- Start with a smaller subset of your data to make sure your model can at least overfit that much. If it can't, something is wrong.\n",
    "\n",
    "### Code Review:\n",
    "Your code seems to be straightforward. However, I'd recommend modularizing it a bit more, breaking it down into separate functions for forward and backward passes, error calculations, etc. This would make debugging and testing individual components easier.\n",
    "\n",
    "Addressing these issues should give you a clearer understanding of what's going wrong. Given your research interests in machine learning and quantum computing, solving challenges like these will give you practical skills that are essential for any advanced work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fee7fa2-d2b2-4b68-ad02-209e032582fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I:0 Train-Err:0.690 Train-Acc:0.5668833333333333 Test-Err:0.543 Test-Acc:0.7383\n",
      " I:10 Train-Err:0.357 Train-Acc:0.8636166666666667 Test-Err:0.349 Test-Acc:0.8675\n",
      " I:20 Train-Err:0.328 Train-Acc:0.8758166666666667 Test-Err:0.322 Test-Acc:0.8781\n",
      " I:30 Train-Err:0.313 Train-Acc:0.8816166666666667 Test-Err:0.307 Test-Acc:0.8835\n",
      " I:40 Train-Err:0.304 Train-Acc:0.8861666666666667 Test-Err:0.299 Test-Acc:0.8871\n",
      " I:50 Train-Err:0.299 Train-Acc:0.8886566666666666 Test-Err:0.294 Test-Acc:0.8892\n",
      " I:60 Train-Err:0.295 Train-Acc:0.8900333333333333 Test-Err:0.291 Test-Acc:0.8907\n",
      " I:70 Train-Err:0.293 Train-Acc:0.8912566666666667 Test-Err:0.289 Test-Acc:0.8921\n",
      " I:80 Train-Err:0.292 Train-Acc:0.8922666666666667 Test-Err:0.288 Test-Acc:0.8933\n",
      " I:90 Train-Err:0.291 Train-Acc:0.8924833333333333 Test-Err:0.287 Test-Acc:0.8947\n",
      " I:100 Train-Err:0.290 Train-Acc:0.892753333333334 Test-Err:0.286 Test-Acc:0.8951\n",
      " I:110 Train-Err:0.289 Train-Acc:0.8928566666666667 Test-Err:0.286 Test-Acc:0.8942\n",
      " I:120 Train-Err:0.289 Train-Acc:0.8925666666666666 Test-Err:0.285 Test-Acc:0.8937\n",
      " I:130 Train-Err:0.288 Train-Acc:0.8925566666666666 Test-Err:0.285 Test-Acc:0.8939\n",
      " I:140 Train-Err:0.287 Train-Acc:0.8928166666666667 Test-Err:0.284 Test-Acc:0.8937\n",
      " I:150 Train-Err:0.286 Train-Acc:0.8931666666666667 Test-Err:0.283 Test-Acc:0.8943\n",
      " I:160 Train-Err:0.285 Train-Acc:0.8934333333333333 Test-Err:0.282 Test-Acc:0.8951\n",
      " I:170 Train-Err:0.284 Train-Acc:0.8939166666666667 Test-Err:0.281 Test-Acc:0.8958\n",
      " I:180 Train-Err:0.283 Train-Acc:0.8937333333333334 Test-Err:0.280 Test-Acc:0.8954\n",
      " I:190 Train-Err:0.281 Train-Acc:0.8938566666666667 Test-Err:0.279 Test-Acc:0.8962\n",
      " I:200 Train-Err:0.280 Train-Acc:0.8938333333333334 Test-Err:0.277 Test-Acc:0.8957\n",
      " I:210 Train-Err:0.279 Train-Acc:0.8941333333333333 Test-Err:0.276 Test-Acc:0.8965\n",
      " I:220 Train-Err:0.278 Train-Acc:0.8939666666666667 Test-Err:0.276 Test-Acc:0.8967\n",
      " I:230 Train-Err:0.277 Train-Acc:0.8938833333333334 Test-Err:0.275 Test-Acc:0.8965\n",
      " I:240 Train-Err:0.277 Train-Acc:0.8935833333333333 Test-Err:0.275 Test-Acc:0.8963\n",
      " I:250 Train-Err:0.276 Train-Acc:0.8936333333333333 Test-Err:0.275 Test-Acc:0.8961\n",
      " I:260 Train-Err:0.276 Train-Acc:0.8934333333333333 Test-Err:0.274 Test-Acc:0.8956\n",
      " I:270 Train-Err:0.276 Train-Acc:0.8930333333333333 Test-Err:0.274 Test-Acc:0.895\n",
      " I:280 Train-Err:0.276 Train-Acc:0.8923833333333333 Test-Err:0.274 Test-Acc:0.8945\n",
      " I:290 Train-Err:0.275 Train-Acc:0.8922333333333333 Test-Err:0.274 Test-Acc:0.8934\n",
      " I:300 Train-Err:0.275 Train-Acc:0.8922333333333333 Test-Err:0.274 Test-Acc:0.8928\n",
      " I:310 Train-Err:0.275 Train-Acc:0.8920833333333333 Test-Err:0.273 Test-Acc:0.893\n",
      " I:320 Train-Err:0.275 Train-Acc:0.8918566666666667 Test-Err:0.273 Test-Acc:0.8927\n",
      " I:330 Train-Err:0.275 Train-Acc:0.8922166666666667 Test-Err:0.273 Test-Acc:0.8926\n",
      " I:340 Train-Err:0.274 Train-Acc:0.8919666666666667 Test-Err:0.272 Test-Acc:0.8922\n",
      " I:349 Train-Err:0.274 Train-Acc:0.8921833333333333 Test-Err:0.272 Test-Acc:0.8917\n"
     ]
    }
   ],
   "source": [
    "#change alpha = 0.0001\n",
    "import sys, numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "images, labels = (x_train.reshape(60000,28*28) / 255, y_train)\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels),10))\n",
    "for i,l in enumerate(labels):\n",
    "    one_hot_labels[i][l] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test),28*28) / 255\n",
    "test_labels = np.zeros((len(y_test),10))\n",
    "for i,l in enumerate(y_test):\n",
    "    test_labels[i][l] = 1\n",
    "\n",
    "np.random.seed(1)\n",
    "relu = lambda x:(x>=0)*x \n",
    "relu2deriv = lambda x: x>=0\n",
    "alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.0001,350,40,784,10)\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image,hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size, num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations): #backprogation\n",
    "    error, correct_cnt = (0.0,0)\n",
    "    for i in range(len(images)):\n",
    "        layer_0 = images[i:i+1]\n",
    "        layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "        layer_2 = layer_1.dot(weights_1_2)\n",
    "\n",
    "        error += np.sum((labels[i:i+1] - layer_2)**2)\n",
    "        correct_cnt += int(np.argmax(layer_2) == np.argmax(labels[i:i+1]))\n",
    "\n",
    "        layer_2_delta = labels[i:i+1] - layer_2\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "\n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "    sys.stdout.write(\"\\r I:\" + str(j) + \" Train-Err:\" + str(error/float(len(images)))[0:5] + \" Train-Acc:\" + str(correct_cnt/float(len(images))))\n",
    "    if(j % 10 == 0 or j == iterations - 1):\n",
    "        error, correct_cnt = (0.0,0)\n",
    "        for i in range(len(test_images)):\n",
    "            layer_0 = test_images[i:i+1]\n",
    "            layer_1 = relu(layer_0.dot(weights_0_1))\n",
    "            layer_2 = layer_1.dot(weights_1_2)\n",
    "\n",
    "            error += np.sum((layer_2 - test_labels[i:i+1])**2)\n",
    "            correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "        \n",
    "        sys.stdout.write(\" Test-Err:\" + str(error/float(len(test_images)))[0:5] + \" Test-Acc:\" + str(correct_cnt/float(len(test_images))))\n",
    "        print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42f5ac8-1437-4091-b4ac-ab99b1cefea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
